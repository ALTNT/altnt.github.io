<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>ALTNT's Hexo Blog | ALTNT's Hexo Blog</title><meta name="author" content="ALTNT"><meta name="copyright" content="ALTNT"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="title: “周报10月12日&quot; tag: crop classification，研究生周报 mathjax: true password: ly123 x&#39;c # 周报 10 月 12日 阅读论文： 《Reconstructing NDVI time series in cloud-prone regions: A fusion-and-fit approach with deep lea">
<meta property="og:type" content="article">
<meta property="og:title" content="ALTNT&#39;s Hexo Blog">
<meta property="og:url" content="http://blog.705553939.xyz/2024/10/12/crop_classification/Crop%20classification/10%E6%9C%88%2012%E6%97%A5%E5%91%A8%E6%8A%A5/index.html">
<meta property="og:site_name" content="ALTNT&#39;s Hexo Blog">
<meta property="og:description" content="title: “周报10月12日&quot; tag: crop classification，研究生周报 mathjax: true password: ly123 x&#39;c # 周报 10 月 12日 阅读论文： 《Reconstructing NDVI time series in cloud-prone regions: A fusion-and-fit approach with deep lea">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://blog.705553939.xyz/img/altnt.jpeg">
<meta property="article:published_time" content="2024-10-12T13:13:57.974Z">
<meta property="article:modified_time" content="2024-10-13T03:37:05.149Z">
<meta property="article:author" content="ALTNT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.705553939.xyz/img/altnt.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://blog.705553939.xyz/2024/10/12/crop_classification/Crop%20classification/10%E6%9C%88%2012%E6%97%A5%E5%91%A8%E6%8A%A5/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ALTNT\'s Hexo Blog',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-13 11:37:05'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/altnt.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="ALTNT's Hexo Blog"><span class="site-name">ALTNT's Hexo Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Untitled</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-10-12T13:13:57.974Z" title="Created 2024-10-12 21:13:57">2024-10-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-10-13T03:37:05.149Z" title="Updated 2024-10-13 11:37:05">2024-10-13</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><hr>
<p>title: “周报10月12日" tag: crop classification，研究生周报 mathjax:
true password: ly123 x'c # 周报 10 月 12日</p>
<h2 id="阅读论文">阅读论文：</h2>
<p>《Reconstructing NDVI time series in cloud-prone regions: A
fusion-and-fit approach with deep learning residual constraint》</p>
<p>这是一篇关于使用时空融合的方式，重建多云区域的缺失图像，进而对 NDVI
曲线进行拟合的论文</p>
<p>提出的网络名：</p>
<p><strong>Recoff</strong> （<strong>Re</strong>sidual (Re)
<strong>Co</strong>nstraints (Co)
<strong>f</strong>usion-and-<strong>f</strong>it (ReCoff)）</p>
<p>ReCoff
由<strong>融合和拟合</strong>两个步骤组成，其中<strong>融合用于重建缺失数据，是最关键的一步，</strong>感觉有参考性</p>
<p>解决的问题：</p>
<p>解决了在多云地区（<strong>既有大面积的数据间隙，又有长期的时间间隔)</strong>
重建 30 米 landsat NDVI 时间序列数据的挑战。</p>
<p>使用的卫星数据：</p>
<p>landsat和中分辨率成像光谱仪（MODIS）的图像</p>
<h3 id="问题背景">问题背景</h3>
<p>对 NDVI 时间序列数据的重建两类方法（这里应该还没涉及到云的影响)：</p>
<h4 id="基于时间拟合和基于空间填充的方法来自同一传感器的时间或空间信息去填充缺失的信息">（1)基于时间（拟合）和基于空间（填充）的方法来自<strong>同一传感器</strong>的时间或空间信息去填充缺失的信息</h4>
<h5 id="填充的方法依赖空间的相关性">1、填充的方法（依赖空间的相关性)</h5>
<p>对于处理图像中的小间隙很有用，它通过使用同一图像中相似像素的值或时间上接近的像素的值来填充缺失数据</p>
<p>但在处理<strong>大面积缺失数据</strong>时，<strong>缺乏足够的先验信息</strong></p>
<h5 id="拟合的方法依赖时间的相关性">2、拟合的方法（依赖时间的相关性)</h5>
<p>也称为时间插值，，主要特点是利用时间序列信息</p>
<p>它包括三种主要类型：</p>
<p>（1)有<strong>基于函数的曲线拟合方法</strong>，例如非对称高斯（AG）模型（Jonsson
和 Eklundh，2002）和双逻辑（DL）函数（Beck
等人，2006）。这些方法擅长生成相对平滑的结果，捕捉年际动态，但在捕捉突变信息方面可能缺乏精度（Zhu
等人，2012）</p>
<p>（2)采<strong>用频域方法</strong>，如时间序列谐波分析（HANTS）和小波方法（Zhou
等人，2015）</p>
<p>在调整参数以适应不同的 NDVI 信号方面具有灵活性，</p>
<p>缺陷：但是其准确性可能会受到有效数据点数量的影响（Roy 和
Yan，2020）</p>
<p>（3)<strong>滑动窗口滤波技术</strong></p>
<p>例如Savitzky–Golay（SG）滤波器（Chen 等人，2004）</p>
<p>由于其实现简单，已在时间序列数据的重建中得到广泛应用（Chen
等人，2021）。</p>
<p>缺陷：然而，滤波参数的选择对滤波结果有很大影响（Shen 等人，2015）</p>
<h4 id="这两种方法的挑战">这两种方法的挑战</h4>
<p>填充和拟合方法分别依赖于空间或时间域中的相关性。</p>
<p>然而，在多云地区，既有大面积的数据间隙，又有长期的时间间隔，这给这些方法带来了很大的限制。</p>
<h4 id="融合策略">（2)融合策略</h4>
<p><strong>融合策略（主要是指时空融合）可以通过融合高分辨率和低分辨率的图像（如
landsat和中分辨率成像光谱仪（MODIS）的图像）来生成完整的高分辨率图像</strong>因此，即使在多云地区，密集的低分辨率数据也为重建高分辨率图像提供了额外的信息</p>
<p>将融合方法分为<strong>传统方法</strong>和<strong>基于学习的</strong>方法（主要侧重于深度学习）</p>
<h5 id="传统融合方法-------依赖参考日期的至少一对无云的高分辨率和低分辨率图像和目标日期的低分辨率图像"><strong>1、传统融合方法-------依赖参考日期的至少一对无云的高分辨率和低分辨率图像和目标日期的低分辨率图像</strong></h5>
<p>传统融合方法具有明确的物理解释，利用在<strong>参考（或辅助）日期</strong>获取的至少一对无云的高分辨率和低分辨率图像，以及在目标日期的低分辨率图像（Wang
和 Atkinson，2018）</p>
<p>这种方法即使在多云地区也能重建时间序列数据，并且适用于均匀区域（例如，时空自适应反射率融合模型（STARFM））（Beck
等人，2006）和非均匀区域（例如，增强型 STARFM（ESTARFM）（Zhu
等人，2010）、灵活时空数据融合（FSDAF）（Zhu
等人，2016）以及模型拟合（Fit）、空间滤波（F）、残差补偿（Fit-FC）（Wang
和
Atkinson，2018））。此外，具有明确物理解释的模型增强了其稳健性，并保留了更多的空间细节。</p>
<p><strong>（缺陷)</strong>然而，由于其预定义<strong>线性函数</strong>的<strong>泛化能力有限</strong>（Tan
等人，2018），大多数传统方法在应对各种<strong>土地覆盖变化</strong>情况时面临挑战（Liu
等人，2019）。在多云地区，<strong>长期的时间数据间隙</strong>很可能与<strong>土地覆盖的变化</strong>相关。</p>
<p>。</p>
<p>这些局限性已经通过<strong>深度学习技术</strong>的发展得到了部分解决</p>
<h5 id="基于深度学习的方法"><strong>2、基于深度学习的方法</strong></h5>
<p><strong>基于是否使用参考高分辨率图像，深度学习融合方法大致可以分为两大类</strong>。</p>
<p>（1)首先，遵循超分辨率的概念，深度学习融合方法可以<strong>直接在低分辨率图像和高分辨率图像之间建立非线性关系</strong>（Wang
和 Wang，2020）。</p>
<p>（2)第二类是<strong>使用卷积神经网络（CNN）自动推导加权非线性函数，取代预定义的线性函数</strong>（如使用生成对抗网络的时空融合（STFGAN）（Zhang
等人，2021）和多场景时空融合网络（MUSTFN）（Qin 等人，2022））。</p>
<p>优势：这些方法能够整合来自不同传感器的时空信息，适用于涉及物候和土地覆盖变化的场景（Jia
等人，2022）</p>
<p>缺陷：</p>
<p>（1)然而，<strong>大多数深度学习方法缺乏明确的物理解释，导致模型的不确定性增加</strong>，图像模糊的风险更高（Li
等人，2020b）。</p>
<p>（2)此外，在多云地区确定令人满意的参考图像对（MODIS -
Landsat）具有挑战性，并且对融合精度有很大影响（Chen 等人，2021）。</p>
<p>（3)此外，<strong>无论选择哪种融合方法，所使用的参考数据量始终存在一个限制，通常仅限于一两对</strong>。这种限制<strong>忽略了有价值的长时间序列信息</strong>。如果在参考数据中未捕获土地覆盖变化信息，它会对重建的有效性施加限制（Shen
等人，2015）。</p>
<p>（4)最后，<strong>低分辨率图像的最佳分辨率仍然不确定</strong>。</p>
<h3 id="方法">方法</h3>
<p>ReCoff 由<strong>ReCoF融合(</strong>Re<strong>sidual (Re)
</strong>Co<strong>nstraints (Co)
</strong>f<strong>usion)</strong>和<strong>SG拟合（Savitzky–Golay)</strong>
两个步骤</p>
<figure>
<img src="/2024/10/12/crop_classification/Crop%20classification/10%E6%9C%88%2012%E6%97%A5%E5%91%A8%E6%8A%A5/1728653870849.png" alt="1728653870849">
<figcaption aria-hidden="true">1728653870849</figcaption>
</figure>
<p>图1.
ReCoff的流程图；（a）时空融合使用ReCoF重建时间序列数据；（b）SG滤波器平滑时间序列数据</p>
<h4 id="残差约束融合模块---recof">残差约束融合模块 - ReCoF</h4>
<figure>
<img src="/2024/10/12/crop_classification/Crop%20classification/10%E6%9C%88%2012%E6%97%A5%E5%91%A8%E6%8A%A5/1727261274757.png" alt="1727261274757">
<figcaption aria-hidden="true">1727261274757</figcaption>
</figure>
<pre><code>图2. ReCoF的网络架构</code></pre>
<p><strong>输入：</strong></p>
<p>ReCoF的<strong>输入</strong>包括<strong>来自参考日期的一对高低分辨率参考图像（<span class="math inline">\(F_1\)</span>和<span class="math inline">\(C_1\)</span>），以及来自目标日期的一幅低分辨率图像（<span class="math inline">\(C_{2}\)</span>）</strong>。此外，它还<strong>包含了这两个低分辨率数据之间的差异（<span class="math inline">\(C_{2}-C_{1}\)</span>)</strong>。</p>
<p><strong>输出：</strong></p>
<p>重建后的<strong>目标日期的高分辨率图像（<span class="math inline">\(F_2\)</span>)</strong></p>
<p>步骤如下：</p>
<p>（1)首先，为了保留高分辨率图像中的详细信息，ReCoF<strong>采用了不同的对原始的低分辨率和高分辨率图像的卷积，保持输入数据的相对独立性</strong>。</p>
<p>（2)
模型的骨干网络使用各种卷积模块和跳跃连接<strong>从浅到深生成三个层次的特征（<span class="math inline">\(f_1\)</span>，<span class="math inline">\(g_1\)</span>，<span class="math inline">\(h_1\)</span>）</strong>。------------这里估计是为了利用空间域的信息</p>
<p>这么做的原因是<strong>深层特征</strong>包含更多<strong>语义信息</strong>，增强了模型的<strong>表达能力</strong>，而<strong>浅层信息</strong>保留了更多<strong>细节</strong>，有利于图像恢复。</p>
<p>（3)为了重建不同大小的各种土地覆盖类型，我们在<span class="math inline">\(g_1\)</span>和<span class="math inline">\(h_1\)</span>上<strong>采用两种不同的特征金字塔结构来提取不同尺度的特征（<span class="math inline">\(f_2\)</span>和<span class="math inline">\(f_3\)</span>)</strong>。</p>
<pre><code>1、首先，基于$g_1$应用三次**自适应平均池化（AAPool）**操作来生成各种大小的特征图。然后对这些特征图进行**重采样并堆叠**形成$f_2$  。

2、为了减少信息损失，在更深的**$h_1$层** ，ReCoF利用**不同速率的扩张卷积**，**生成三个不同尺度的特征并连接起来形成$f_3$**。</code></pre>
<p>（4) 因此，我们获得了<strong>多尺度和多层次的特征（<span class="math inline">\(f_1\)</span>、<span class="math inline">\(f_2\)</span>、<span class="math inline">\(f_3\)</span>）</strong>，随后，采用<strong>通道注意力机制</strong>来整合这些特征。</p>
<p>（5)在应用Tanh激活函数后，模型产生一个<strong>残差</strong><span class="math inline">\(\varepsilon\)</span>,这个残差用于约束整个模型精确预测土地覆盖变化，同时保留原始高分辨率图像的空间细节</p>
<p>（6) 最后，目标高分辨率图像由<strong>残差</strong><span class="math inline">\(\varepsilon\)</span>和参考高分辨率图像<span class="math inline">\(F_1\)</span>和 <span class="math inline">\(a *
(C_2 - C_1)\)</span>共同通过<span class="math inline">\(F_2=\varepsilon+a * (C_2-C_1)+
F_1\)</span>计算得到,其中 a 为线性模型的相关系数</p>
<p>。</p>
<h5 id="公式推导">公式推导</h5>
<p>上面的过程的公式推导过程如下：</p>
<p>对于在粗分辨率图像中被确定为<strong>端元（endmember)
的给定粗分辨率像素（<span class="math inline">\(C\)</span>），-----</strong>（端元的定义:组成混合像元的纯净地物被称作是端元
)，</p>
<p>当重采样到与高分辨率像素相同的分辨率时，假设在不同获取条件下特定区域内是稳定的，其在日期<span class="math inline">\(m\)</span>的高分辨率图像中对应的<strong>高分辨率像素（F）（注意这里是像素)</strong>
可以用以下方程表示：</p>
<p><span class="math inline">\(F_{j,m}=a\cdot C_{j,m}+b
(2)\)</span>，————（这个公式的的物理含义怎么理解？为什么高分辨率和低分辨率之间是线性的关系？跟下面的线性混合像素分解理论有关系吗？)</p>
<p>其中<strong>a和b是用于粗分辨率和高分辨率图像之间相对校准的线性回归模型的系数</strong>，<strong>j是第j个像素</strong>。------(注意
j 应该说说在高分辨率图中的第 j 个像素，不是低分辨率的)</p>
<p>.</p>
<p>在实际情况中，<strong>C通常是一个混合像素</strong>。</p>
<p>根据<strong>线性混合像素分解理论</strong>，在日期m的一个特定混合像素可以表示为<strong>各类端元值乘以其各自比例的总和再加上残差</strong>。</p>
<p><span class="math inline">\(C_{m}=\sum_{i=1}^{N} C_{i, m} \cdot
f_{i}+\varepsilon_{m} (3)\)</span>，</p>
<p>其中<span class="math inline">\(C_{i，m}\)</span>是第i个端元类别（可以理解为作物或土壤、水等物质的类别，这些类别混合起来构成一个<span class="math inline">\(C_m\)</span>)，其对应的<strong>面积比例是</strong><span class="math inline">\(f_{i}\)</span>，<span class="math inline">\(N\)</span>是混合像素<span class="math inline">\(C_{m}\)</span>内端元类别的总数，<span class="math inline">\(\varepsilon_{m}\)</span>是混合像素残差</p>
<p>.</p>
<p>如果将每个端元视为一个个体，<span class="math inline">\(C_{m}\)</span>（由方程（3）得出的）也可以表示为所有对应端元的平均值加上残差：</p>
<p><span class="math inline">\(C_{m}=\frac{1}{K} \sum_{j=1}^{K}
C_{j,m}+\varepsilon_{m}\)</span>，（4）)</p>
<p>其中<span class="math inline">\(C_{j,m}\)</span>是第j个端元，K是混合像素内端元的总数，<span class="math inline">\(\varepsilon_{m}\)</span>是混合像素残差。---------注意
j 应该说说在高分辨率图中的第 j 个端元，不是低分辨率的</p>
<p>方程（4）等同于以下公式：</p>
<p><span class="math inline">\(\frac{1}{K} \sum_{j=1}^{K}
C_{m}=\frac{1}{K} \sum_{j=1}^{K}\left(C_{j, m}+\varepsilon_{j, m}\right)
(5)\)</span>，-------这里实际上就是<span class="math inline">\(\varepsilon_{m}=\frac{1}{K}
\sum_{j=1}^{K}\varepsilon_{j, m}\)</span>变了个形，<span class="math inline">\(C_m\)</span>就是 K 个端元值的和取平均值</p>
<p>其中<span class="math inline">\(C_{m}\)</span>是一个不受j影响的常数，这样理解的话就可以看出来<span class="math inline">\(\frac{1}{K} \sum_{j - 1}^{K}
C_{m}\)</span>等同于<span class="math inline">\(C_{m}\)</span>，<span class="math inline">\(\varepsilon_{j， m}\)</span>是每个端元的残差。</p>
<p>.</p>
<p><strong>将<span class="math inline">\(\varepsilon_{j,
m}\)</span>视为一个可训练的参数</strong>，它的值取决于不同的端元<span class="math inline">\(C_{j,m}\)</span>和相应的混合像素<span class="math inline">\(C_{m}\)</span>，这使得方程（5）成立。</p>
<p>在这种情况下，混合像素可以表示为其内部端元<span class="math inline">\(C_{j,m}\)</span>之一加上相应的端元残差<span class="math inline">\(\varepsilon_{j,m}\)</span>，----------------这句话不能这么理解吧？混合像素是平均值怎么可以用单个端元表示？</p>
<p>对于不同的端元像素<span class="math inline">\(C_{j,m}\)</span>，<span class="math inline">\(\varepsilon_{j,m}\)</span>是一个需要确定的可变参数：</p>
<p><span class="math inline">\(C_{m}=C_{j, m}+\varepsilon_{j, m}
(6)\)</span>------------感觉这个公式有问题</p>
<p>将方程（2）代入方程（6）并化简（消掉<span class="math inline">\(C_{j,
m}\)</span>)，我们可以得到：</p>
<p><span class="math inline">\(C_{m}=\frac{F_{j,
m}}{a}-\frac{b}{a}+\varepsilon_{j, m} (7)\)</span></p>
<p>同样地，对于目标日期n：</p>
<p><span class="math inline">\(C_{n}=\frac{F_{j,
n}}{a}-\frac{b}{a}+\varepsilon_{j,n} (8)\)</span>。</p>
<p>（8）式减去（7）式并化简：</p>
<h6 id="方法所使用公式的最终推导结果形式">（方法所使用公式的最终推导结果形式)</h6>
<p><span class="math inline">\(F_{j,n}=a\cdot(C_{n}-C_{m})+F_{j,m}+\varepsilon_{j,n}\)</span>
（9)</p>
<p>预测日期（n）的高分辨率像素<span class="math inline">\(F_{j,n}\)</span>等于参考日期（m）的高分辨率像素<span class="math inline">\(F_{j,m}\)</span>加上<strong>相应混合像素的变化量（<span class="math inline">\(C_{n}-C_{m}\)</span>）</strong>以及<strong>相应端元残差</strong>。</p>
<h6 id="参数-a-的获取可以直接预先设置">（参数 a
的获取——可以直接预先设置)</h6>
<p>参数a可以通过跨不同区域构建<strong>线性方程</strong>来确定（见表
S1）。------这个参数怎么算的？</p>
<figure>
<img src="/2024/10/12/crop_classification/Crop%20classification/10%E6%9C%88%2012%E6%97%A5%E5%91%A8%E6%8A%A5/1728718216080.png" alt="1728718216080">
<figcaption aria-hidden="true">1728718216080</figcaption>
</figure>
<h6 id="残差-varepsilon的计算使用深度学习模型来计算他">（残差 <span class="math inline">\(\varepsilon\)</span>的计算————使用深度学习模型来计算他)</h6>
<p>同时，<strong>残差受到不同日期的混合像素及其对应不同土地覆盖类型的端元的影响。</strong></p>
<p>随着时间的推移，端元的土地覆盖会发生变化，通常表现为<strong>非线性和复杂的变化。</strong>
这种现象<strong>导致<span class="math inline">\(\varepsilon_{j}\)</span>成为一个复杂的非线性参数</strong>。幸运的是，深度学习方法在处理此类非线性参数方面具有显著优势（LeCun
等人，2015）。</p>
<p>通过<strong>将模型的标签设置为残差 <span class="math inline">\(\varepsilon\)</span></strong>，我们可以利用深度学习模型来求解它，然后接着使用
残差 <span class="math inline">\(\varepsilon\)</span>的得到目标高分辨率图像<span class="math inline">\(F_{2}\)</span>。</p>
<p><span class="math inline">\(\varepsilon=F_{2}-F_{1}-a\cdot(C_{2}-C_{1})(10)\)</span>。--------（即公式（9)的变形)</p>
<p>。</p>
<p>通过使用残差作为标签而不是直接使用目标图像（<span class="math inline">\(F_{2}\)</span>），整个模型的输出被约束以满足以下标准：</p>
<p>同时捕捉高分辨率图像的空间纹理细节变化（<span class="math inline">\(F_{2}-F_{1}\)</span>）和低分辨率图像的光谱变化（<span class="math inline">\(C_{2}-C_{1}\)</span>）。</p>
<p>在<strong>参考高分辨率遥感图像（<span class="math inline">\(F_{1}\)</span>）</strong>
的帮助下，这些约束进一步有助于恢复原始遥感图像的空间细节。</p>
<p>最后，预测日期的高分辨率图像可以表示为：</p>
<p><span class="math inline">\(F_{2}=a\cdot(C_{2}-C_{1})+F_{1}+\varepsilon
(11)\)</span>，</p>
<p>其中<span class="math inline">\(F_{2}\)</span>是目标日期的高分辨率图像。</p>
<p>。</p>
<h6 id="方程11的解释">（方程（11）的解释)</h6>
<p>方程（11）具有明确的物理意义，其中待预测的高分辨率图像（<span class="math inline">\(F_{2}\)</span>）由参考日期的高分辨率图像（<span class="math inline">\(F_{1}\)</span>）以及两个不同日期的低分辨率图像的变化量（<span class="math inline">\(C_{2}-C_{1}\)</span>）决定。</p>
<p>该模型学习不同日期和分辨率的图像之间的变化差异，将其视为<strong>残差</strong>以获得<span class="math inline">\(F_{2}\)</span>，并将低分辨率图像的变化映射到高分辨率图像上。</p>
<p>同时，残差控制着低分辨率像素和高分辨率像素之间的信息传递，确保保留高分辨率图像的空间信息和低分辨率图像的时间信息。</p>
<p>与直接输出预测图像不同，通过构建线性方程获得目标数据避免了卷积过程引起的图像模糊，并在尽可能保留原始空间纹理细节的同时努力恢复变化信息。</p>
<h4 id="savitzkygolaysg滤波器拟合">Savitzky–Golay，SG滤波器——拟合</h4>
<p>（参考资料：https://blog.csdn.net/weixin_36815313/article/details/121238628）</p>
<p><strong>融合过程利用的是相对较短的时空信息</strong>，而长期的实际
landsat归一化植被指数（NDVI）时间序列数据在较长时期内提供了广泛的时间信息。</p>
<p>我们用重建的图像替换实际图像中的缺失值，并在实际图像不可用的时间段仅使用重建图像,
然后，我们将萨维茨基-高莱（Savitzky-Golay，SG）滤波器应用于组合的landsat时间序列，使用长期的实际landsat图像对重建的
NDVI 时间序列图像进行微调。</p>
<p>如方程（12）所示（Chen 等人，2004），<strong>SG
滤波器使用一定次数的多项式在固定窗口内对一组连续值进行最小二乘拟合。</strong></p>
<p><span class="math inline">\(N D V I_{j}^{*}=\frac{\sum_{i=-m}^{i - m}
C_{i} \cdot N D V I_{j + 1}}{N} (12)\)</span>，</p>
<p>其中<span class="math inline">\(NDVI^{*}\)</span>是目标归一化植被指数值，<span class="math inline">\(C_{i}\)</span>是滑动窗口中第<span class="math inline">\(i\)</span>个归一化植被指数值的系数，<span class="math inline">\(m\)</span>是滑动窗口的半径（<span class="math inline">\(m = 3\)</span>），<span class="math inline">\(j\)</span>是时间序列数据的索引，<span class="math inline">\(N\)</span>是 convoluting integers的数量（<span class="math inline">\(N = 2\)</span>）。</p>
<p>在此之后，我们<strong>利用空间、时间、长时间序列周期性和多传感器信息重建了
NDVI 时间序列数据</strong>。</p>
<h3 id="总结">总结</h3>
<p>1、本文主要的一个创新点是下面这个公式的一个推导过程，<strong>要预测的目标数据（F₂）与高分辨率参考数据（F₁）直接相关，而无需经过深度学习模型的高维特征变换（直接建模输出
F₂）</strong>。这种方法<strong>避免了卷积操作引起的图像模糊</strong>，从而保留了更多的空间细节信息。------现在都有残差结构，一般不会出现这种问题，感觉最大的好处是即使参考图像与目标图像的时间间隔差距较大，也可以由参考图像生成目标图像（找到了参考图像与目前图像之间的映射关系，而不用考虑临近图像长期缺失问题)</p>
<figure>
<img src="/2024/10/12/crop_classification/Crop%20classification/10%E6%9C%88%2012%E6%97%A5%E5%91%A8%E6%8A%A5/1728785405969.png" alt="1728785405969">
<figcaption aria-hidden="true">1728785405969</figcaption>
</figure>
<p>2、使用参考低分辨率图像与目前低分辨率图像的残差作为输入
是为了解决土地覆盖发生变化后（农作物被收割、森林被砍伐等很多情况)后重建的高分辨率图像不进行响应的问题。----在发生土地覆盖变化后，ReCoF利用目标日期和参考日期的粗分辨率数据之间的差异作为变化信息，直接将其集成到模型中，实现了构建非线性深度学习模型将粗分辨率（C₂-C₁）的变化映射到高分辨率图像（F₂-F₁）</p>
<p>3、通过将深度学习融合方法（从不同传感器中提取多尺度和多层次时空特征）与传统
SG 过滤（捕获长期特征）相结合，<strong>ReCoff
可以同时利用空间、时间、长时间序列（周期性）和多传感器信息。</strong></p>
<h4 id="思考">思考</h4>
<p>1、这篇文章重建有云时刻的高分辨率图像时候，<strong>参考图像</strong>只使用了一张高分辨率图像和一张低分辨率图像，而<strong>目标图像</strong>也只使用了一张低分辨率图像，多次生成来形成连续的高分辨率图像，感觉是在做时空融合的超分</p>
<p>2、本论文的时空融合，时间上主要是参考图像和目标图像上的时序融合以及使用
sg 滤波器进行了融合；空间上主要是在 参考高和低分辨率图像
以及低分辨率映射到高分辨率图像的过程中体现</p>
<p>3、这里时空融合做超分用的图片用了几个波段，NDVI
计算只需要两个波段，可是分类任务需要的波段数更多，如果先超分再融合计算量是不是会变大</p>
<p>4、这个方法在超分的过程中好像还是得考虑测试区域和训练区域之间存在显著的空间异质性问题</p>
<h2 id="代码方面">代码方面</h2>
<h3 id="cacm进行数据集制作">1、CACM进行数据集制作</h3>
<p>主要在跑通CACM进行数据集制作（下载哨兵二号图像数据、标签数据、气候数据)的整个流程。</p>
<p>目前已成功跑通在加拿大阿尔伯塔省第二分区（Canada Division No. 2,
Alberta）和马尼托巴省第 14 分区（Division No. 14,
Manitoba）上获取遥感图像和标签以及获取气候数据的 tiff 图片，并利用tiff
图片成功制作了csv 表格，并且 csv
文件成功作为模型输入和标签跑通了训练流程（虽然这两个区域在论文中是作为测试集使用的)</p>
<p>整个记录如何生产 CACM 数据集的过程及遇到的坑如下链接所示：</p>
<p><a href="https://blog.705553939.xyz/2024/10/07/crop_classification/Document_how_to_produce_CACM_datasets/">https://blog.705553939.xyz/2024/10/07/crop_classification/Document_how_to_produce_CACM_datasets/</a></p>
<h3 id="美国区域数据集制作">2、美国区域数据集制作</h3>
<p>目前下载了了一个印第安纳州的哨兵二号图像数据</p>
<p><strong>印第安纳州（Indiana）</strong></p>
<ul>
<li><p><strong>气候特点</strong>：</p>
<ul>
<li>印第安纳州气候为温带大陆性湿润气候，在作物生长季云量不算多。春季和秋季较为温和，夏季比较炎热，光照充足。年平均日照时间较长，能够保证农作物在生长期间获得足够的光照进行光合作用。</li>
<li>降水量较为充沛，年降水量大约在900 -
1000毫米左右，并且降水时间分布相对合理，对玉米、大豆和部分水稻种植较为有利。</li>
</ul></li>
<li><p><strong>农业情况</strong>：</p>
<ul>
<li><p>该州是玉米、大豆等农作物的重要产地。其良好的气候条件加上先进的农业技术，使得农作物种植面积广泛，产量稳定。在一些有灌溉条件的区域，也有少量水稻种植试验和生产。</p>
<figure>
<img src="/2024/10/12/crop_classification/Crop%20classification/10%E6%9C%88%2012%E6%97%A5%E5%91%A8%E6%8A%A5/1728787086826.png" alt="1728787086826">
<figcaption aria-hidden="true">1728787086826</figcaption>
</figure></li>
</ul>
<p>把这个州分成了32 个小矩形区域，每个矩形宽度大概50公里</p>
<figure>
<img src="/2024/10/12/crop_classification/Crop%20classification/10%E6%9C%88%2012%E6%97%A5%E5%91%A8%E6%8A%A5/1728786858718.png" alt="1728786858718">
<figcaption aria-hidden="true">1728786858718</figcaption>
</figure></li>
</ul>
<h4 id="图片格式说明">图片格式说明：</h4>
<p>由于在 GEE 上下载每张图片都是一个
Task，每下载一张图片都需要点击一次按钮，太麻烦了，所以将同一地点同一年份的所有图像合成一张图像。</p>
<p>一张图像对应上面的一个区域一年的5月到9月（120天到255天）的图像，时间间隔为15天，所以一张图像共有（270-120）/15=10个时刻的图像，每个时刻10个波段，所以每张图片100个波段。</p>
<figure>
<img src="/2024/10/12/crop_classification/Crop%20classification/10%E6%9C%88%2012%E6%97%A5%E5%91%A8%E6%8A%A5/1728787559972.png" alt="1728787559972">
<figcaption aria-hidden="true">1728787559972</figcaption>
</figure>
<p>图像命名为“地点_ID_年份”，</p>
<p><img src="/2024/10/12/crop_classification/Crop%20classification/10%E6%9C%88%2012%E6%97%A5%E5%91%A8%E6%8A%A5/1728787142249.png" alt="1728787142249">
因为数据量太大，暂时下了十张左右的图像</p>
<figure>
<img src="/2024/10/12/crop_classification/Crop%20classification/10%E6%9C%88%2012%E6%97%A5%E5%91%A8%E6%8A%A5/1728787171883.png" alt="1728787171883">
<figcaption aria-hidden="true">1728787171883</figcaption>
</figure>
<p>单张图片展示效果如图所示：</p>
<figure>
<img src="/2024/10/12/crop_classification/Crop%20classification/10%E6%9C%88%2012%E6%97%A5%E5%91%A8%E6%8A%A5/1728787992532.png" alt="1728787992532">
<figcaption aria-hidden="true">1728787992532</figcaption>
</figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz">ALTNT</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz/2024/10/12/crop_classification/Crop%20classification/10%E6%9C%88%2012%E6%97%A5%E5%91%A8%E6%8A%A5/">http://blog.705553939.xyz/2024/10/12/crop_classification/Crop%20classification/10%E6%9C%88%2012%E6%97%A5%E5%91%A8%E6%8A%A5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/altnt.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/10/10/crop_classification/2021-CVPRSelf-Supervised-RSE-Representations/" title="“Self-Supervised Learning of Remote Sensing Scene Representations Using Contrastive Multiview Coding&quot;"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">“Self-Supervised Learning of Remote Sensing Scene Representations Using Contrastive Multiview Coding&quot;</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/altnt.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ALTNT</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">57</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ALTNT"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87"><span class="toc-number">1.</span> <span class="toc-text">阅读论文：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E8%83%8C%E6%99%AF"><span class="toc-number">1.1.</span> <span class="toc-text">问题背景</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E6%8B%9F%E5%90%88%E5%92%8C%E5%9F%BA%E4%BA%8E%E7%A9%BA%E9%97%B4%E5%A1%AB%E5%85%85%E7%9A%84%E6%96%B9%E6%B3%95%E6%9D%A5%E8%87%AA%E5%90%8C%E4%B8%80%E4%BC%A0%E6%84%9F%E5%99%A8%E7%9A%84%E6%97%B6%E9%97%B4%E6%88%96%E7%A9%BA%E9%97%B4%E4%BF%A1%E6%81%AF%E5%8E%BB%E5%A1%AB%E5%85%85%E7%BC%BA%E5%A4%B1%E7%9A%84%E4%BF%A1%E6%81%AF"><span class="toc-number">1.1.1.</span> <span class="toc-text">（1)基于时间（拟合）和基于空间（填充）的方法来自同一传感器的时间或空间信息去填充缺失的信息</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A1%AB%E5%85%85%E7%9A%84%E6%96%B9%E6%B3%95%E4%BE%9D%E8%B5%96%E7%A9%BA%E9%97%B4%E7%9A%84%E7%9B%B8%E5%85%B3%E6%80%A7"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">1、填充的方法（依赖空间的相关性)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8B%9F%E5%90%88%E7%9A%84%E6%96%B9%E6%B3%95%E4%BE%9D%E8%B5%96%E6%97%B6%E9%97%B4%E7%9A%84%E7%9B%B8%E5%85%B3%E6%80%A7"><span class="toc-number">1.1.1.2.</span> <span class="toc-text">2、拟合的方法（依赖时间的相关性)</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%99%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%E7%9A%84%E6%8C%91%E6%88%98"><span class="toc-number">1.1.2.</span> <span class="toc-text">这两种方法的挑战</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%9E%8D%E5%90%88%E7%AD%96%E7%95%A5"><span class="toc-number">1.1.3.</span> <span class="toc-text">（2)融合策略</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95-------%E4%BE%9D%E8%B5%96%E5%8F%82%E8%80%83%E6%97%A5%E6%9C%9F%E7%9A%84%E8%87%B3%E5%B0%91%E4%B8%80%E5%AF%B9%E6%97%A0%E4%BA%91%E7%9A%84%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E5%92%8C%E4%BD%8E%E5%88%86%E8%BE%A8%E7%8E%87%E5%9B%BE%E5%83%8F%E5%92%8C%E7%9B%AE%E6%A0%87%E6%97%A5%E6%9C%9F%E7%9A%84%E4%BD%8E%E5%88%86%E8%BE%A8%E7%8E%87%E5%9B%BE%E5%83%8F"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">1、传统融合方法-------依赖参考日期的至少一对无云的高分辨率和低分辨率图像和目标日期的低分辨率图像</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">2、基于深度学习的方法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E7%BA%A6%E6%9D%9F%E8%9E%8D%E5%90%88%E6%A8%A1%E5%9D%97---recof"><span class="toc-number">1.2.1.</span> <span class="toc-text">残差约束融合模块 - ReCoF</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">公式推导</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E6%89%80%E4%BD%BF%E7%94%A8%E5%85%AC%E5%BC%8F%E7%9A%84%E6%9C%80%E7%BB%88%E6%8E%A8%E5%AF%BC%E7%BB%93%E6%9E%9C%E5%BD%A2%E5%BC%8F"><span class="toc-number">1.2.1.1.1.</span> <span class="toc-text">（方法所使用公式的最终推导结果形式)</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%8F%82%E6%95%B0-a-%E7%9A%84%E8%8E%B7%E5%8F%96%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E9%A2%84%E5%85%88%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.2.1.1.2.</span> <span class="toc-text">（参数 a
的获取——可以直接预先设置)</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE-varepsilon%E7%9A%84%E8%AE%A1%E7%AE%97%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E6%9D%A5%E8%AE%A1%E7%AE%97%E4%BB%96"><span class="toc-number">1.2.1.1.3.</span> <span class="toc-text">（残差 \(\varepsilon\)的计算————使用深度学习模型来计算他)</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%96%B9%E7%A8%8B11%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="toc-number">1.2.1.1.4.</span> <span class="toc-text">（方程（11）的解释)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#savitzkygolaysg%E6%BB%A4%E6%B3%A2%E5%99%A8%E6%8B%9F%E5%90%88"><span class="toc-number">1.2.2.</span> <span class="toc-text">Savitzky–Golay，SG滤波器——拟合</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.3.</span> <span class="toc-text">总结</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%9D%E8%80%83"><span class="toc-number">1.3.1.</span> <span class="toc-text">思考</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E6%96%B9%E9%9D%A2"><span class="toc-number">2.</span> <span class="toc-text">代码方面</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#cacm%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%B6%E4%BD%9C"><span class="toc-number">2.1.</span> <span class="toc-text">1、CACM进行数据集制作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BE%8E%E5%9B%BD%E5%8C%BA%E5%9F%9F%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%B6%E4%BD%9C"><span class="toc-number">2.2.</span> <span class="toc-text">2、美国区域数据集制作</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E6%A0%BC%E5%BC%8F%E8%AF%B4%E6%98%8E"><span class="toc-number">2.2.1.</span> <span class="toc-text">图片格式说明：</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/12/crop_classification/Crop%20classification/10%E6%9C%88%2012%E6%97%A5%E5%91%A8%E6%8A%A5/" title="Untitled">Untitled</a><time datetime="2024-10-12T13:13:57.974Z" title="Created 2024-10-12 21:13:57">2024-10-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/10/crop_classification/2021-CVPRSelf-Supervised-RSE-Representations/" title="“Self-Supervised Learning of Remote Sensing Scene Representations Using Contrastive Multiview Coding&quot;">“Self-Supervised Learning of Remote Sensing Scene Representations Using Contrastive Multiview Coding&quot;</a><time datetime="2024-10-10T04:45:21.206Z" title="Created 2024-10-10 12:45:21">2024-10-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/08/crop_classification/2019-rse-TempCNNs/" title="Temporal Convolutional Neural Network for the Classification of Satellite Image">Temporal Convolutional Neural Network for the Classification of Satellite Image</a><time datetime="2024-10-08T08:15:32.730Z" title="Created 2024-10-08 16:15:32">2024-10-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/07/crop_classification/10%E6%9C%88%207%20%E6%97%A5%E5%91%A8%E6%8A%A5/" title="“周报10月 7 日&quot;">“周报10月 7 日&quot;</a><time datetime="2024-10-07T12:31:56.907Z" title="Created 2024-10-07 20:31:56">2024-10-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/10/07/crop_classification/Document_how_to_produce_CACM_datasets/" title="记录如何生产 CACM 数据集过程">记录如何生产 CACM 数据集过程</a><time datetime="2024-10-07T03:35:47.057Z" title="Created 2024-10-07 11:35:47">2024-10-07</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By ALTNT</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>