<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Diffusion Policy | ALTNT's Hexo Blog</title><meta name="author" content="ALTNT"><meta name="copyright" content="ALTNT"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Diffusion Policy: Visuomotor Policy Learning via Action Diffusion输入是各种感知信息（比如相机拍到的视频，还有机器人各个关节的位置），而输出的是机器人要执行的动作 Diffusion Policy是一种新型机器人行为生成方法（Robot Action Generation），将机器人的视觉动作策略（Visuomotor Policy）">
<meta property="og:type" content="article">
<meta property="og:title" content="Diffusion Policy">
<meta property="og:url" content="http://blog.705553939.xyz/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/index.html">
<meta property="og:site_name" content="ALTNT&#39;s Hexo Blog">
<meta property="og:description" content="Diffusion Policy: Visuomotor Policy Learning via Action Diffusion输入是各种感知信息（比如相机拍到的视频，还有机器人各个关节的位置），而输出的是机器人要执行的动作 Diffusion Policy是一种新型机器人行为生成方法（Robot Action Generation），将机器人的视觉动作策略（Visuomotor Policy）">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://blog.705553939.xyz/img/altnt.jpeg">
<meta property="article:published_time" content="2024-06-05T15:46:21.537Z">
<meta property="article:modified_time" content="2024-06-05T15:46:21.539Z">
<meta property="article:author" content="ALTNT">
<meta property="article:tag" content="Ros机器人">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.705553939.xyz/img/altnt.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://blog.705553939.xyz/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Diffusion Policy',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-05 23:46:21'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/altnt.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="ALTNT's Hexo Blog"><span class="site-name">ALTNT's Hexo Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Diffusion Policy</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-06-05T15:46:21.537Z" title="Created 2024-06-05 23:46:21">2024-06-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-06-05T15:46:21.539Z" title="Updated 2024-06-05 23:46:21">2024-06-05</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Diffusion Policy"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Diffusion-Policy-Visuomotor-Policy-Learning-via-Action-Diffusion"><a href="#Diffusion-Policy-Visuomotor-Policy-Learning-via-Action-Diffusion" class="headerlink" title="Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"></a>Diffusion Policy: Visuomotor Policy Learning via Action Diffusion</h1><p>输入是各种感知信息（比如相机拍到的视频，还有机器人各个关节的位置），而输出的是机器人要执行的动作</p>
<p>Diffusion Policy是一种新型机器人行为生成方法（Robot Action Generation），将机器人的视觉动作策略（Visuomotor Policy）表示为条件去噪扩散过程（Conditional Denoising Diffusion Process）。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714101035927.png" alt="1714101035927"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714101187640.png" alt="1714101187640"></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>**扩散策略是一种新的机器人行为生成方法，它将机器人的视觉运动策略表示为一个条件去噪扩散过程。<br>**</p>
<p>扩散策略学习动作-分布评分函数的梯度，并在推理过程中通过一系列随机朗之万动力学步骤对该梯度场进行迭代优化。</p>
<p>我们发现扩散公式在用于机器人策略时具有强大的优势，包括优雅地处理多模态动作分布，适用于高维动作空间，并表现出令人印象深刻的训练稳定性</p>
<p>为了充分发挥扩散模型在物理机器人视觉运动策略学习中的潜力，本文提出了一系列关键的技术贡献，包括结合后退horizon control、视觉调节和时间序列扩散transformer  (the incorporation of receding horizon control,visual conditioning, and the time-series diffusion transformer)。</p>
<p>关键词:模仿学习;视觉运动策略;操纵</p>
<h2 id="1介绍"><a href="#1介绍" class="headerlink" title="1介绍"></a>1介绍</h2><p>从演示中进行策略学习，其最简单的形式可以表述为学习 <strong>将观察映射到行动</strong>的 监督回归任务。</p>
<p>然而，在实践中，与其他监督学习问题相比，预测机器人行为的独特性-例如多模态分布的存在，顺序相关性和高精度的要求-使得这项任务与众不同且具有挑战性。</p>
<p>先前的工作试图通过<strong>探索不同的动作表示</strong>(图1a)来解决这一挑战——使用混合的gaussian（ Mandlekar等人(2021)）、量化动作（Shafiullah等人(2022)）的分类表示，或者通过将策略表示(图1b)从显式转换为隐式，以更好地捕获多模态分布Florence等人(2021);Wu et al(2020)。(介绍以前的动作表示方法的论文)</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714057682528.png" alt="1714057682528"></p>
<p>图1所示。policy 表示。</p>
<p>a)具有不同类型<strong>动作</strong>表示的显式策略。</p>
<p>b)隐式策略学习以<strong>行动</strong>和<strong>观察</strong>为条件的energy function，并优化最小化energy landscape的行动</p>
<p>c)扩散策略通过学习的梯度场将噪声细化为行动。该公式提供了稳定的训练，允许学习策略准确地建模多模态动作分布，并适应高维动作序列。</p>
<p>在这项工作中，我们试图通过引入一种新形式的机器人视觉运动策略来解决这一挑战，该策略通过“机器人动作空间上的条件去噪扩散过程Ho等人(2020)”扩散策略来生成行为。</p>
<p>在<strong>这个公式中，策略不是直接输出一个动作，而是根据视觉观察推断K次去噪迭代的动作得分梯度(图1c)。这个公式允许机器人策略继承扩散模型的几个关键属性——即显著提高性能。</strong></p>
<p>•<strong>表达多模态动作分布</strong>。通过学习动作得分函数（Song和Ermon(2019)）的梯度，并对该梯度场进行Stochastic Langevin动态采样，扩散策略可以表示任意的归一化分布 （Neal等人(2011)），其中包括多模态动作分布，这是策略学习的一个众所周知的挑战。</p>
<p>•高维输出空间。正如他们令人印象深刻的图像生成结果所证明的那样，扩散模型在高维输出空间中表现出了出色的可扩展性。此属性允许策略共同推断未来操作的序列，而不是单步操作，这对于鼓励时间操作一致性和避免短视规划至关重要。</p>
<p>•稳定的训练。训练基于能量的策略通常需要负采样来估计难以处理的归一化常数，这已知会导致训练不稳定性（Du et al (2020);Florence等人(2021)）。扩散策略通过学习能量函数的梯度来绕过这一要求，从而在保持分布表达性的同时实现稳定的训练。</p>
<p>我们的主要贡献是将上述优势引入机器人领域，并在复杂的现实世界机器人操作任务中展示其有效性。为了成功地将扩散模型应用于视觉运动策略学习，我们提出了以下技术贡献，以提高扩散策略的性能并释放其在物理机器人上的全部潜力:</p>
<p>•闭环动作序列。我们将策略预测高维动作序列的能力与后退水平控制相结合，以实现鲁棒执行。这种设计允许策略以闭环的方式不断地重新规划其行动，同时保持时间行动的一致性——在长期规划和响应性之间实现平衡。</p>
<p>•Visual conditioning.。我们引入了vision conditioned diffusion policy，其中<strong>视觉观察被视为条件</strong>而不是联合数据分布的一部分。在该公式中，无论去噪迭代如何，策略都只提取一次视觉表示，这大大减少了计算量并实现了实时动作推理。（<strong>重点看下这个视觉观察被视为条件怎么实现的，这个只取一次视觉表示，难道以前会取多次？</strong>）</p>
<p>•时序扩散transformer。我们提出了一种新的基于transformer的扩散网络，它最大限度地减少了典型的基于cnn的模型的过度平滑效应，并在需要高频动作变化和速度控制的任务上实现了最先进的性能。（”Over-smoothing”是卷积神经网络（CNN-based models）中可能出现的一种现象，它通常指的是当网络层过多时，模型的每一层都会对输入数据进行平滑处理，导致特征之间的区分度逐渐丧失。这种效应可能会导致模型在深层网络中难以保持有效的特征表示，尤其是在处理图像、图形和其他高维数据时。）</p>
<p>Transformer模型采用了一种不同于传统CNN模型的架构，这使得它能够在一定程度上缓解或解决过度平滑的问题。以下是Transformer模型的一些关键特点，这些特点帮助它避免了CNN模型中常见的过度平滑效应：</p>
<ol>
<li>自注意力机制（Self-Attention）：Transformer模型的核心是自注意力机制，它允许模型在处理序列的任何部分时直接查看序列中的其他部分。这种机制使得模型能够捕捉到长距离依赖关系，而不需要通过多层网络传递信息，从而减少了信息在传递过程中的损失。</li>
<li>全连接层：与CNN使用局部卷积核不同，Transformer模型通过自注意力机制和全连接层处理整个输入序列，这样可以保留更多的全局信息，并减少信息在层与层之间传递时的损耗。</li>
<li>残差连接（Residual Connections）：Transformer模型中广泛使用残差连接，这有助于梯度直接流过网络，从而减少梯度消失问题，并允许模型学习到更加复杂的函数映射。</li>
<li>层标准化（Layer Normalization）：Transformer模型中的层标准化有助于稳定训练过程，它在每一层的输出上进行标准化操作，有助于保持训练过程中的数值稳定性。</li>
<li>可伸缩性（Scalability）：由于自注意力机制的并行性，Transformer模型可以更有效地处理大型数据集和长序列数据，而不会像深层CNN模型那样受到过度平滑的影响。</li>
</ol>
<p>总体而言，Transformer模型通过自注意力机制和残差连接等设计，有效地保持了不同输入之间的区分度，同时避免了信息在深层网络中的丢失，这使得它在处理序列数据时比传统的CNN模型表现更好。</p>
<h2 id="difussion-policy-formulation"><a href="#difussion-policy-formulation" class="headerlink" title="difussion policy formulation"></a>difussion policy formulation</h2><p>我们将视觉运动机器人策略制定为去噪扩散概率模型  ((ddpm) Ho等(2020))。</p>
<h3 id="2-1扩散概率模型去噪"><a href="#2-1扩散概率模型去噪" class="headerlink" title="2.1扩散概率模型去噪"></a>2.1扩散概率模型去噪</h3><p>ddpm是一类生成模型，其中输出生成建模为去噪过程，通常称为随机朗格万动力学(Welling and Teh, 2011)。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714105104195.png" alt="1714105104195"></p>
<p>也可以表示成：<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714105152950.png" alt="1714105152950"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714105280029.png" alt="1714105280029"></p>
<p>选择α， γ，σ作为迭代步长k的函数，也称为噪声调度，可以解释为梯度体面过程的学习率调度。略小于1的α已被证明可以提高稳定性(Ho et al(2020))。</p>
<p>有关噪音时间表的详情将于第3.3节讨论。</p>
<h3 id="2-2DDPM-Training"><a href="#2-2DDPM-Training" class="headerlink" title="2.2DDPM Training"></a>2.2DDPM Training</h3><p>训练过程首先从数据集中随机抽取未修改的样本x0。对于每个样本，我们随机选择一个去噪迭代k，然后对迭代k采样一个具有适当方差的随机噪声ε k。</p>
<p>噪声预测网络被要求从加了噪声的数据样本中预测噪声。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714105591513.png" alt="1714105591513"></p>
<p>如Ho et al(2020)所示，最小化Eq 3中的损失函数也最小化了使用Eq 1的数据分布p(x 0)与从DDPM q(x 0)抽取的样本分布之间的kl -散度的变分下界。</p>
<h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><h3 id="3-1-Network-Architecture-Options"><a href="#3-1-Network-Architecture-Options" class="headerlink" title="3.1 Network Architecture Options"></a>3.1 Network Architecture Options</h3><p>第一个设计决策是选择εθ的神经网络结构。在这项工作中，我们研究了两种常见的网络架构类型，卷积神经网络(cnn) Ronneberger等人(2015)和Transformers Vaswani等人(2017)，并比较了它们的性能和训练特征。请注意，噪声预测网络εθ的选择与视觉编码器无关，这将在第3.2节中描述</p>
<h4 id="CNN-based-Diffusion-Policy"><a href="#CNN-based-Diffusion-Policy" class="headerlink" title="CNN-based Diffusion Policy"></a>CNN-based Diffusion Policy</h4><p>我们采用Janner等人(2022b)的一维时间CNN，并进行了一些修改:</p>
<p>首先，我们仅通过使用Feature-wise Linear Modulation (FiLM 特征线性调制？)（4 Perez等人(2018)）以及去噪迭代k来调节观察特征<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714106015972.png" alt="1714106015972">上的动作生成过程，从而对条件分布p(At |Ot)进行建模，如图2 (b)所示。</p>
<p>其次，我们仅预测动作轨迹，而不是预测拼接的观察动作轨迹。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714105908721.png" alt="1714105908721"></p>
<p>在实践中，我们发现基于cnn的主干在大多数开箱即用的任务上工作得很好，而不需要太多的超参数调优。</p>
<p>然而，当期望的动作序列随时间快速而剧烈地变化时(例如速度命令动作空间)，它的性能很差，这可能是由于时间卷积的归纳偏差更倾向于低频信号(Tancik et al(2020))。</p>
<h4 id="Time-series-diffusion-transformer"><a href="#Time-series-diffusion-transformer" class="headerlink" title="Time-series diffusion transformer"></a>Time-series diffusion transformer</h4><p>为了减少CNN模型Tancik等人(2020)的过平滑效应，我们引入了一种新的基于transformer的DDPM，该模型采用了minGPT Shafiullah等人的transformer架构(2022)进行行动预测。</p>
<p>带有噪声<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714106415792.png" alt="1714106415792">的动作作为transformer解码器块的输入tokens传入，扩散迭代k的正弦embedding作为第一个令牌。通过共享MLP将观测值<img src="https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/yuanyuan/iCloudDrive/%E8%AF%BE%E4%BB%B6%E5%8F%8A%E4%BD%9C%E4%B8%9A/learning-notes/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714106015972.png" alt="1714106015972">转化为观测值嵌入序列，作为输入特征传递到变压器译码器堆栈中。“梯度”εθ (Ot,At k, k)由解码器堆栈的每个相应输出令牌预测</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714108254416.png" alt="1714108254416"></p>
<p>在我们的基于状态的实验中，大多数性能最好的策略都是用transformer主干网实现的，特别是在任务复杂性和动作变化率很高的情况下。</p>
<p>然而，我们发现transformer对超参数更敏感。Liu等人(2020)的transformer训练困难并不是扩散策略所独有的，未来可能会通过改进transformer训练技术或增加数据规模来解决。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714106557306.png" alt="1714106557306"></p>
<h4 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h4><p>一般来说，我们建议从基于cnn的扩散策略实现开始，作为新任务的第一次尝试。如果由于任务复杂性或高速率动作变化而导致性能低下，则可以使用Time-series Diffusion Transformer公式来潜在地提高性能，但要付出额外调优的代价</p>
<h3 id="3-2-Visual-Encoder"><a href="#3-2-Visual-Encoder" class="headerlink" title="3.2 Visual Encoder"></a>3.2 Visual Encoder</h3><p>视觉编码器将原始图像序列映射到潜在embedding Ot中，并使用扩散策略进行端到端训练。</p>
<p>不同的摄像机视图使用单独的编码器，每个时间步长的图像独立编码，然后连接形成Ot。</p>
<p>我们使用标准的ResNet-18(未经预训练)作为编码器，并进行了以下修改:</p>
<p>1)用空间softmax池替换全局平均池以保持空间信息(Mandlekar et al(2021))。</p>
<p>2)将BatchNorm替换为GroupNorm Wu and He(2018)进行稳定训练。当将归一化层与Exponential Moving Average结合使用时，这一点很重要(He et al.2020)(常用于ddpm)。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714108254416.png" alt="1714108254416"></p>
<h3 id="3-3-Noise-Schedule"><a href="#3-3-Noise-Schedule" class="headerlink" title="3.3 Noise Schedule"></a>3.3 Noise Schedule</h3><p>由σ、α、γ和加性高斯噪声<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714108899983.png" alt="1714108899983">作为k的函数定义的noise schedule已经得到了积极的研究(Ho et al. (2020); Nichol and Dhariwal (2021))。</p>
<p><strong>潜在的noise schedule控制   扩散策略捕捉动作信号的高频和低频特征的    程度</strong>。</p>
<p>在我们的控制任务中，我们经验地发现iDDPM Nichol和Dhariwal(2021)中提出的平方余弦时间表最适合我们的任务。</p>
<h3 id="3-4-Accelerating-Inference-for-Real-time-Control-实时控制加速推理"><a href="#3-4-Accelerating-Inference-for-Real-time-Control-实时控制加速推理" class="headerlink" title="3.4 Accelerating Inference for Real-time Control   实时控制加速推理"></a>3.4 Accelerating Inference for Real-time Control   实时控制加速推理</h3><p>我们使用扩散过程作为机器人的策略;因此，快速的推理速度对闭环实时控制至关重要。  (????这两句有必然联系？)</p>
<p><strong>去噪扩散隐式模型(The Denoising Diffusion Implicit Models ，DDIM)方法Song等人(2021)将训练和推理中的去噪迭代次数解耦，从而允许算法使用更少的迭代进行推理，从而加快过程。</strong></p>
<p>在我们的实际实验中，使用DDIM进行100次训练迭代和10次推理迭代，在Nvidia 3080 GPU上实现0.1s的推理延迟。</p>
<h2 id="4-扩散策略的4个有趣性质"><a href="#4-扩散策略的4个有趣性质" class="headerlink" title="4 扩散策略的4个有趣性质"></a>4 扩散策略的4个有趣性质</h2><h3 id="4-1模型多模态动作分布"><a href="#4-1模型多模态动作分布" class="headerlink" title="4.1模型多模态动作分布"></a>4.1模型多模态动作分布</h3><p>直观地说，扩散策略的多模态行为产生有两个来源——<strong>潜在的随机抽样过程和随机初始化</strong>(an underlying stochastic sampling procedure and a stochastic initialization))。</p>
<p>在随机朗之万动力学(Stochastic Langevin Dynamics)中，在每个采样过程开始时从标准高斯中提取(drawn)初始样本<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714124253330.png" alt="1714124253330">，这有助于为最终动作预测<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714124348747.png" alt="1714124348747">指定(specify)不同可能的收敛convergence basins。</p>
<p>然后，该动作进一步随机优化，在大量迭代中添加高斯扰动(Gaussian perturbations)，这使得单个动作样本能够收敛并在不同的多模态动作盆地(different multi-modal action basins)之间移动。(啥意思啊？？？？)</p>
<p>图3显示了平面推送任务(Push T，下面介绍)中扩散策略的多模态行为示例，没有对测试场景进行明确演示。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714124457490.png" alt="1714124457490"></p>
<h3 id="4-2-Synergy-with-Position-Control与位置控制协同"><a href="#4-2-Synergy-with-Position-Control与位置控制协同" class="headerlink" title="4.2 Synergy with Position Control与位置控制协同"></a>4.2 Synergy with Position Control与位置控制协同</h3><p>我们发现具有位置控制动作空间的扩散策略始终优于具有速度控制velocity control的扩散策略，如图4所示。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714147061957.png" alt="1714147061957"></p>
<p>Figure 4. Velocity速度 v.s. Position Control位置控制. 从速度控制切换到位置控制时的性能差异。<br>在BCRNN和BET性能下降的同时，扩散策略能够利用位置优势，提高其性能</p>
<p>这一令人惊讶的结果与最近大多数依赖于速度控制的行为克隆工作形成了鲜明对比,推测这种差异有两个主要原因:</p>
<p>首先，动作多模态在位置控制模式下比在速度控制模式下更为明显。由于扩散策略比现有方法更好地表达了行动多模态，我们推测它本质上比现有方法受这一缺陷的影响更小。</p>
<p>此外，与速度控制相比，位置控制<strong>受复合误差影响较小</strong>，因此更适合于动作序列预测(如下节所述)。</p>
<ul>
<li><strong>误</strong>差<strong>累</strong>积**:** <strong>在</strong>速<strong>度</strong>控<strong>制</strong>中<strong>，</strong>每<strong>个</strong>动<strong>作</strong>都<strong>会</strong>影<strong>响</strong>下<strong>一</strong>个<strong>动</strong>作<strong>的</strong>起<strong>点</strong>，<strong>因</strong>此<strong>误</strong>差<strong>会</strong>随<strong>着</strong>时<strong>间</strong>推<strong>移</strong>而<strong>累</strong>积<strong>。</strong>而<strong>在</strong>位<strong>置</strong>控<strong>制</strong>中<strong>，</strong>每<strong>个</strong>动<strong>作</strong>都<strong>是</strong>独<strong>立</strong>的<strong>，</strong>因<strong>此</strong>误<strong>差</strong>不<strong>会</strong>累<strong>积</strong>。<strong>这</strong>使<strong>得</strong>位<strong>置</strong>控<strong>制</strong>更<strong>适</strong>合<strong>于</strong>预<strong>测</strong>长<strong>序</strong>列<strong>的</strong>动<strong>作</strong>。</li>
</ul>
<p>因此，扩散策略既不受位置控制的主要缺点的影响，又能更好地利用位置控制的优点。</p>
<p>注：</p>
<p><strong>位</strong>置<strong>控</strong>制** <strong>(<strong>P</strong>o</strong>s<strong>i</strong>t<strong>i</strong>o<strong>n</strong> <strong>C</strong>o<strong>n</strong>t<strong>r</strong>o<strong>l</strong>)<strong>:</strong>**</p>
<ul>
<li><p><strong>目</strong>标**:** <strong>控</strong>制<strong>机</strong>械<strong>系</strong>统<strong>或</strong>机<strong>器</strong>人<strong>末</strong>端<strong>执</strong>行<strong>器</strong>达<strong>到</strong>预<strong>定</strong>的<strong>目</strong>标<strong>位</strong>置<strong>。</strong></p>
</li>
<li><p><strong>控</strong>制<strong>方</strong>式**:** <strong>系</strong>统<strong>直</strong>接<strong>接</strong>收<strong>目</strong>标<strong>位</strong>置<strong>指</strong>令<strong>，</strong>并<strong>通</strong>过<strong>反</strong>馈<strong>机</strong>制 <strong>(<strong>如</strong>编</strong>码<strong>器</strong>) <strong>测</strong>量<strong>当</strong>前<strong>位</strong>置<strong>。</strong>控<strong>制</strong>器<strong>根</strong>据<strong>目</strong>标<strong>位</strong>置<strong>和</strong>当<strong>前</strong>位<strong>置</strong>之<strong>间</strong>的<strong>误</strong>差<strong>计</strong>算<strong>出</strong>所<strong>需</strong>的<strong>控</strong>制<strong>信</strong>号 <strong>(<strong>例</strong>如</strong>，<strong>电</strong>机<strong>转</strong>矩<strong>或</strong>力**)<strong>，</strong>以<strong>驱</strong>动<strong>系</strong>统<strong>向</strong>目<strong>标</strong>位<strong>置</strong>移<strong>动</strong>。**</p>
</li>
<li><p><strong>优</strong>点**:**</p>
<ul>
<li><strong>简</strong>单<strong>直</strong>观<strong>，</strong>易<strong>于</strong>理<strong>解</strong>和<strong>实</strong>现<strong>。</strong></li>
<li><strong>可</strong>以<strong>实</strong>现<strong>精</strong>确<strong>的</strong>位<strong>置</strong>控<strong>制</strong>，<strong>尤</strong>其<strong>适</strong>用<strong>于</strong>需<strong>要</strong>高<strong>精</strong>度<strong>的</strong>任<strong>务</strong>。</li>
</ul>
</li>
<li><p><strong>缺</strong>点**:**</p>
<ul>
<li><strong>可</strong>能<strong>导</strong>致<strong>运</strong>动<strong>轨</strong>迹<strong>不</strong>平<strong>滑</strong>，<strong>因</strong>为<strong>系</strong>统<strong>只</strong>关<strong>注</strong>最<strong>终</strong>位<strong>置</strong>，<strong>而</strong>忽<strong>略</strong>了<strong>中</strong>间<strong>过</strong>程<strong>。</strong></li>
<li><strong>对</strong>扰<strong>动</strong>和<strong>模</strong>型<strong>误</strong>差<strong>敏</strong>感<strong>，</strong>需<strong>要</strong>精<strong>确</strong>的<strong>模</strong>型<strong>和</strong>校<strong>准</strong>。</li>
</ul>
</li>
</ul>
<p><strong>速</strong>度<strong>控</strong>制 <strong>(<strong>V</strong>e</strong>l<strong>o</strong>c<strong>i</strong>t<strong>y</strong> <strong>C</strong>o<strong>n</strong>t<strong>r</strong>o<strong>l</strong>)<strong>:</strong></p>
<ul>
<li><p><strong>目</strong>标**:** <strong>控</strong>制<strong>机</strong>械<strong>系</strong>统<strong>或</strong>机<strong>器</strong>人<strong>末</strong>端<strong>执</strong>行<strong>器</strong>的<strong>运</strong>动<strong>速</strong>度<strong>。</strong></p>
</li>
<li><p><strong>控</strong>制<strong>方</strong>式**:** <strong>系</strong>统<strong>接</strong>收<strong>目</strong>标<strong>速</strong>度<strong>指</strong>令<strong>，</strong>并<strong>通</strong>过<strong>反</strong>馈<strong>机</strong>制 <strong>(<strong>如</strong>速</strong>度<strong>传</strong>感<strong>器</strong>) <strong>测</strong>量<strong>当</strong>前<strong>速</strong>度<strong>。</strong>控<strong>制</strong>器<strong>根</strong>据<strong>目</strong>标<strong>速</strong>度<strong>和</strong>当<strong>前</strong>速<strong>度</strong>之<strong>间</strong>的<strong>误</strong>差<strong>计</strong>算<strong>出</strong>所<strong>需</strong>的<strong>控</strong>制<strong>信</strong>号 <strong>(<strong>例</strong>如</strong>，<strong>电</strong>机<strong>电</strong>压<strong>或</strong>电<strong>流</strong>)<strong>，</strong>以<strong>驱</strong>动<strong>系</strong>统<strong>达</strong>到<strong>目</strong>标<strong>速</strong>度<strong>。</strong></p>
</li>
<li><p><strong>优</strong>点**:**</p>
<ul>
<li><strong>可</strong>以<strong>实</strong>现<strong>平</strong>滑<strong>的</strong>运<strong>动</strong>轨<strong>迹</strong>，<strong>因</strong>为<strong>系</strong>统<strong>关</strong>注<strong>速</strong>度<strong>变</strong>化<strong>。</strong></li>
<li><strong>对</strong>扰<strong>动</strong>和<strong>模</strong>型<strong>误</strong>差<strong>不</strong>太<strong>敏</strong>感<strong>，</strong>比<strong>位</strong>置<strong>控</strong>制<strong>更</strong>鲁<strong>棒</strong>。</li>
</ul>
</li>
<li><p><strong>缺</strong>点**:**</p>
<ul>
<li><strong>位</strong>置<strong>控</strong>制<strong>精</strong>度<strong>不</strong>如<strong>位</strong>置<strong>控</strong>制<strong>，</strong>因<strong>为</strong>系<strong>统</strong>只<strong>控</strong>制<strong>速</strong>度<strong>，</strong>而<strong>没</strong>有<strong>直</strong>接<strong>控</strong>制<strong>位</strong>置<strong>。</strong></li>
<li><strong>需</strong>要<strong>额</strong>外<strong>的</strong>传<strong>感</strong>器<strong>来</strong>测<strong>量</strong>速<strong>度</strong>。</li>
</ul>
<p><strong>应</strong>用<strong>场</strong>景**:**</p>
<ul>
<li><strong>位</strong>置<strong>控</strong>制**:** <strong>适</strong>用<strong>于</strong>需<strong>要</strong>高<strong>精</strong>度<strong>定</strong>位<strong>的</strong>场<strong>景</strong>，<strong>例</strong>如 <strong>C</strong>N<strong>C</strong> <strong>机</strong>床<strong>、</strong>机<strong>器</strong>人<strong>装</strong>配<strong>、</strong>3<strong>D</strong> <strong>打</strong>印<strong>等</strong>。</li>
<li><strong>速</strong>度<strong>控</strong>制**:** <strong>适</strong>用<strong>于</strong>需<strong>要</strong>平<strong>滑</strong>运<strong>动</strong>的<strong>场</strong>景<strong>，</strong>例<strong>如</strong>机<strong>器</strong>人<strong>行</strong>走<strong>、</strong>车<strong>辆</strong>控<strong>制</strong>、<strong>传</strong>送<strong>带</strong>等<strong>。</strong></li>
</ul>
</li>
</ul>
<h3 id="4-3动作序列预测的好处"><a href="#4-3动作序列预测的好处" class="headerlink" title="4.3动作序列预测的好处"></a>4.3动作序列预测的好处</h3><p>之前的研究不使用动作序列预测的原因：</p>
<p>由于难以从高维输出空间(<strong>高</strong>维<strong>输</strong>出<strong>空</strong>间**:** <strong>序</strong>列<strong>预</strong>测<strong>需</strong>要<strong>一</strong>次<strong>性</strong>预<strong>测</strong>整<strong>个</strong>动<strong>作</strong>序<strong>列</strong>，<strong>这</strong>意<strong>味</strong>着<strong>输</strong>出<strong>空</strong>间<strong>的</strong>维<strong>度</strong>非<strong>常</strong>高<strong>。</strong>例<strong>如</strong>，<strong>如</strong>果<strong>需</strong>要<strong>预</strong>测 <strong>1</strong>0 <strong>个</strong>时<strong>间</strong>步<strong>的</strong>机<strong>械</strong>臂<strong>动</strong>作<strong>，</strong>而<strong>每</strong>个<strong>时</strong>间<strong>步</strong>包<strong>含</strong> <strong>7</strong> <strong>个</strong>关<strong>节</strong>角<strong>度</strong>，<strong>则</strong>输<strong>出</strong>空<strong>间</strong>的<strong>维</strong>度<strong>为</strong> <strong>7</strong>0<strong>。</strong>从<strong>如</strong>此<strong>高</strong>维<strong>的</strong>空<strong>间</strong>中<strong>进</strong>行<strong>有</strong>效<strong>的</strong>采<strong>样</strong>是<strong>一</strong>个<strong>挑</strong>战<strong>，</strong>因<strong>为</strong>可<strong>能</strong>的<strong>动</strong>作<strong>序</strong>列<strong>数</strong>量<strong>非</strong>常<strong>庞</strong>大<strong>。</strong>)中有效采样，大多数策略学习方法往往避免了序列预测。例如，IBC在对具有非平滑能量景观的高维动作空间进行有效采样时会遇到困难。(<strong>非</strong>平<strong>滑</strong>能<strong>量</strong>地<strong>形</strong>:**** <strong>在</strong>高<strong>维</strong>动<strong>作</strong>空<strong>间</strong>中<strong>，</strong>目<strong>标</strong>函<strong>数</strong>的<strong>能</strong>量<strong>地</strong>形<strong>可</strong>能<strong>非</strong>常<strong>复</strong>杂<strong>，</strong>存<strong>在</strong>许<strong>多</strong>局<strong>部</strong>最<strong>优</strong>解<strong>和</strong>非<strong>平</strong>滑<strong>区</strong>域<strong>。</strong>这<strong>使</strong>得<strong>基</strong>于<strong>梯</strong>度<strong>的</strong>优<strong>化</strong>方<strong>法</strong>难<strong>以</strong>找<strong>到</strong>全<strong>局</strong>最<strong>优</strong>解<strong>，</strong>也<strong>使</strong>得<strong>采</strong>样<strong>方</strong>法<strong>难</strong>以<strong>有</strong>效<strong>地</strong>探<strong>索</strong>整<strong>个</strong>空<strong>间</strong>。)类似地，BCRNN和BET也难以确定动作分布中存在的模式数(GMM或k-means步长所需要的)。(<strong>多</strong>模<strong>态</strong>分<strong>布</strong>:**** <strong>在</strong>许<strong>多</strong>任<strong>务</strong>中<strong>，</strong>可<strong>能</strong>存<strong>在</strong>多<strong>个</strong>不<strong>同</strong>的<strong>动</strong>作<strong>序</strong>列<strong>可</strong>以<strong>实</strong>现<strong>相</strong>同<strong>或</strong>相<strong>似</strong>的<strong>目</strong>标<strong>。</strong>这<strong>意</strong>味<strong>着</strong>动<strong>作</strong>分<strong>布</strong>具<strong>有</strong>多<strong>个</strong>模<strong>态</strong>。<strong>例</strong>如<strong>，</strong>要<strong>将</strong>一<strong>个</strong>物<strong>体</strong>从 <strong>A</strong> <strong>点</strong>移<strong>动</strong>到 <strong>B</strong> <strong>点</strong>，<strong>可</strong>以<strong>有</strong>多<strong>条</strong>不<strong>同</strong>的<strong>路</strong>径<strong>。</strong>现<strong>有</strong>的<strong>一</strong>些<strong>策</strong>略<strong>学</strong>习<strong>方</strong>法<strong>，</strong>例<strong>如</strong>基<strong>于</strong>高<strong>斯</strong>混<strong>合</strong>模<strong>型</strong> <strong>(<strong>G</strong>M</strong>M**)** <strong>或</strong> <strong>k</strong>-<strong>m</strong>e<strong>a</strong>n<strong>s</strong> <strong>聚</strong>类<strong>的</strong>方<strong>法</strong>，<strong>需</strong>要<strong>预</strong>先<strong>指</strong>定<strong>模</strong>态<strong>数</strong>量<strong>，</strong>这<strong>在</strong>实<strong>际</strong>应<strong>用</strong>中<strong>往</strong>往<strong>难</strong>以<strong>确</strong>定<strong>。</strong>)</p>
<p>相反，DDPM可以很好地扩展输出维度，而不会牺牲模型的表达性，正如许多图像生成应用程序所证明的那样。</p>
<p>利用这种能力，扩散策略以高维动作序列的形式表示动作，它自然地解决了以下问题:</p>
<p>•时间动作一致性:以图3为例。</p>
<p>要将T块从底部推入目标，策略可以从左侧或右侧绕过T块。</p>
<p>然而，假设序列中的每个动作被预测为独立的多模态分布(就像在BCRNN和BET中所做的那样)。在这种情况下，可以从不同的模式中绘制连续的动作，从而导致在两个有效轨迹之间交替的紧张动作。</p>
<p>•对空闲动作的鲁棒性:当演示暂停并导致相同位置动作或接近零速度动作的序列时，会发生空闲动作。</p>
<p>它在远程操作中很常见，有时也需要像液体倾倒这样的任务。然而，单步策略很容易过度适应这种暂停行为。</p>
<p>例如，BC-RNN和IBC经常在现实世界的实验中陷入困境，因为没有明确地从训练中移除空闲动作。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714148799685.png" alt="1714148799685"></p>
<h3 id="4-4-Training-Stability"><a href="#4-4-Training-Stability" class="headerlink" title="4.4 Training Stability"></a>4.4 Training Stability</h3><p>IBC在理论上应该具有与扩散策略相似的优势。然而，由于IBC固有的训练不稳定性，在实践中获得可靠和高性能的IBC结果是具有挑战性的(2022)。</p>
<p>图6显示了整个训练过程中的训练误差峰值和不稳定的评估性能，使得超参数转向变得关键，检查点选择变得困难。因此，Florence等人(2021)评估了每个检查点，并报告了表现最佳的检查点的结果。在实际环境中，此工作流需要对硬件上的许多策略进行评估，以选择最终策略。在这里，我们讨论为什么扩散策略在训练中显得更加稳定</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714149036858.png" alt="1714149036858"></p>
<p>隐式策略使用Energy-Based Model (EBM)表示动作分布:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714149081417.png" alt="1714149081417"></p>
<p>where Z(o,θ) is an intractable normalization constant（难以处理的标准化常数） (with respect to a)</p>
<p>为了训练EBM的隐式策略，使用了InfoNCE-style的损失函数，它等于Eq 6的负对数似然:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714149188951.png" alt="1714149188951"></p>
<p>其中，使用一组负样本<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714149285457.png" alt="1714149285457">来估计难以处理的归一化常数Z(o，θ)。在实践中，已知负采样的不准确性会导致EBM的训练不稳定;等等。(2022)。</p>
<p>通过对Eq 6中相同动作分布的分数函数Song and Ermon(2019)进行建模，扩散策略和ddpm完全回避了Z(a，θ)的估计问题:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714149377759.png" alt="1714149377759"></p>
<p>其中，噪声预测网络εθ (a,o)近似于分数函数∇a log p(a|o)的负值，它与归一化常数Z(o，θ)无关。</p>
<p>因此，扩散策略的推理(Eq 4)和训练(Eq 5)过程都不涉及评估Z(o，θ)，从而使扩散策略的训练更加稳定。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714149501755.png" alt="1714149501755"></p>
<h3 id="4-5与控制理论的联系"><a href="#4-5与控制理论的联系" class="headerlink" title="4.5与控制理论的联系"></a>4.5与控制理论的联系</h3><p>当任务非常简单时，扩散策略具有简单的limiting行为;这可能会让我们从控制理论中得到一些严格的理解。</p>
<p>考虑这样一个情况，我们有一个线性动力系统，在标准状态空间形式，我们希望控制:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714149598077.png" alt="1714149598077"></p>
<p>现在假设我们从线性反馈策略:<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714149627366.png" alt="1714149627366">获得演示(rollout)。</p>
<p>该策略可以通过求解线性二次型调节器等线性最优控制问题来获得。模仿此策略不需要扩散的建模能力，但作为完整性检查，我们可以看到扩散策略做了正确的事情。</p>
<p>特别地，当预测视界为一个时间步长，Tp &#x3D; 1时，可以看出，最优去噪量是最小的</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714234710983.png" alt="1714234710983"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714234730754.png" alt="1714234730754"></p>
<p>其中σk为去噪迭代k的方差。</p>
<p>此外，在推理时，DDIM采样将收敛到<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714234777519.png" alt="1714234777519">处的全局最小值</p>
<p>轨迹预测(Tp &gt; 1)自然随之而来。为了预测<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714234892103.png" alt="1714234892103">是st的函数，最佳去噪器将产生<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714234914791.png" alt="1714234914791">;</p>
<p>所有涉及到wt的项期望都是零。</p>
<p>这表明，为了完美地克隆依赖于状态的行为，学习者必须隐式学习(任务相关)动态模型Subramanian和Mahajan (2019);Zhang et al(2020)。</p>
<p>请注意，如果工厂或政策是非线性的，那么预测未来的行动可能会变得更具挑战性，并且再次涉及多模式预测</p>
<h2 id="5-Evaluation"><a href="#5-Evaluation" class="headerlink" title="5 Evaluation"></a>5 Evaluation</h2><p>我们系统地评估了4个基准中的15个任务的扩散策略(Florence等人(2021);Gupta等人</p>
<p>(2019);Mandlekar et al (2021);Shafiullah et al(2022)。</p>
<p>该评估套件包括模拟和真实环境、单任务和多任务基准、完全驱动和欠驱动系统、刚性和流体对象。我们发现，在所有测试的基准测试中，扩散策略始终优于先前的最先进技术，平均成功率提高了46.9%。在下面的部分中，我们将提供每个任务的概述，我们对该任务的评估方法以及我们的关键要点。</p>
<h3 id="5-1仿真环境和数据集"><a href="#5-1仿真环境和数据集" class="headerlink" title="5.1仿真环境和数据集"></a>5.1仿真环境和数据集</h3><p><strong>Robomimic Mandlekar et al. (2021)       是一个大型机器人操作基准，旨在研究模仿学习和离线强化学习。基准测试包括5个任务，每个任务都有一个精通的人(PH)远程操作演示数据集，其中4个任务(总共9个变体)有精通&#x2F;非精通的人(MH)混合演示数据集。对于每个变体，我们报告了基于状态和基于图像的观测结果。表3总结了每个任务的属性。</strong></p>
<p><strong>Push-T adapted from IBC Florence et al. (2021)</strong>,</p>
<p>需要用圆形末端执行器(蓝色)推动t形块(灰色)到固定目标(红色)。T块和末端执行器的随机初始条件增加了变异。该任务需要利用复杂和接触丰富的对象动力学来精确地推动T块，使用点接触。</p>
<p>有两种变体:一种是RGB图像观测，另一种是从T块的底真位姿获得的9个2D关键点，两者都具有对effeffector位置的本体感觉。</p>
<p><strong>Multimodal Block Pushing adapted from BET Shafiullah et al. (2022)</strong>,</p>
<p>该任务通过推入两个块按使得任意顺序分成两个正方形。来测试策略对多模态动作分布建模的能力</p>
<p>演示数据是由脚本化的oracle生成的，可以访问groundtruth状态信息。这个oracle随机选择一个初始块来推送，并将其移动到一个随机选择的正方形。然后将剩余的块推入剩余的方块中</p>
<p><strong>3. 什么是Diffusion Policy？</strong></p>
<p>Diffusion Policy是一种新型机器人行为生成方法（Robot Action Generation），将机器人的视觉动作策略（Visuomotor Policy）表示为条件去噪扩散过程（Conditional Denoising Diffusion Process）。这是来自论文摘要里的一句介绍，但我想大多数和我一样非专业背景的人看到这段话应该是一脸懵，每个字都认识放在一起就不知道什么意思。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1713977280187.png" alt="1713977280187"></p>
<p>关于什么是Diffusion Policy，迟宬有一段很好的解释：Diffusion Policy其实解决的是一个机器人输出的问题，过往的很多工作大家都注重在解决输入的问题，但机器人最终要执行，我们的工作就在于解决机器人动作和输出的问题。更确切地说，我们的创新聚焦于机器人的动作端而非输入端，在输入端使用的是非常普通的东西。尽管输入端有很多可以提高的地方，但机器人学习方法必须注重输出端，先前的算法在输出端的表现都不够好。因此，无论输入端有多么创新，如果输出端表现不佳，就像”茶壶煮饺子倒不出”一样，将无法发挥潜力。</p>
<p>RSS 2023宋舒然有一段对Diffusion Policy的讲解视频，也非常清晰。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1713977431779.mp4" alt="1713977431779"></p>
<p><strong>Diffusion Policy和Diffusion Model的关系</strong></p>
<p>Diffusion Policy可以理解为Diffusion Model在机器人领域的应用。大家都知道Diffusion Model在图像领域的应用产生了非常好的图像生成模型，比如Stable Diffusion这些，比GAN这类图像生成模型的效果好很多。同样的，Diffusion Policy可以理解为这套Diffusion Model在机器人动作生成领域的应用，尤其是部署到物理机器人领域的动作生成。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1713977512026.png" alt="1713977512026"></p>
<p><strong>4. 为什么用Diffusion Policy？</strong></p>
<p><strong>Diffusion Policy解决的核心问题</strong></p>
<p>从最简单的形式来看，从演示中学习策略可以被表述为学习将观察映射到动作的监督回归任务。然而，在实践中，预测机器人动作有一系列挑战，比如存在多模态分布、时序相关性和训练稳定性的要求。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1713977555135.png" alt="1713977555135"></p>
<p><strong>解决机器人Multi-Modal分布问题</strong></p>
<p>机器人Multi-Modal的问题，简单理解，现实世界中解决某一特定任务的方式是多样的，而不是唯一的。但神经网络预测只能给出单一的方式，无法应对可能有多种方式的任务情况。</p>
<p>关于什么是机器人Multi-Moda问题，迟宬给了非常清晰的解释：假设我现在在开车，前面有一棵树。比如说，我雇佣了100个司机来解决这个问题。在这种情况下，有可能有50个司机选择往左拐，绕过树的左边，还有50个司机选择从树的右边绕过去。在这种情况下，往左绕和往右绕都是完全合理的。然而，当我们将所有这些解决方案合并为一个时，问题就变成了一个多模态分布，即我看到的相同场景有两种不同的选择。这对传统神经网络的预测来说并不友好，因为它通常使用均方误差（MSE）损失进行训练，而这无法有效处理Multi-Modal情况。</p>
<p>前馈神经网络本质上是一个函数，即对于给定输入，只有一种输出。然而，在某些情况下，一个输入可能对应两种不同的输出。这导致了一个问题，即我想要神经网络执行的任务和它能够执行的任务存在冲突。强行让神经网络处理这种情况，通常是通过预测一个中间值，试图最小化与数据之间的距离。但这可能导致不符合预期的行为，例如直接往树上撞。为了解决这个问题，引入了概率分布，使得神经网络不再是一个输入一个输出的函数，而是一个输入可以有多个输出的函数。这种方法提供了更大的灵活性，可以表示各种概率分布，解决了原有方法的限制。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1713977634123.png" alt="1713977634123"></p>
<p>引入概率分布的方法提供了更大的灵活性，让机器人有机会在相同的情况下选择不同的行为，从而避免陷入不可预测的循环。这种非确定性的特性在实际应用中表现出很大的优势，可以防止算法陷入一成不变的状态。但这和传统机器人控制需要对机器人动作严格控制的思路相违背，每次只执行一项任务，整个机器人系统都被认为是受到严格控制的。这也是为什么大多数人没有把机器人动作生成表现为一个概率分布的原因。</p>
<p>但仔细想下人类在决策时也具有一定的非确定性，即在相同情境下可能做出不同选择。比如说，就像我刚才说的，正前方有棵树。你可能今天心情好，你就往右拐；今天心情不好，你就往左拐。这种非确定性对人类来说可能是一种优势，而不是劣势，对机器人同样如此。举例来说，假设我在机器人的算法里有个bug，就是看到一个锥筒，机器人就会绕着它打转。如果我的算法是确定性的，比如说你现在在这条路上开车，我往路上扔一个锥桶，你就开始绕着这个锥桶打转，可能一辈子也开不出来。然而，如果我的机器人预测不是单一模型，而是一个概率分布，有一定的概率它会绕着打转，有一定的概率它会往直走，这样它就能够突破这个无穷打转的情况，最终回到正常的状态。这个性质在实际应用中对提高算法的鲁棒性有很大的影响，它可以防止算法陷入在某些不太熟悉的情况下一直打转的困境。</p>
<p><strong>Action Space Scalability的问题</strong></p>
<p>关于Action Space Scalabiltiy或者sequential correlation问题，我简单理解就是机器人对未来动作的预测不应该只局限于眼前的一步两步动作，而应该更有前瞻性，可以往前预测数十步动作。</p>
<p>针对这个问题，迟宬给了非常清晰的解释：数据预测有两种方法：一是直接输出一个数值，另一种是将可能的数值分成几个区间，进行离散预测。在预测Multi-Modal Action的时候，人们倾向于采用离散预测，将连续值问题转化为分类问题，但这样做涉及的算力成本很高，尤其在处理高维空间时。此外，针对机器人控制问题，如果采用分类方法，每一步都需要预测下一步要执行的动作，而实际需求是希望一次性预测多步动作，这就涉及到了连续控制中的动作一致性问题。解决这个问题的挑战在于平衡成本和对高维连续控制的需求。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1713977763457.png" alt="1713977763457"></p>
<p>假设我们仍然在进行驾驶操作。我们有选择往左或往右拐的能力，将这个过程比喻为方向盘的控制，这是唯一的控制维度。我们将这个控制维度划分成了100个段。接着，我通过神经网络让它在这100个段中选择我们要采取的方向。这种方法理论上是可行的，但问题在于实施的成本非常高昂。如果我们只有一个维度，例如方向盘，那么使用分类是可行的。但假设我要控制的是一个具有六个自由度的机械手，甚至考虑到夹爪开关，有七个自由度，这时如果我要对其进行分类，就不再是在一个维度上切分成100份，而是每个维度都要切分成1000份。然后，将所有这些切分的部分相乘，才能得到我们整个空间的方法。如果采用这种方法，成本将会非常非常高。随着维度的增加，成本会呈指数级增长。</p>
<p>我们再回到了之前提到的问题。假设我们在驾驶车辆，我们可以在下一步稍微往左偏一点，再下一步再进一步左偏。实际上，我们所绘制的行车轨迹有两种可行的选择。一种是持续向左开，从左侧绕过去，一直保持这种路径。另一种是持续向右开，从右侧绕过去。在预测这个动作时，我们可以逐步进行预测，即在每个时刻，预测下一步应该怎么走。然而，采用这种方式可能会导致问题，例如，如果我稍微向左偏了一点，我可能会左右摇摆；如果我稍微向右偏了一点，也有可能左右摇摆。这个问题被称为动作不一致（Action Inconsistent），即当我希望向左行驶时，神经网络的预测中仍然存在一定概率是向右的情况，这时候就会发现决策非常犹豫，时而向左，时而向右，这是一个问题。</p>
<p>在我刚刚提到的分类中，由于它们预测高维空间的成本非常高，因为它们只能预测一步，接下来的步骤是什么。如果再加上更多的步骤，维度就会变得越来越高，它们就无法胜任。然而，实际上我们现在追求的是具有以下特性的方法：不仅可以预测每一步，而且可以在高维连续控制中实现。对于我们来说，我们可以直接预测未来每一步，无论是接下来的20步还是100步，是向左还是向右，而不是在每一步预测之后再执行，再决定下一步该怎么走。</p>
<p><strong>Training Stability问题</strong></p>
<p>Diffusion Policy和其他使用生成式模型的策略比，他的最大特点是训练过程非常稳定。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1713978152791.png" alt="1713978152791"></p>
<p>关于训练稳定性，迟宬的进一步解释是：在Robot Learning领域，机器人动作执行主要有三种方法：包括<strong>直接回归</strong>、<strong>分类预测</strong>和<strong>生成式模型</strong>。</p>
<p>第一类回归，即将神经网络视为一个函数，输入是图片，输出是一个动作。这是最常见的方法，绝大多数强化学习都采用这种方式。然而，这种方法存在一些问题，正如之前提到的。</p>
<p>第二类分类预测，这种方法通过预测分类来生成动作，前文已经大致描述，不再详细赘述。</p>
<p>第三类生成模型，理论上所有的生成模型都可以预测连续的多模态分布，但很多生成模型的问题是训练不稳定。</p>
<p>基于Diffusion Model的第三类方法具有一个重要的优势，即训练非常稳定。这也是为什么Diffusion Model当前在图像生成方面取得了成功，而当时的生成对抗网络（GAN）并没有成功的原因。在当时，GAN在学术界能够产生一些不错的效果，但当你真的将其应用于产品时，你会发现非常困难。要训练一个有效的GAN，你需要疯狂地调整参数，然后才能训练出可用的生成器。</p>
<p>而Diffusion方法的强大之处在于，它的性能不逊色于GAN，但其训练过程非常稳定。基本上，你可以随便调整参数，生成器就能够输出结果，可能效果不是最优的，但基本上都能work。同时，这也解释了为什么像Stable Diffusion这样的方法，以及现在各种图像生成模型能够在如此庞大的数据集上进行训练，这是因为它们的训练非常稳定。如果你在如此大规模的数据上使用其他方法进行训练，可能会在训练一段时间后出现奇怪的问题，模型无法进一步优化。</p>
<p><strong>5. 采访中提到的其他问题</strong></p>
<p><strong>Diffusion Policy和RL以及Imitational Learning是什么关系？</strong></p>
<p>在Robot Learning领域，机器人操作比较常用的两个路径是强化学习（Reinforcement Learning）和模仿学习（Imitation Learning），Diffusion policy并不与强化学习和模仿学习冲突，它可以应用于两者。该方法是一种策略逻辑，适用于输入图像并输出相应动作的情境。在论文中，我们使用了模仿学习，即由人类遥控机器人执行动作，收集数据，并通过学习将其转化为策略。这种学习过程通过遥控机器人完成一系列动作开始，然后将其迁移到机器人身上。输入数据包括过去几帧的图像，而输出涉及对未来动作的预测。</p>
<p>很多强化学习的人，他们使用强化学习在模拟器中生成大量数据。在这个过程中，为了加速训练，RL policy的输入不是图片，而是一些低维度的底层状态信息。但是由于这些状态信息在现实环境里是无法获得的，因此这个RL policy不能直接用于驱动机器人。这个时候，他们会把RL policy生成的数据用于训练一个图片作为输入的模仿学习策略，这被称为蒸馏。在这种情况下，模仿的对象并非人类，而是一个强化学习“代理”（Agent）。这也是这种方法的应用之一。</p>
<p><strong>操作（Manipulation）和移动（Locomotion）的训练有什么不同？</strong></p>
<p>RL在移动有更好的效果，Sim2Real Gap的问题相对好解决；但在操作，RL存在最大的问题是Sim2Real Gap没法很好的解决。对于操控而言，需要考虑的因素较多，其中一个关键区别是在机器人操作中除了需要考虑机器人本身的物理特性，同时还要适应复杂多变的环境和被操作物体。操控涉及与各种各样的物体进行交互，每个物体都具有独特的物理特性，如重心、摩擦力和动力学。这些在模拟器中难以准确模拟，即便能够模拟，精度通常较低，速度较慢。相比之下，对于locomotion，外界环境大多可以视为一个刚体，物理特性基本可以忽略。这使得可以花费更多时间来建立机器人本体的精确物理模型，以及设计更复杂的物理引擎。这是为什么RL更适合Locomotion，而对有物理机器人部署Manipulation没有那么好的效果。</p>
<p><strong>Diffusion Policy目前会存在什么问题，未来有哪些工作？</strong></p>
<p>目前最大的问题不是Policy本身，而是数据。训练数据对于机器人执行特定任务至关重要。尽管我们已经积累了一些关于遥控机器人执行任务的数据，但要将其部署到实际应用中，比如设计一个家用机器人来洗碗，就需要更大规模、更丰富多样的数据集，类似于ChatGPT的规模。确保机器人能够在各种家庭环境中表现出足够的稳健性。目前，最大的挑战在于如何有效地收集大量、多样化的数据。这是下一步研究的关键，通过数据收集和训练，期望能够解决当前面临的问题，同时也认识到可能会有新的挑战随着数据规模的增加而浮现。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714062874734.png" alt="1714062874734"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/1714062874734.png" alt="1714062874734"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz">ALTNT</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/">http://blog.705553939.xyz/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA/">Ros机器人</a></div><div class="post_share"><div class="social-share" data-image="/img/altnt.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/%E4%BA%91%E6%9C%BA%E5%99%A8%E4%BA%BA/" title="云机器人"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">云机器人</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/" title="Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/" title="模仿学习"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-05</div><div class="title">模仿学习</div></div></a></div><div><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/ros%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/ROS%20%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/" title="ROS通信机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-05</div><div class="title">ROS通信机制</div></div></a></div><div><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/%E6%8E%A5%E4%B8%8B%E6%9D%A5%E5%8F%AF%E8%83%BD%E7%9C%8B%E7%9A%84%E8%AE%BA%E6%96%87/" title="接下来可能看的论文"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-05</div><div class="title">接下来可能看的论文</div></div></a></div><div><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/ros%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/2.1%E8%AF%9D%E9%A2%98%E9%80%9A%E4%BF%A1/00%20%E6%A6%82%E5%BF%B5/" title="Ros通信机制-话题通信"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-05</div><div class="title">Ros通信机制-话题通信</div></div></a></div><div><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/ros%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/2.1%E8%AF%9D%E9%A2%98%E9%80%9A%E4%BF%A1/01.base%20example/" title="Ros通信机制-话题通信2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-05</div><div class="title">Ros通信机制-话题通信2</div></div></a></div><div><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/MOMA-Force:%20Visual-Force%20Imitation%20for%20Real-World%20Mobile%20Manipulation/" title="MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-05</div><div class="title">MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/altnt.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ALTNT</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">16</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">7</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ALTNT"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Diffusion-Policy-Visuomotor-Policy-Learning-via-Action-Diffusion"><span class="toc-number">1.</span> <span class="toc-text">Diffusion Policy: Visuomotor Policy Learning via Action Diffusion</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.2.</span> <span class="toc-text">1介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#difussion-policy-formulation"><span class="toc-number">1.3.</span> <span class="toc-text">difussion policy formulation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1%E6%89%A9%E6%95%A3%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B%E5%8E%BB%E5%99%AA"><span class="toc-number">1.3.1.</span> <span class="toc-text">2.1扩散概率模型去噪</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2DDPM-Training"><span class="toc-number">1.3.2.</span> <span class="toc-text">2.2DDPM Training</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.4.</span> <span class="toc-text">具体实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Network-Architecture-Options"><span class="toc-number">1.4.1.</span> <span class="toc-text">3.1 Network Architecture Options</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#CNN-based-Diffusion-Policy"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">CNN-based Diffusion Policy</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Time-series-diffusion-transformer"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">Time-series diffusion transformer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BB%BA%E8%AE%AE"><span class="toc-number">1.4.1.3.</span> <span class="toc-text">建议</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Visual-Encoder"><span class="toc-number">1.4.2.</span> <span class="toc-text">3.2 Visual Encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Noise-Schedule"><span class="toc-number">1.4.3.</span> <span class="toc-text">3.3 Noise Schedule</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-Accelerating-Inference-for-Real-time-Control-%E5%AE%9E%E6%97%B6%E6%8E%A7%E5%88%B6%E5%8A%A0%E9%80%9F%E6%8E%A8%E7%90%86"><span class="toc-number">1.4.4.</span> <span class="toc-text">3.4 Accelerating Inference for Real-time Control   实时控制加速推理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%89%A9%E6%95%A3%E7%AD%96%E7%95%A5%E7%9A%844%E4%B8%AA%E6%9C%89%E8%B6%A3%E6%80%A7%E8%B4%A8"><span class="toc-number">1.5.</span> <span class="toc-text">4 扩散策略的4个有趣性质</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E6%A8%A1%E5%9E%8B%E5%A4%9A%E6%A8%A1%E6%80%81%E5%8A%A8%E4%BD%9C%E5%88%86%E5%B8%83"><span class="toc-number">1.5.1.</span> <span class="toc-text">4.1模型多模态动作分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Synergy-with-Position-Control%E4%B8%8E%E4%BD%8D%E7%BD%AE%E6%8E%A7%E5%88%B6%E5%8D%8F%E5%90%8C"><span class="toc-number">1.5.2.</span> <span class="toc-text">4.2 Synergy with Position Control与位置控制协同</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E5%8A%A8%E4%BD%9C%E5%BA%8F%E5%88%97%E9%A2%84%E6%B5%8B%E7%9A%84%E5%A5%BD%E5%A4%84"><span class="toc-number">1.5.3.</span> <span class="toc-text">4.3动作序列预测的好处</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-Training-Stability"><span class="toc-number">1.5.4.</span> <span class="toc-text">4.4 Training Stability</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5%E4%B8%8E%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA%E7%9A%84%E8%81%94%E7%B3%BB"><span class="toc-number">1.5.5.</span> <span class="toc-text">4.5与控制理论的联系</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Evaluation"><span class="toc-number">1.6.</span> <span class="toc-text">5 Evaluation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1%E4%BB%BF%E7%9C%9F%E7%8E%AF%E5%A2%83%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.6.1.</span> <span class="toc-text">5.1仿真环境和数据集</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5/" title="强化学习相关概念">强化学习相关概念</a><time datetime="2024-06-28T04:03:18.846Z" title="Created 2024-06-28 12:03:18">2024-06-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/" title="机器学习相关概念">机器学习相关概念</a><time datetime="2024-06-26T09:09:56.521Z" title="Created 2024-06-26 17:09:56">2024-06-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/22/%E8%B5%84%E6%96%99/%E5%91%BD%E4%BB%A4/" title="命令">命令</a><time datetime="2024-06-22T03:44:40.000Z" title="Created 2024-06-22 11:44:40">2024-06-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/05/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%AE%BA%E7%AC%94%E8%AE%B0/" title="数据库系统概论笔记">数据库系统概论笔记</a><time datetime="2024-06-05T15:46:23.990Z" title="Created 2024-06-05 23:46:23">2024-06-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/05/hello-world/" title="Hello World">Hello World</a><time datetime="2024-06-05T15:46:21.765Z" title="Created 2024-06-05 23:46:21">2024-06-05</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By ALTNT</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>