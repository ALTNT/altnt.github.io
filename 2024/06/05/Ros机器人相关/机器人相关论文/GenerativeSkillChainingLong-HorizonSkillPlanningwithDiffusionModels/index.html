<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models | ALTNT's Hexo Blog</title><meta name="author" content="ALTNT"><meta name="copyright" content="ALTNT"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models利用扩散模型对长期技能规划 1、摘要Long-horizon tasks, usually characterized by complex subtask dependencies, present a significant challenge">
<meta property="og:type" content="article">
<meta property="og:title" content="Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models">
<meta property="og:url" content="http://blog.705553939.xyz/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/index.html">
<meta property="og:site_name" content="ALTNT&#39;s Hexo Blog">
<meta property="og:description" content="Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models利用扩散模型对长期技能规划 1、摘要Long-horizon tasks, usually characterized by complex subtask dependencies, present a significant challenge">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://blog.705553939.xyz/img/altnt.jpeg">
<meta property="article:published_time" content="2024-06-05T15:46:20.537Z">
<meta property="article:modified_time" content="2024-06-14T08:48:18.461Z">
<meta property="article:author" content="ALTNT">
<meta property="article:tag" content="Ros机器人">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.705553939.xyz/img/altnt.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://blog.705553939.xyz/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-14 16:48:18'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/altnt.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">17</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="ALTNT's Hexo Blog"><span class="site-name">ALTNT's Hexo Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-06-05T15:46:20.537Z" title="Created 2024-06-05 23:46:20">2024-06-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-06-14T08:48:18.461Z" title="Updated 2024-06-14 16:48:18">2024-06-14</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="Generative-Skill-Chaining-Long-Horizon-Skill-Planning-with-Diffusion-Models"><a href="#Generative-Skill-Chaining-Long-Horizon-Skill-Planning-with-Diffusion-Models" class="headerlink" title="Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models"></a>Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models</h2><p>利用扩散模型对长期技能规划</p>
<h2 id="1、摘要"><a href="#1、摘要" class="headerlink" title="1、摘要"></a>1、摘要</h2><p>Long-horizon tasks, usually characterized by complex subtask dependencies, present a significant challenge in manipulation planning. Skill chaining is a practical approach to solving unseen tasks by combining learned skill priors. However, such methods are myopic if sequenced greedily and face scalability issues with search-based planning strategy. To address these challenges, we introduce Generative Skill Chaining (GSC), a probabilistic framework that learns skill-centric diffusion models and composes their learned distributions to generate long-horizon plans during inference. GSC samples from all skill models in parallel to efficiently solve unseen tasks while enforcing geometric constraints. We evaluate the method on various long-horizon tasks and demonstrate its capability in reasoning about action dependencies, constraint handling, and generalization, along with its ability to replan in the face of perturbations. We show results in simulation and on real robot to validate the efficiency and scalability of GSC, highlighting its potential for advancingLong-horizon task planning. More details are available at: <a target="_blank" rel="noopener" href="https://generative-skill-chaining.github.io/">https://generative-skill-chaining.github.io/</a></p>
<ul>
<li><p><strong>长期任务</strong>通常以复杂的子任务依赖性为特征，对操作规划提出了重大挑战。</p>
<p>——————————<strong>长时间范围任务</strong>包含一连串的相互依赖的子任务，每个步骤对后续步骤的成功至关重要。在机器人操控（manipulation）领域，设计一个能够理顺这些复杂依赖关系的计划是非常具有挑战的。</p>
</li>
<li><p><strong>技能链</strong>是一种通过结合学到的技能先验来解决看不见的任务的实用方法。(如何理解?)</p>
</li>
</ul>
<p>———————————<strong>技能链</strong>是解决未知任务的常用方法，通过组合已学习的技能先验来形成针对新任务的解决方案。</p>
<p>**<br>    技能先验**指的是机器人之前学习过的动作或者行为模式。</p>
<ul>
<li>然而，如果贪婪地排序，这样的方法是短视的，并且会面临基于搜索的规划策略的可扩展性问题   (如何理解?)</li>
</ul>
<p>———————————<strong>贪婪序列化与搜索挑战</strong> ：</p>
<pre><code>**   **- 若技能链中的技能是基于贪婪算法（即总是选择当前看起来最佳的选项）进行序列化，可能会变得短视（myopic），因为这种策略不考虑更长远的					任务目标。此外，基于搜索的规划策略在面对规模更大的任务时可扩展性差。

大概就是说: 贪婪序列化是指类似贪心算法,总是找当前最佳的选项,此种方法有短视的特点,不考虑长远的任务目标.

基于搜索的规划策略面对规模越来越复杂的任务可能在性能上无法扩展
</code></pre>
<ul>
<li><p>为了应对这些挑战，我们引入<strong>生成技能链（GSC）</strong>，一种概率框架，用于学习以技能为中心的扩散模型并组合其学习的分布以在推理过程中生成长期计划。GSC 从所有技能模型中<strong>并行采样</strong>，以有效解决看不见的任务，同时<strong>强制实施几何约束</strong>。(对论文提出模型的介绍)</p>
<p>——————————首先,GSC是一种概率框架(什么是概率框架?),它使用技能为中心的扩散模型学习技能，并在推理阶段组合这些技能的分布来生成长时间范围计划。（啊，扩散模型还能这么用？应该是指每个技能单独训练？不理解）</p>
<p>———————————在解决未见任务时，GSC能够并行地从所有技能模型中采样，提高了效率，并且能够强制执行几何约束。(什么意思?)</p>
</li>
<li><p>我们该方法在不同的长时间范围任务上进行了评估，并展示其<strong>推理动作依赖性、约束处理和泛化</strong>的能力，以及<strong>面对扰动时重新计划</strong>的能力。 (测试)</p>
</li>
<li><p>我们展示了仿真和真实机器人的结果，以验证 GSC 的效率和可扩展性，强调其推进长期任务规划的潜力。 (结果)</p>
</li>
</ul>
<p>更多详细信息请访问：<a target="_blank" rel="noopener" href="https://generative-skill-chaining.github.io/">https://generative-skill-chaining.github.io/</a></p>
<hr>
<p>简而言之，GSC通过学习技能集中的行为模型，能够为机器人长时间范围的复杂任务生成有效的协调动作计划，同时解决了以往技能链方法中的短视和扩展性问题。</p>
<h2 id="2、介绍"><a href="#2、介绍" class="headerlink" title="2、介绍"></a>2、介绍</h2><p><strong>长期推理</strong>在解决涉及复杂的步骤间依赖关系的现实世界的操作任务中是至关重要的.</p>
<p>一个说明性的示例如图1（底部），机器人必须推理每个行动选择的长期影响，如对象的放置姿势和如何掌握和使用工具，为了设计一个计划，将满足各种环境约束和最终的任务目标（将对象在机架下）。</p>
<p>然而，寻找一个有效的解决方案通常需要在一个非常大的<strong>规划空间</strong>中搜索，该<strong>空间</strong>随着任务长度呈<strong>指数级扩展</strong></p>
<p>任务和运动规划（TAMP）方法通过联合搜索一系列原始技能（例如，选择、放置和推）及其低级控制参数来解决这些问题。(如何理解?不会是遍历动作和参数吧?)</p>
<p>这些方法虽然有效，但它们需要了解底层的系统状态和环境的运动动力学模型，这使得它们在实际应用中不太实用。（？为什么需要了解就不实用了？）</p>
<p>这项工作寻求开发一种<strong>基于学习</strong>的技能规划方法，以解决长期的操作问题。(<strong>所以本文中还是基于学习的方法</strong>)</p>
<p>( 传统专注于长期任务的学习方法)</p>
<p>先前<strong>专注于长期任务的学习方法</strong>通常采用选项框架（options framework ? 什么东西）</p>
<p>[1,2]，并训练具有原始技能策略的元策略作为其时间扩展的动作空间[3–8]。然而，由此产生的元策略是<strong>特定于任务的</strong>，除了训练任务之外的泛化性有限。</p>
<p><strong>最近的一些工作</strong>转向了<strong>技能水平</strong>的模型，它可以通过测试时间优化来解决新的任务[9-15]。(如何理解?)他们成功的关键是<strong>技能链</strong>，它可以决定每个参数化的技能是否能够导致满足计划中下一个技能的先决条件的状态，并最终实现整个任务的成功。(不理解)</p>
<p>然而，这些方法是有<strong>区别</strong>的，这意味着它们只能<strong>估计</strong>一个给定计划的<strong>可行性</strong>，(只能估计可行性)    并且<strong>需要</strong>一个详尽的搜索过程来解决一个任务。这个瓶颈造成了一个严重的可伸缩性问题处理越来越复杂和漫长的技能序列.(不理解)</p>
<p>在本文中，我们提出了一个生成的和组合的框架，**允许在给定计划框架的情况下直接采样有效的技能链(如何理解?)**。</p>
<p>我们的方法的<strong>关键</strong>是<strong>技能水平概率生成模型</strong>，该模型捕捉每个技能的前提条件-技能参数-效果的联合分布。</p>
<p>对有效的技能链进行采样可以<strong>归结</strong>为，对于计划中的每一项技能，有条件地生成满足下一项技能的先决条件的技能参数和后决条件状态，受起始状态和最终目标的约束。</p>
<p><strong>关键的技术挑战</strong>是确保从初始状态（正向流动）可以实现技能参数序列，以满足长期目标（反向流动）并考虑额外的限制</p>
<p>为此，我们引入了生成技能链（GSC），这是一个<strong>训练个人技能扩散模型的框架</strong>，并在测试时根据   <strong>给定的具有任意约束的看不见的任务骨架</strong>   将其组合。</p>
<p><strong>每种技能都被训练成一个无条件的扩散模型</strong>，学习到的分布被<strong>线性链接</strong>，以在评估过程中解决一个看不见的长期目标。此外，我们使用基于分类器的引导来满足任何指定的约束。GSC使迄今为止用于解决TAMP问题的方法发生了范式转变，并使用概率模型来建立复合性和长期依赖性的原因，而无需接受任何此类任务的培训。我们展示了GSC在三个具有挑战性的操作任务域上的效率，探索了其约束处理优势，并在物理机器人硬件上部署了闭环版本，以显示泛化能力和鲁棒性</p>
<p>为此，我们引入了一种名为Generative Skill Chaining（GSC）的框架，该框架旨在训练<strong>个别的****技能扩散模型</strong>，并根据在测试时给定的<strong>未见任务框架和任意约束</strong>将它们组合起来。</p>
<p><strong>每项技能作为无条件扩散模型进行训练</strong>，所学习的分布被<strong>线性链接</strong>，以在评估期间解决未见的长期目标。此外，我们采用基于<strong>分类器</strong>的指导来满足任何指定的约束。</p>
<p>GSC在迄今为止用于解决任务与运动规划（TAMP）问题的方法中带来了一种范式转变，它使用概率模型建立组成性，并在没有接受过任何此类任务训练的情况下推理长期依赖性。</p>
<p>我们展示了GSC在三个具有挑战性的操作任务领域的效率，探索了其处理约束的优势，并在实体机器人硬件上部署了一个闭环版本，以展示其泛化能力和鲁棒性。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713454898608.png" alt="1713454898608"></p>
<p>图1：（上图）Generative Skill Chaining（GSC）旨在通过使用线性概率链并行地从多个技能特定转换的联合分布 qπ(s, a, s′) 采样来解决给定技能序列的长期任务，这些转换是使用扩散模型学习得到的。该框架在展示灵活的约束处理能力的同时，隐式地考虑了转换的可行性和随后技能的可获得性。</p>
<p>（下图）一个由多项技能组成的长期任务与运动规划（TAMP）问题的例子。这样的任务需要对动作之间的相互依赖性进行推理。</p>
<h2 id="3、相关工作"><a href="#3、相关工作" class="headerlink" title="3、相关工作"></a>3、相关工作</h2><h3 id="Task-and-Motion-Planning"><a href="#Task-and-Motion-Planning" class="headerlink" title="Task and Motion Planning"></a>Task and Motion Planning</h3><p>任务与运动规划</p>
<p><strong>理解顺序动作之间的相互依赖性</strong>是解决长期问题的一个基本挑战。</p>
<p>解决这些问题的<strong>关键思路</strong>是<strong>将规划问题分解为一系列符号上可行的较小子任务[1-3, 7, 8]<strong>，并</strong>以基本动作或技能[4-6]来描述它们的解决方案</strong>。</p>
<p>这样的方法<strong>依赖于</strong>制定完全可观察的<strong>条件</strong>和准确的<strong>系统动态预测</strong>[16, 17]，以<strong>实现</strong>   应用技能（可获得性）及其相应效果的     <strong>前提条件</strong>[18-24]。</p>
<p>在这个方向上，**逻辑几何编程[25, 26]和层次化框架[27]**已经解决了一系列技能的符号上的可行性[18, 28, 29]，这些技能足以从初始状态达到目标条件。</p>
<p>(缺点)尽管这类方法很全面，但它们强大的假设限制了它们的实际应用和可扩展性。</p>
<p>为了克服这一点，我们选择了一个<strong>基于学习</strong>的框架[11, 30]。</p>
<h6 id="解释"><a href="#解释" class="headerlink" title="解释:"></a>解释:</h6><p>这段话是关于 “任务与运动规划”（Task and Motion Planning, TAMP）的问题，它是机器人学和人工智能领域的一个研究领域，关注的是让机器人理解和规划完成一系列动作以达成某个长远目标的策略。这段话的核心信息如下：</p>
<ol>
<li><strong>长远问题的复杂性</strong> ：当面对长期任务时，需要理解一系列动作之间的依赖关系。这是由于动作所导致的状态变化可能会影响后续动作的可行性。</li>
<li><strong>问题分解</strong> ：一个有效的策略是将长期任务分解成一系列较小且符号上可行的子任务，这意味着每个子任务都在某个规划框架内被认为是可以实现的。</li>
<li><strong>动作与技能</strong> ：子任务的解决方案用基本的动作或技能来描述，技能可以视为完成任务的具体步骤或方法。</li>
<li><strong>系统动态和前提条件</strong> ：为了有效地执行这些技能，需要能够完全观察到条件变化，以及准确预测系统的动态（即机器人和环境的相互作用）。这有助于确定何时可以应用技能（即技能的可获得性）以及技能产生的影响。</li>
<li><strong>逻辑几何编程和层次化框架</strong> ：有些解决方案尝试使用逻辑几何编程方法和层次化的框架来处理问题的符号可行性。这些方法旨在确保从初始状态到达目标状态的一系列技能的有效性。</li>
<li><strong>这些方法的限制</strong> ：尽管这些方法详尽且在理论上可行，但在实际中，由于它们对条件和系统动态的强大假设，有时实用性和可扩展性受限。</li>
<li><strong>转向基于学习的框架</strong> ：为了克服上述方法的限制，作者选择了一个基于学习的框架。这种框架不是靠设置假设条件，而是通过机器学习方法让机器人自主学习如何执行任务。</li>
</ol>
<p>总之，这段话讨论了TAMP问题的复杂性，并提出了一种新的学习基础框架来处理长期依赖性和动作顺序，这种框架旨在比传统的、基于假设的方法更加实用和具有可扩展性。</p>
<h3 id="Learning-to-solve-long-horizon-tasks"><a href="#Learning-to-solve-long-horizon-tasks" class="headerlink" title="Learning to solve long-horizon tasks."></a>Learning to solve long-horizon tasks.</h3><p>基于学习解决长期任务</p>
<p><strong>技能链方法</strong>模拟预定义技能的<strong>前置条件</strong>和<strong>后置条件</strong>，以搜索可行的达成目标的计划，但<strong>大多数方法都集中在单一任务设置上</strong>[9-15]。一些近期的方法已经研究了可组合的技能模型，以学习多任务规划器[11, 27, 30]。</p>
<p>然而，这样的方法是判别式的，并且需要进行详尽的搜索。</p>
<p>此外，它们的自回归性质导致了随着任务变长，级联错误和庞大的探索空间。</p>
<p>最近，Agia 等人[11]提出了一种基于 CEM（Cross Entropy Method）的技能链策略，通过<strong>将单独的技能作为强化学习（RL）代理进行训练</strong>，以最大化序列中各个动作的成功率。他们的方法仍然局限于训练出的策略（决定性）先验，缺乏发现多模态解决方案，并且不能考虑额外的规划约束。</p>
<h6 id="解释-1"><a href="#解释-1" class="headerlink" title="解释:"></a>解释:</h6><p>这段话在讨论自动规划系统的一种特定方法，称为技能链（Skill-chaining），这是在学习如何解决长期任务时的一个领域。以下是对这段话内容的解析：</p>
<ol>
<li><strong>技能链方法的基本思想</strong> ：</li>
</ol>
<p>**   <strong>技能链方法是在自动规划系统中的一种技术，它侧重于</strong>模拟技能的前提条件（pre-conditions）和结束条件（post-conditions）**。</p>
<p>这意味着它<strong>定义了</strong>在一项<strong>技能可以开始之前必须满足的条件</strong>，以及<strong>技能完成后所达成的状态</strong>。</p>
<p>系统利用这些信息来搜索一个计划，<strong>这个计划会将目标分解成一系列可以实际执行的技能</strong>。</p>
<ol start="2">
<li><strong>目前研究的局限</strong> ：</li>
</ol>
<p>**   <strong>目前大多数技能链方法</strong>着重研究的是单一任务**，即系统被训练来解决一种特定的问题或一连串固定的任务9-15。</p>
<ol start="3">
<li><strong>多任务规划器</strong> ：</li>
</ol>
<p>**   <strong>近期的研究</strong>转向探究可组合的技能模型<strong>，</strong>目的<strong>是为了</strong>学习<strong>可以处理多种任务的</strong>规划器**11, 27, 30。</p>
<p>这表示<strong>所提出的系统</strong>可以<strong>根据不同的任务需求适当地组合技能，以制定新的计划</strong>。</p>
<ol start="4">
<li><strong>方法的问题</strong> ：</li>
</ol>
<p>**   <strong>不过，这些先进的技能链方法常常是</strong>判别式的（discriminative）**，意味着它们试图直接从数据中区分不同的任务，而不是生成新的任务实例。这些方法通常依赖于详尽搜索，这可能非常耗时且在实际中效率低下。随着任务的增长，它们自回归的特性可能会导致错误累积和庞大的探索空间，这使得难度增加。</p>
<ol start="5">
<li><strong>最新的策略</strong> ：</li>
</ol>
<p>**   <strong>最近，Agia等人11提出了一种基于CEM的技能链策略，其</strong>主要目标<strong>是</strong>通过作为强化学习代理分别训练各个技能，以最大化序列中单个动作的成功率**。</p>
<p>然而，这种方法仍然受限于已训练的策略。这些策略可能是<strong>确定性的</strong>，它们<strong>缺乏解决问题的多模式能力（即找到多种可能的解决方案）</strong>，并且<strong>不能处理规划中可能出现的额外约束。</strong></p>
<p>简而言之，这段话在说明尽管技能链方法在单任务规划上取得了一定进展，但在多任务和能够自适应复杂环境的长期任务规划上仍然存在诸多挑战。当前的方法在扩展性和柔性方面仍有限，而且难以适应新的、未见过的约束条件。</p>
<h3 id="Generative-models-for-planning"><a href="#Generative-models-for-planning" class="headerlink" title="Generative models for planning."></a>Generative models for planning.</h3><p>规划的生成模型</p>
<p>生成模型已被广泛用于高斯过程[31]和对抗网络[32,33]的规划中。</p>
<p>随着最近在机器人学领域基于<strong>扩散模型</strong>的生成策略规划的进步，这样的方法已被用于**模仿学习[34-38]和离线强化学习[39, 40]**设置中。</p>
<p>与本工作最相关的是Diffuser [39]，Decision Diffuser [40]和Diffusion Policy [37]。</p>
<p>虽然它们可以合成“状态”（和&#x2F;或“动作”）序列作为<strong>计划</strong>，并且可以选择计入<strong>约束</strong>[40]，但这样的方法仍然专注于<strong>捕捉训练任务解决方案的分布</strong>，并且<strong>不容易解决未见的新任务</strong>。</p>
<p>在本工作中，我们引入了一种<strong>组合性[41, 42]生成规划方法</strong>，可以灵活<strong>组合技能层面的生成模型</strong>，并在<strong>推理时引入新任务和约束上进行泛化</strong>。</p>
<p>所提出的框架受到了最近在使用<strong>并行扩散</strong>生成<strong>高质量扩展图像链</strong>[43]和<strong>可扩展的带变换器的扩散模型</strong>[44]的工作的<strong>启发</strong>。</p>
<h6 id="解释-2"><a href="#解释-2" class="headerlink" title="解释:"></a>解释:</h6><p>这段话涉及到<strong>生成模型</strong>在<strong>规划任务中的</strong>应用，特别是在自动规划和机器人学领域。下面是对该文本内容的解释：</p>
<ol>
<li><strong>生成模型在规划中的广泛应用</strong> :</li>
</ol>
<p>**   **<strong>生成模型</strong>是一类能够创建数据分布模型的算法，它们在规划任务中的应用很广。(如何理解?)</p>
<p>高斯过程（Gaussian processes）和对抗网络（adversarial networks）是其中两种常用的生成模型方法。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据分布模型 :数据分布模型是一个数学模型，它描述了一组数据点在某些维度上的分布。例如，在自然界中，很多事物的大小、重量、速度等可以用正态分布（高斯分布）来描述，即大多数事物都围绕一个中心值（均值）分布，而远离中心值的事物较少。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">生成模型 ：生成模型是机器学习中的一种模型，它的目标是学习到一个数据集的分布，并能够基于这个分布生成新的数据点。换句话说，生成模型尝试去「理解」数据是如何产生的，这样它就能自己「创造」看起来与真实数据相似的新数据。</span><br><span class="line">生成模型工作的方式通常涉及以下步骤：</span><br><span class="line">观察现实世界数据 ：这可能涉及到自然语言文本、图像、音频等。</span><br><span class="line">学习数据分布 ：模型通过分析这些数据来了解其分布特征。这经常涉及到发现数据中的模式（例如，通过深度学习网络的层来发掘这些模式）。</span><br><span class="line">数据生成 ：一旦模型「学习」了数据的分布，它就可以生成新的数据实例，这些实例应该与实际观察到的数据具有相同或类似的特性。</span><br><span class="line">所以，当说到生成模型是创建数据分布模型的算法时，意思是这种类型的算法能根据已有的数据样本学习其潜在分布，并基于这个学习到的概念来生成新的数据样本。在规划任务中，这意味着生成模型可以用来预测一系列可能的未来状态或动作，这些状态或动作可以作为到达某个目标状态的潜在路径。</span><br></pre></td></tr></table></figure>

<ol start="2">
<li><strong>扩散模型的最新进展</strong> :</li>
</ol>
<p>**   <strong>最近，在机器人学规划中，基于</strong>扩散模型（一个计算过程，模拟如何从一个随机噪声分布生成或重构数据样本的过程）<strong>的生成策略得到了很大的发展，这些方法已经被采用在</strong>模仿学习和离线强化学习**等领域。(什么是扩散模型?)</p>
<ol start="3">
<li><strong>相关的工作</strong> :</li>
</ol>
<p>**   **针对此领域，文中提到了几个相关的工作：Diffuser、Decision Diffuser和Diffusion Policy，它们用于在规划问题中生成状态和动作序列，同时也可以考虑约束。</p>
<ol start="4">
<li><strong>存在的限制</strong> :</li>
</ol>
<p>**   <strong>尽管当前的方法能够生成规划序列，并选择性地考虑约束，但它们</strong>仍然集中在捕捉训练任务解决方案的分布上，并且通常不能轻易地解决新的、未见过的任务**。</p>
<ol start="5">
<li><strong>本工作的贡献</strong> :</li>
</ol>
<p>**   <strong>为了解决上述限制，作者引入了</strong>一种新的*<em>组合性生成规划方法</em>***。这种方法能够灵活地将技能层面的生成模型结合起来，并且能够泛化到在推理时新引入的任务和约束。</p>
<ol start="6">
<li><strong>灵感来源</strong> :</li>
</ol>
<p>**   <strong>提出的框架受到了最近的工作启发，这些工作包括使用</strong>并行扩散生成高质量的图像链<strong>以及开发能够和</strong>Transformers模型一起工作的扩散模型**。</p>
<p>综上所述，作者在这段话中提到了一种新的规划方法，该方法结合了最新的生成模型技术，以期望在自动规划和机器人学领域得到更有效、更灵活的结果，尤其是在处理新任务和新约束方面。</p>
<h2 id="4、准备工作"><a href="#4、准备工作" class="headerlink" title="4、准备工作"></a>4、准备工作</h2><p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748545655.png" alt="1713748545655">技能链骨架</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748418413.png" alt="1713748418413">为技能，或者说<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748523132.png" alt="1713748523132">，o为技能操作的对象集</p>
<p>每个技能π ∈ Π由<strong>一组连续的可行技能参数a ∈ Aπ参数化，所以a为参数，这些</strong>参数<strong>控制着在</strong>状态空间S<strong>中的某个状态s执行技能时所需的</strong>期望运动****</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748638757.png" alt="1713748638757">为当前状态，<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748663784.png" alt="1713748663784">为经过技能π得到后的下一个参数，而这个技能派对应的参数为<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748729021.png" alt="1713748729021">，这些转换遵循<strong>转换模型<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748765988.png" alt="1713748765988"></strong></p>
<h3 id="问题表述"><a href="#问题表述" class="headerlink" title="问题表述"></a>问题表述</h3><p>我们通过考虑给定的符号上可行的<strong>技能链骨架</strong>ΦK&#x3D; {(π1, o1), (π2, o2), . . . , (πK, oK)}来表述技能链接问题，这些<strong>技能</strong>来自<strong>预定义的库π 1:K ∈ Π</strong>以及<strong>技能操作的对象集o</strong>。</p>
<p>每个技能π ∈ Π由<strong>一组连续的可行技能参数a ∈ Aπ参数化</strong>，这些<strong>参数</strong>控制着在<strong>状态空间S</strong>中的某个状态s执行技能时所需的<strong>期望运动</strong>。</p>
<p><strong>目标： 目标</strong>是<strong>找到最优的技能参数</strong>，使得骨架在<strong>几何上是可行的</strong>，并且<strong>每个技能的效果满足后续技能的前提条件，同时满足目标条件。</strong></p>
<h3 id="环境设置"><a href="#环境设置" class="headerlink" title="环境设置"></a>环境设置</h3><p>我们<strong>使用专家策略来解决单个技能</strong>，并收集由执行带有参数aπ∈ Aπ的技能π时，当前状态(s ∈ S)和下一个状态(s′∈ S)之间的<strong>状态-动作-状态转换</strong>，这些转换遵循<strong>转换模型Tπ: S × Aπ→ S。</strong></p>
<p>此外，继之前的研究[11]，我们考虑选择一些基本对象，如钩子、架子以及各种尺寸的盒子，以构建环境设置。</p>
<p>环境的状态包括每个在场对象的完全可观测的姿态和大小。</p>
<h3 id="扩散模型-是一种生成模型，用于模拟和学习数据的分布"><a href="#扩散模型-是一种生成模型，用于模拟和学习数据的分布" class="headerlink" title="扩散模型:(是一种生成模型，用于模拟和学习数据的分布)"></a>扩散模型:(是一种生成模型，用于模拟和学习数据的分布)</h3><p>扩散模型是一个参数化模型pθ(x0)，它使用样本   x0∼ q0(x0)   来估计一个未知分布q(x0)。(即旨在通过观察样本来估计一个未知的数据分布q(x0),模型使用从分布q0(x0)中抽取的样本x0来估计原始分布。)</p>
<p>它由两个扩散过程组成：一个正向增噪过程和一个反向去噪过程。</p>
<p><strong>正向过程</strong>逐渐向来自q0(x0)的样本注入<strong>独立同分布的高斯噪声</strong>，并导致一系列噪声分布**qt(xt)**。(即这个过程将独立同分布的高斯噪声逐步加入到原始数据样本中，这会导致一系列被噪声影响的分布qt(xt))</p>
<p>在<strong>干净数据x0</strong>条件下<strong>xt</strong>的分布也是高斯的：<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713750402854.png" alt="1713750402854">，其中<strong>σt定义了一个固定的噪声水平序列</strong>，<strong>这个序列随着正向扩散t单调递增</strong>。</p>
<p><strong>反向去噪过程</strong>通过迭代移除添加的噪声来恢复干净的数据样本，这个过程可以表示为以下随机微分方程（SDE）[45–47]：(这个公式怎么理解?)</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713496089006.png" alt="1713496089006"></p>
<p>其中∇xlog qt(xt)被称为<strong>噪声分布的score function</strong>，<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713750489352.png" alt="1713750489352">是一个<strong>标准维纳过程</strong>。我们遵循DDPM [48]在连续设置中的采样策略[49]。<strong>得分函数</strong>允许恢复在给定xt的情况下x0的最小均方误差估计器[50, 51]:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713496815575.png" alt="1713496815575"></p>
<p>其中我们可以将˜x0视为在时间t时xt的“去噪”版本。</p>
<p>在实践中，未知的得分函数使用神经网络<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713750597563.png" alt="1713750597563">来估计(这个得分函数给出了在每一步去噪中应该如何调整数据)，通过最小化去噪得分匹配[49]目标<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713498032776.png" alt="1713498032776">来实现，其中λ(t)是一个随时间变化的权重(用于调整不同时间步上的损失函数重要性)。</p>
<p>扩散模型是可扩展的(扩散模型可以扩展到大规模数据集，能够学习复杂的数据分布)，学习到的分布代表了所有满足分布启发式的正样本，因此是多模态的(学习到的分布能够代表所有符合分布启发式的正样本，意味着模型可以捕捉到数据的多种模态或多样性)。</p>
<p>此外，它们简单的概率表示允许广泛的灵活采样策略[47, 48, 52, 53]，结合了约束处理能力[40, 54–56]。(这句话不理解)</p>
<p><strong>简单的概率表示</strong> ：</p>
<p>**   **- 扩散模型具有简单的概率表示（即在扩散模型中，系统的状态变化遵循一定的概率规则，这些规则通常相对简单并且明确定义），这使得它们能够采用多种灵活的采样策略（多种灵活的采样策略指的是，研究者可以根据需要选择不同的方法来生成模型的状态序列，这些方法可能包括不同的随机行走策略、蒙特卡洛方法或其他采样算法。简单的概率表示意味着这些策略可以相对容易地应用于模型，因为状态转移的概率规则不复杂，便于计算和模拟）。</p>
<p><strong>约束处理能力</strong> ：</p>
<p>**   **- 扩散模型还具有处理约束的能力，这意味着它们可以被用来生成满足特定条件或限制的数据样本。</p>
<p>总的来说，扩散模型通过正向过程将噪声注入到数据中，然后通过反向过程尝试恢复出原始的干净数据。这个过程依赖于对噪声分布和干净数据之间关系的学习，以及能够逆转噪声影响的能力。<strong>扩散模型在生成模型领域，尤其是在图像和音频生成中，已经显示出了很高的潜力。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">注:&quot;独立同分布的高斯噪声&quot;（i.i.d. Gaussian noise）是一个统计学中的概念，用于描述随机变量的某种特定分布。让我们分解这个短语来更好地理解它：高斯噪声（Gaussian noise） ：**   **- 高斯噪声，也称为正态噪声，是一种概率分布遵循正态分布（高斯分布）的随机变量。高斯分布是一个对称的钟形曲线，由两个参数决定：均值（mean）和方差（variance）。在自然界和工程应用中，许多随机过程可以用高斯分布来建模。独立同分布（i.i.d.） ：**   **- 独立同分布是指一组随机变量具有两个属性：**     **-  独立性（Independence） ：每个随机变量的取值不受其他变量的影响。即一个变量的结果不会影响到另一个变量的结果。**     **-  同分布性（Identically distributed） ：这些随机变量有相同的概率分布。即它们都遵循同一种分布规律，具有相同的均值和方差。将这些概念放在一起，&quot;独立同分布的高斯噪声&quot;意味着每次加入到数据中的噪声都是一个随机抽取的值，这些值互相之间没有关联（独立），并且每次抽取的噪声值都来自于相同的高斯分布（同分布）。这种噪声模型在很多统计和机器学习算法中都很常见，因为它简化了分析过程，并且很多实际现象都可以用这种模型来近似描述。</span><br><span class="line"></span><br><span class="line">扩散模型通过正向过程将噪声注入数据中，然后通过反向过程恢复出原始干净数据的做法，其核心思想是将复杂的数据生成问题转化为一系列简单的问题。这样做的数学意义和动机可以从以下几个方面理解：</span><br><span class="line">1. 数据分布的学习：</span><br><span class="line">   - 扩散模型的目标是学习复杂数据的分布，这通常很难直接做到。通过逐步引入噪声，我们可以将原始数据分布转化为一系列更简单、更易于建模的分布。</span><br><span class="line"></span><br><span class="line">2. 简化学习过程：</span><br><span class="line">   - 通过正向过程，我们可以逐渐将数据变得更加随机和均匀分布，最终接近简单的高斯分布。在这个过程中，数据的复杂结构和相关性被逐渐“抹平”。</span><br><span class="line"></span><br><span class="line">3. 逆向过程的引导：</span><br><span class="line">   - 一旦数据被转化为高斯分布，反向过程就可以逐步移除噪声，恢复数据的原始结构。在这个过程中，模型学习如何从简单的高斯分布中重建复杂的数据分布。</span><br><span class="line"></span><br><span class="line">4. 生成新的数据样本：</span><br><span class="line">   - 通过掌握从高斯噪声到原始数据的映射，扩散模型可以生成新的数据样本，这些样本看起来和训练数据类似，但却是全新的实例。</span><br><span class="line"></span><br><span class="line">5. 最小均方误差（MMSE）估计：</span><br><span class="line">   - 在反向过程中，扩散模型使用得分函数来指导每一步的去噪。得分函数是关于数据分布的梯度，它可以用来估计最小均方误差，这意味着模型试图找到最接近原始数据的恢复数据。</span><br><span class="line"></span><br><span class="line">6. 控制生成过程：</span><br><span class="line">   - 正向和反向过程的设计允许模型控制生成数据的过程。通过调整噪声的注入和去除，可以精细调整数据样本的生成。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="解释-3"><a href="#解释-3" class="headerlink" title="解释"></a>解释</h4><p>视频1：</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV14c411J7f2/?p=2&spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=69d49a91e3d96bad6e2f1ea1eb1f6c22">https://www.bilibili.com/video/BV14c411J7f2/?p=2&amp;spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=69d49a91e3d96bad6e2f1ea1eb1f6c22</a></p>
<p>原视频：<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ifCDXFdeaaM&t=210s">https://www.youtube.com/watch?v=ifCDXFdeaaM&amp;t=210s</a></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713513330422.png" alt="1713513330422"></p>
<p>Denoise模组内部</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713513462296.png" alt="1713513462296"></p>
<p>那么如何训练Noise Predicter?</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713513601102.png" alt="1713513601102"></p>
<p>那么训练所用的噪音大ground truth是怎么来的呢?————人工创造出来的 ,需要人工生成训练集</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713513714700.png" alt="1713513714700"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713513774978.png" alt="1713513774978"></p>
<p>上面讲的是如何从杂讯图里生成一张图?这是不够的,没有把文字考虑进来</p>
<p>(HW6(homework6的数据集只有7万张,ImageNet有1M张, 而当前最新的stable diffusion等模型使用的LAION数据集有5.85B)</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713514020796.png" alt="1713514020796"></p>
<p>修改后模型输入加入了一个额外的图片的描述信息:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713514288069.png" alt="1713514288069"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713514340852.png" alt="1713514340852"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713514357606.png" alt="1713514357606"></p>
<p>DDPM的完整公式:</p>
<p>（有关ddpm的过程这篇文章也讲的不错，但是有些细节没讲，可以结合下一起看<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/624851115%EF%BC%89">https://zhuanlan.zhihu.com/p/624851115）</a></p>
<p>这篇文章有ddpm的pytorch代码实现<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/563661713">https://zhuanlan.zhihu.com/p/563661713</a></p>
<ul>
<li><a href="https://link.zhihu.com/?target=https://github.com/hojonathanho/diffusion">GitHub - hojonathanho&#x2F;diffusion: Denoising Diffusion Probabilistic Models</a>（官方TensorFlow实现）</li>
<li><a href="https://link.zhihu.com/?target=https://github.com/openai/improved-diffusion">GitHub - openai&#x2F;improved-diffusion: Release for Improved Denoising Diffusion Probabilistic Models</a> （OpenAI基于PyTorch实现的DDPM+）</li>
<li><a href="https://link.zhihu.com/?target=https://github.com/lucidrains/denoising-diffusion-pytorch">GitHub - lucidrains&#x2F;denoising-diffusion-pytorch: Implementation of Denoising Diffusion Probabilistic Model in Pytorch</a></li>
</ul>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713514379493.png" alt="1713514379493"></p>
<p>视频2：</p>
<p>影像生成模型：</p>
<p>stable diffusion:</p>
<p>分为三个module,这三个module一般是分开来训练的,然后最后组合起来的(目前主流的网络模型一般也是这种做法):</p>
<p>1的输出为向量,2的输入红色的为杂讯 <img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713514636026.png" alt="1713514636026"></p>
<p>stable diffusion的具体网络:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713514830624.png" alt="1713514830624"></p>
<p>DALL-E series的具体网络:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713514980135.png" alt="1713514980135"></p>
<p>google的Imagen的做法:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713515109790.png" alt="1713515109790"></p>
<p>现在分module来解释:</p>
<p>首先是第一部分——Text Encoder:</p>
<p>(<strong>训练需要影像和文字成对的资料)</strong></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713515266778.png" alt="1713515266778"></p>
<p>下面图像说明了这个Text Encoder对结果的影响非常大,而图b说明diffusion model的大小反而不那么重要:</p>
<p>(FID的值越小说明生成的图越好,CLIP score是越大越好)</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713515323017.png" alt="1713515323017"></p>
<p>FID的计算（注意fid需要大量图片来计算）:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713516664043.png" alt="1713516664043"></p>
<p>clip score的计算:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713516755292.png" alt="1713516755292"></p>
<p>第二部分是Decoder:</p>
<p>(<strong>训练不需要文字,只需要图片</strong>)</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713516892480.png" alt="1713516892480"></p>
<p>分为以下几种训练方式:</p>
<p>(1)中间产物为小图:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713516974119.png" alt="1713516974119"></p>
<p>(2)中间产物为latent representation(估计是中间向量类似的):</p>
<p>需要训练一个Auto-encoder</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713517122125.png" alt="1713517122125"></p>
<p>最后一部分是生成模型:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713517195313.png" alt="1713517195313"></p>
<p>需要做一点小改变,因为中间产物和noise的大小是不一样的,noise要加到中间产物上:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713517274412.png" alt="1713517274412"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713517349344.png" alt="1713517349344"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713627879637.png" alt="1713627879637"></p>
<p>结合前面可以知道训练Denoise module的过程如下：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713629118095.png" alt="1713629118095"></p>
<p>视频3 ：</p>
<p>difussion model背后的数学原理：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713629351114.png" alt="1713629351114"></p>
<p>VAE与Diffusion Model的类比（很相似，VAE是变分自编码器）</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713629435199.png" alt="1713629435199"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713629548988.png" alt="1713629548988"></p>
<p>首先是训练noise predictor的演算法：</p>
<p>注意模型为<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713711001633.png" alt="1713711001633">，<strong>我们模型的输入是<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713711062674.png" alt="1713711062674">，输出是对应的高斯噪声</strong> <img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713711084984.png" alt="1713711084984"> 。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713629596594.png" alt="1713629596594"></p>
<p>先sample一张干净的图x0</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713629662033.png" alt="1713629662033"></p>
<p>从1~T之间获取一个t</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713629722436.png" alt="1713629722436"></p>
<p>从正太分布（Normal distribution里sample一个noise，大小会与x0一样）</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713629778144.png" alt="1713629778144"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713629826505.png" alt="1713629826505"></p>
<p>下面<strong>红框里的</strong>是在 weighted sumx0与e0加权求和，结果就是一张有噪声的图（后面有讲推导过程，很复杂，但是这个噪声图公式（红框部分）告诉我们<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713710753320.png" alt="1713710753320">）</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713629971224.png" alt="1713629971224"></p>
<pre><code>权重是下面的这些，是事先定好的

![1713629986817](GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713629986817.png)

通常的设计是递减的，显然上面的红框可以看出，t越大，alpha t越小，原图x0占的比例越少，噪音影响越大，

![1713630066640](GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713630066640.png)
</code></pre>
<p>即可以理解为如下：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713630592081.png" alt="1713630592081"></p>
<p>注：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713630781931.png" alt="1713630781931"></p>
<p>红框里的是Noisy image，再往外一层，就是Noise predictor（至于为什么Noise image是公式<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713710175554.png" alt="1713710175554">，下面有推导，非常麻烦，没看懂）</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713630342101.png" alt="1713630342101"></p>
<p>而下面这个是Target Noise</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713630416839.png" alt="1713630416839"></p>
<p>所以整个就是梯度递减的步骤</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713630480908.png" alt="1713630480908"></p>
<p>所以训练就是期望Noise Predicter输入有噪音的图和t的输出与noise相近</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713630642606.png" alt="1713630642606"></p>
<p>所以实际上训练的方法和我们之前的想法是有区别的：</p>
<p>（主要区别是这个t，想象t是作为输入的，实际上t是用来觉得alpha t作为输入）</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713630857727.png" alt="1713630857727"></p>
<p>现在是产生图的过程：</p>
<p>x T是sample的全部是noise的图</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713664317814.png" alt="1713664317814"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713664434494.png" alt="1713664434494"></p>
<p>重复T次， 以xT开始不断生成x T-1, x T-2,…., x0：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713664602959.png" alt="1713664602959"></p>
<p>其中下面的是Noise Predictor生成的Noise图，然后用x t去减去它：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713664702693.png" alt="1713664702693"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713664779864.png" alt="1713664779864">是常数</p>
<p>注意z在t&gt;1时都时sample出来的噪声，当t&#x3D;1时z&#x3D;0</p>
<p>总的过程如下：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713665009881.png" alt="1713665009881"></p>
<p>视频3：</p>
<p>再从头来看影像生成模型的目标，就是从一个一个均匀distribution的集合中选择一部分经过网络后生成跟真实图片越接近越好的图片：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713665437518.png" alt="1713665437518"></p>
<p>更常见的应用：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713665570094.png" alt="1713665570094"></p>
<p>那怎么去衡量越接近越好?:</p>
<p>最大似然估计的思想，从无数<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713666165420.png" alt="1713666165420">网络中去找最好的网络<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713666136428.png" alt="1713666136428">，使得产生最接近真实图片x1.。。。。xm的几率最高</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713666085441.png" alt="1713666085441"></p>
<p>下面的公式“约等于”实际理解就是不断从Pdata里面爬取图片出来，爬的再多，经过我们做好的网络<img src="https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/yuanyuan/iCloudDrive/%E8%AF%BE%E4%BB%B6%E5%8F%8A%E4%BD%9C%E4%B8%9A/learning-notes/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713666136428.png" alt="1713666136428">，最接近真实图片x1.。。。。xm的几率还是最高</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713666525658.png" alt="1713666525658"></p>
<p>下面红圈里的项是新加的，没什么意思，大小只跟世界上所有的图片有关，但是为了后面好算，不影响整个算式的值</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713666825839.png" alt="1713666825839"></p>
<p>KL（）为KL散度的公式：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713666971170.png" alt="1713666971170"></p>
<p>注：</p>
<p>KL散度，全称是Kullback-Leibler散度，也称为相对熵，是衡量两个概率分布P和Q差异的一种方法。它是信息论中的一个概念，用于描述两个概率分布在统计属性上的不同。KL散度是非对称的，这意味着KL(P||Q)通常不等于KL(Q||P)。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713667314349.png" alt="1713667314349"></p>
<p>在变分自编码器（VAE）中，KL散度用来衡量编码器输出的潜在变量的分布与先验分布（通常假设为标准正态分布）之间的差异。它是VAE损失函数的一部分，用于约束潜在空间的结构，并防止过拟合。通过最小化KL散度，模型学习到的潜在变量的分布被推向先验分布，从而使得潜在空间具有良好的结构性，便于采样和插值。</p>
<p>先不需要理解太多，只要知道这是计算Pdata和Pcita之间的差异，</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713667393670.png" alt="1713667393670"></p>
<p>所以就是要差异越小越好：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713667495991.png" alt="1713667495991"></p>
<p>先讲讲VAE：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713951729971.png" alt="1713951729971"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713951792325.png" alt="1713951792325"></p>
<p>P（）为几率：</p>
<p>全概率公式来计算Pcita （x)</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713667924031.png" alt="1713667924031"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713668112623.png" alt="1713668112623"></p>
<p>下面的正比意思是，生成的图片x跟Mean of Gaussian(z)  （正态分布的分布中心）越接近，x|z的几率就越大（这里假设的是z是确定的，在z条件下，x满足G在均值为G(z)的高斯分布）</p>
<p>在实际计算VAE的时候，无法直接使P cita（x)变大，要提高logP cita(x)的下边界（lower bound）:(<strong>注意下面的Pcita(x)为了方便可能会直接写成p(x)</strong>)</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713672696546.png" alt="1713672696546"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713672742234.png" alt="1713672742234"><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713672777178.png" alt="1713672777178">(<strong>不是，这个公式怎么来的啊？怎么看不懂</strong>)</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713948794614.png" alt="1713948794614"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713950170926.png" alt="1713950170926"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713672947867.png" alt="1713672947867"></p>
<p>唉，感觉有问题，算了，先这么看吧</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713951599035.png" alt="1713951599035"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713952088822.png" alt="1713952088822"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713952538327.png" alt="1713952538327"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713952148813.png" alt="1713952148813"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713952211632.png" alt="1713952211632">.</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713952260494.png" alt="1713952260494"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713952427243.png" alt="1713952427243"></p>
<p>E为期望：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713673006352.png" alt="1713673006352"></p>
<p>q(z|x)就是我们的Encoder</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713673166548.png" alt="1713673166548"></p>
<p>DDPM的Pcita（x)的计算：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713673372382.png" alt="1713673372382"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713673436015.png" alt="1713673436015"></p>
<p>注：</p>
<p>马尔科夫链是一种随机过程，它具有马尔科夫性质，即系统的下一个状态只依赖于当前状态，而与之前的状态无关。这种特性被称为“无记忆性”或“马尔科夫性”。马尔科夫链可以是离散的，也可以是连续的，这里我们主要讨论离散时间的马尔科夫链。</p>
<p>在离散时间的马尔科夫链中，每一步的状态转移都遵循一个固定的概率规则，这些规则通常用状态转移矩阵来表示。状态转移矩阵的每个元素( P_{ij} )代表了系统从状态i转移到状态j的概率。对于任何状态i，从i出发到所有可能状态的转移概率之和必须等于1。</p>
<p>[ sum{j} P{ij} &#x3D; 1 ]</p>
<p>马尔科夫链可以用来模拟各种随机系统，如排队理论中的顾客服务过程、物理学中的粒子运动、经济学中的股票价格变动、语言学中的文字生成等等。马尔科夫链的一个关键概念是稳态分布，如果存在，稳态分布是一个特殊的概率分布，在这个分布下，系统随时间的演化将保持不变。</p>
<p>马尔科夫链的研究有助于我们理解和预测随机过程的长期行为，例如预测某个状态最终会以多大概率出现，或者系统是否会收敛到一个稳定的分布。马尔科夫链是现代概率论和统计学的一个重要工具，在机器学习和数据科学中也有广泛的应用。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713686148164.png" alt="1713686148164"></p>
<p>视频5</p>
<p>如何求<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713686496785.png" alt="1713686496785">？：（我看了下其实就是在推导前面为什么Noise image是公式<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713710175554.png" alt="1713710175554">的推导过程）</p>
<pre><code>![1713630342101](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/yuanyuan/iCloudDrive/%E8%AF%BE%E4%BB%B6%E5%8F%8A%E4%BD%9C%E4%B8%9A/learning-notes/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713630342101.png)
</code></pre>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713686462821.png" alt="1713686462821"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713686425450.png" alt="1713686425450"></p>
<p>可简化，不需要两个sample噪声，只用一个sample噪声：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713686615613.png" alt="1713686615613"></p>
<p>进一步归纳：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713686766163.png" alt="1713686766163"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713687265220.png" alt="1713687265220"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713687685848.png" alt="1713687685848"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713687753652.png" alt="1713687753652"></p>
<p>我们已经知道：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713687774460.png" alt="1713687774460"></p>
<p>对<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713687800453.png" alt="1713687800453">：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713687947388.png" alt="1713687947388"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713687987676.png" alt="1713687987676"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713688016670.png" alt="1713688016670"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713688108663.png" alt="1713688108663"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713688136107.png" alt="1713688136107"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713688243220.png" alt="1713688243220"></p>
<p>这就是前面为什么Noise image是公式<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713710175554.png" alt="1713710175554">的整个推导过程了。</p>
<p>下面是知乎上另一篇文章的推导过程，其实就是上面的：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713711230610.png" alt="1713711230610"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713711249356.png" alt="1713711249356"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713688524840.png" alt="1713688524840"></p>
<p>建议看视频6</p>
<p>diffusion model在语音方面的应用（跟图片类似，没什么好讲的）</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713689193663.png" alt="1713689193663"></p>
<p>diffusion model在文字方面的应用（不把噪音加到文字上，加到word embedding上）</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713689499771.png" alt="1713689499771"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713689561488.png" alt="1713689561488"></p>
<p>另一种思路，不加gaussion noise</p>
<h2 id="5、Methodology方法论"><a href="#5、Methodology方法论" class="headerlink" title="5、Methodology方法论"></a>5、Methodology方法论</h2><p>生成技能链（GSC）为接近具有给定技能框架的长期规划提供了一种新范式。</p>
<p>GSC的<strong>主要目标</strong>是确定<strong>未见任务框架</strong>的<strong>最优技能参数</strong>，使得执行计划能够实现长期目标的同时满足特定于任务的约束。</p>
<p>它引入了<strong>短期转换分布的概率链（probabilistic chaining of distributions of short-horizon transitions）</strong>，以从长期轨迹分布中进行抽样。</p>
<p>此外，该框架在推理时组合个别技能，形成序列级轨迹分布，可以通过<strong>并行扩散</strong>进行抽样，以生成可行的技能参数序列作为规划解决方案。</p>
<p>这与以往工作[11, 30]中广泛使用的基于自回归启发式搜索的方法[17, 57–59]不同。</p>
<p><strong>回忆前面：</strong></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748545655.png" alt="1713748545655">技能链骨架</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748418413.png" alt="1713748418413">为技能，或者说<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748523132.png" alt="1713748523132">，o为技能操作的对象集</p>
<p>每个技能π ∈ Π由<strong>一组连续的可行技能参数a ∈ Aπ参数化，所以a为参数，这些</strong>参数<strong>控制着在</strong>状态空间S<strong>中的某个状态s执行技能时所需的</strong>期望运动****</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748638757.png" alt="1713748638757">为当前状态，<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748663784.png" alt="1713748663784">为经过技能π得到后的下一个参数，而这个技能派对应的参数为<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748729021.png" alt="1713748729021">，这些转换遵循<strong>转换模型<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748765988.png" alt="1713748765988"></strong></p>
<p>附加约束<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713757095834.png" alt="1713757095834"></p>
<p><strong>掩蔽采样得分模型</strong>（masked sampling score model）为<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713754193297.png" alt="1713754193297">和<strong>扩散模型得分函数</strong><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713753195186.png" alt="1713753195186"></p>
<p>我们考虑一个给定的技能（及相关对象）框架Φ，它满足序列在环境中的符号可行性(symbolic feasibility))。</p>
<p><strong>主要目标</strong>是生成状态序列和技能参数（如图2(a)所示），以使最终状态（这里final state<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713753848812.png" alt="1713753848812"></p>
<p>）满足一个目标条件，并导致最后一个技能的成功执行。（这个好理解）</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713499932581.png" alt="1713499932581"></p>
<h3 id="动作原语作为扩散模型"><a href="#动作原语作为扩散模型" class="headerlink" title="动作原语作为扩散模型"></a><strong>动作原语作为扩散模型</strong></h3><p>我们通过在环境中执行时观察到的状态-动作-状态转换的性质来表征各个技能。</p>
<p><strong>每个技能π的操作</strong>然后可以由一个无条件分布<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713753032125.png" alt="1713753032125">来表示。这样的表示同时捕获了技能策略（skill policy)<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713753088463.png" alt="1713753088463">和转换动态分布<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713753123839.png" alt="1713753123839">，并确保它们的一致性。对于技能库中的每个技能π，如图3（右）所示，我们使用提供的每个技能演示数据来<strong>训练一个</strong>   带有Transformer backbone（主干）  的<strong>扩散模型得分函数</strong><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713753195186.png" alt="1713753195186">（注意就是下面的<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713754932647.png" alt="1713754932647">，因为图下面有解释<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713754961006.png" alt="1713754961006">）</p>
<pre><code>![1713753499604](GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713753499604.png)

图：基于 transformer的技能扩散模型。我们使用带噪声的位于扩散step t的状态-动作-状态分布![1713753565774](GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713753565774.png)来获得采样期间相应的ϵθ（上面讲的扩散模型得分函数![1713753195186](GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713753195186.png)）。技能对象顺序取决于感兴趣的对象，并表示为one-hot vectors的集合。
</code></pre>
<p>我们通过一个顺序来表示感兴趣的对象集，该顺序表示场景中对象与技能1的相关性[11]。(例如，环境中有一个钩子(1)、一个盒子(2)和一个架子(3)，则任务对应的对象顺序为:(a)取盒子:[2,1,3]，(b)将盒子放在架子上:[2,3,1])我们还<strong>分别</strong>表示状态和动作的<strong>掩蔽采样得分模型</strong>（masked sampling score model）为<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713754193297.png" alt="1713754193297">。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713500021852.png" alt="1713500021852"></p>
<h3 id="Sequencing-skill-diffusion-models训练技能扩散模型"><a href="#Sequencing-skill-diffusion-models训练技能扩散模型" class="headerlink" title="Sequencing skill diffusion models训练技能扩散模型"></a>Sequencing skill diffusion models训练技能扩散模型</h3><p>以前的工作：为了解决我们寻找满足Φ（<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748545655.png" alt="1713748545655">技能链骨架)  的合适技能转换序列的目标，在以前的工作中主要使用的自回归方法如下:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713755227406.png" alt="1713755227406">     注意：技能策略（skill policy)<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713753088463.png" alt="1713753088463">和转换动态分布<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713753123839.png" alt="1713753123839"></p>
<p>(理解：这个方法通过概率公式pΦ来定义，其中s(0:2)代表一系列状态，a(0:1)代表在这些状态下采取的动作序列。这个公式实际上是一个概率链，它将状态转移和动作选择的概率相乘，来计算整个序列的概率。π1和π2代表两个不同的策略，用于在各自的状态下选择动作。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713798631290.png" alt="1713798631290">表示在满足某些条件 (<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713798681440.png" alt="1713798681440"> ) 的情况下，给定初始状态 ( s{(0)} )，产生一个从状态 ( s{(0)} ) 到状态 ( s{(2)} ) 的状态序列 ( s{(0:2)} ) 和一个从动作 ( a{(0)} ) 到动作 ( a{(1)} ) 的动作序列 ( a_{(0:1)} ) 的<strong>联合概率</strong>。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713798487777.png" alt="1713798487777">表示在策略π1下，给定初始状态 (s{(0)}) 时，选择动作 (a_{(0)}) 的概率。这是一个条件概率，它描述了代理在某个特定状态下根据其策略采取某个动作的可能性。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713798555236.png" alt="1713798555236">表示在执行了动作 (a{(0)}) 之后，从状态 (s{(0)}) 转移到状态 (s{(1)}) 的概率。这个转移概率是由策略 (pi1) 和动作 (a{(0)}) 共同决定的。它反映了环境对代理的动作响应的动态特性。</p>
<p>)</p>
<p>(以前工作的缺陷)然而，这样的公式是短视的，只能在没有来自最终任务目标的反馈的情况下向前推进。这限制了long-horizon推理，并且先前的方法利用随机[30]或CEM-based rollouts[11]从这种分布中抽样。（传统的自回归方法，如前面提到的 ( p_\Phi(s_{(0:2)}, a_{(0:1)}|s_{(0)}) )，通常只考虑当前状态和下一步动作，而不是整个决策序列直到最终目标。这种方法可能不足以捕捉到在整个决策序列中可能出现的复杂依赖关系）</p>
<p>为了克服上述限制，我们将<strong>无条件技能扩散模型（unconditional skill diffusion models）</strong>转化为<strong>正向和反向条件</strong>分布（这样做是为了能够同时考虑到达达成最终任务目标的反馈，并使得模型能够在更长的时间范围内进行推理。）：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713755514406.png" alt="1713755514406"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713755974362.png" alt="1713755974362"></p>
<p>理解：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713800132143.png" alt="1713800132143">描述了在策略 ( pi1 ) 下，从状态 ( s{(0)} ) 采取动作 ( a{(0)} ) 并转移到状态 ( s{(1)} ) 的联合概率。这是一个条件概率分布，它不仅考虑了从 ( s{(0)} ) 到 ( s{(1)} ) 的转移，而且还同时考虑了在这个过程中采取的动作 ( a{(0)} )。这意味着它关联了起始状态、动作和结果状态。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713800161562.png" alt="1713800161562">描述了在策略 ( pi2 ) 下，给定中间状态 ( s{(1)} ) 的情况下，采取动作 ( a{(1)} ) 并转移到状态 ( s{(2)} ) 的条件概率。这个分布专注于从状态 ( s{(1)} ) 到状态 ( s{(2)} ) 的转移，并且是在已知中间状态 ( s{(1)} ) 的条件下计算的。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713800396011.png" alt="1713800396011">是因为：符号 “∝” 表示成正比，即左边的概率分布 ( pPhi(s{(0:2)}, a{(0:1)}|s{(0)}) ) 与右边两个条件概率分布的乘积成正比。这意味着左边的概率分布可以通过对右边的表达式进行归一化（即确保所有概率加起来等于1）来获得。<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713800515014.png" alt="1713800515014">是在给定初始状态 ( s{(0)} ) 的情况下，状态序列 ( s{(0:2)} ) 和动作序列 ( a{(0:1)} ) 的联合概率分布。</p>
<p>将这两个条件概率分布相乘意味着我们考虑了从 ( s{(0)} ) 到 ( s{(1)} ) 的转移和从 ( s{(1)} ) 到 ( s{(2)} ) 的转移的联合概率。这个联合概率是在考虑了中间状态 ( s_{(1)} ) 的情况下的，这样可以更准确地捕获状态转移的动态。</p>
<p>因此，整个表达式<img src="https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/yuanyuan/iCloudDrive/%E8%AF%BE%E4%BB%B6%E5%8F%8A%E4%BD%9C%E4%B8%9A/learning-notes/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713800396011.png" alt="1713800396011">说明了在策略 ( Phi ) 下，从初始状态 ( s{(0)} ) 开始的整个状态和动作序列的概率分布可以通过两个策略 ( pi1 ) 和 ( pi_2 ) 下的条件概率分布的乘积来近似。这种方法可以帮助模型在长期规划问题中进行更好的推理。</p>
<p>在上述两个方程中，这种关系隐含地产生了skill affordability和transition feasibility的概念，即一种技能的结果状态必须存在于下一种技能的初始状态分布中，反之亦然。</p>
<p>理解：qπ1和qπ2代表了新的条件分布，它们不仅考虑了当前和下一个状态之间的关系，还考虑了最终状态与当前状态之间的关系。</p>
<p>分数函数（score function）在统计学和机器学习中指的是概率密度函数（或概率质量函数）对数的梯度。如果我们有一个随机变量 ( X ) 的概率密度函数 ( p(x) )，那么 ( X ) 的分数函数是：</p>
<p><img src="https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/yuanyuan/iCloudDrive/%E8%AF%BE%E4%BB%B6%E5%8F%8A%E4%BD%9C%E4%B8%9A/learning-notes/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713756112579.png" alt="1713756112579"></p>
<p>这里，( \nabla_x ) 表示对 ( x ) 的梯度，而 ( \log p(x) ) 是 ( p(x) ) 的自然对数。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713803897598.png" alt="1713803897598"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713803978341.png" alt="1713803978341"></p>
<p>分数函数在变分推断和相关领域中很有用，因为它与参数的最优化直接相关。在最优化问题中，我们通常想要找到参数的值，使得某个概率分布的期望值最大化或最小化。分数函数是这个过程的一部分，因为它可以帮助我们找到概率分布的极值点。</p>
<p>例如，在强化学习中，分数函数用于策略梯度方法，其中策略（即在给定状态下选择动作的概率分布）是通过其分数函数来优化的。通过计算分数函数，我们可以得到概率分布相对于其参数的最敏感方向，这可以指导我们如何调整参数以提高策略的预期回报。</p>
<p>在逆向扩散过程中，分数函数用于指导随机过程以生成具有特定特性的样本，这是生成模型中的一种常见技术。通过优化这些分数函数，可以使得生成的样本更好地符合目标分布。</p>
<p>现在，如果我们将一个特定的反向扩散采样步骤t的概率转换成它们各自的得分函数（<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713756112579.png" alt="1713756112579">），我们得到:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713756174633.png" alt="1713756174633">（这个公式怎么理解？这里涉及了上面提到的<strong>掩蔽采样得分模型</strong>（masked sampling score model）为<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713754193297.png" alt="1713754193297">和<strong>扩散模型得分函数</strong><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713753195186.png" alt="1713753195186">）</p>
<p>理解：</p>
<p>等式（3）和（4）表明，整个序列的分数函数可以通过两个策略的分数函数相加，然后减去中间状态下的分数函数来获得。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713801395914.png" alt="1713801395914">是整个序列的分数函数。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713801420959.png" alt="1713801420959">是策略 ( pi1 ) 下的分数函数。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713801442322.png" alt="1713801442322">是策略 ( pi2 ) 下的分数函数。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713801462926.png" alt="1713801462926">和<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713801478113.png" alt="1713801478113">分别是在中间状态 <img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713801514832.png" alt="1713801514832"> 下，策略 ( pi2 ) 和 ( pi_1 ) 的分数函数。</p>
<p>最后，我们线性<strong>组合</strong>由依赖因子γ加权的前向和后向分布的<strong>得分函数</strong>:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713756278903.png" alt="1713756278903"></p>
<p>这里，γ∈[0,1]是一个决策变量，它平衡了技能转换过程中状态对后续技能和目标条件的影响。这是一个重要的方面，它操作着序列中的技能行为和它们各自参数的选择。</p>
<p>通过调整γ 的值，我们可以调节技能之间的依赖关系，从而影响整个序列的行为。如果γ接近 1，那么 ( pi1 ) 的影响更大；如果γ 接近 0，那么 ( pi2 ) 的影响更大。这种调整使得模型能够在执行技能序列时更灵活地适应不同的情况和目标。</p>
<h3 id="基于分类器的约束满足指导"><a href="#基于分类器的约束满足指导" class="headerlink" title="基于分类器的约束满足指导"></a>基于分类器的约束满足指导</h3><p>除了最终任务目标之外，<strong>约束constraints</strong> 在控制环境中动作的可行性和指定任务特定条件方面也起着重要作用，例如在中间或最终状态下最大化&#x2F;最小化两个物体之间的距离。</p>
<p>我们基于扩散模型的结构<strong>允许GSC轻松地将附加约束作为隐式(绘制)或显式(基于分类器)指导合并</strong>。</p>
<p>在这里，我们提出了一个灵活的采样策略，在几个规划约束的存在。原则上，附加约束<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713757095834.png" alt="1713757095834">可以作为附加项附加到<strong>目标抽样分布</strong>中:</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713757253506.png" alt="1713757253506"></p>
<p>其中h(·)是约束作用于由<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713757309119.png" alt="1713757309119">给出的一组状态-动作节点的likelihood（可能性）。</p>
<p>加入约束条件后，相应的扩散分数函数为</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713757378125.png" alt="1713757378125"></p>
<p>其中<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713757548776.png" alt="1713757548776"></p>
<p>考虑图2(b)所示的示例，其中约束取决于节点<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713757699764.png" alt="1713757699764">。</p>
<p><img src="https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/yuanyuan/iCloudDrive/%E8%AF%BE%E4%BB%B6%E5%8F%8A%E4%BD%9C%E4%B8%9A/learning-notes/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713757666548.png" alt="1713757666548"></p>
<p>假设选择<strong>约束</strong>作为满意度的二进制指标(例如 success &#x3D; 1)，并在t &#x3D; 0时为去噪样本<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713757797687.png" alt="1713757797687">和<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713757813543.png" alt="1713757813543">)定义约束。在这种情况下，<strong>可能性likelihood</strong> 被定义为约束满足的指数，使得</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713757892217.png" alt="1713757892217"></p>
<p>值得注意的是，虽然约束是去噪样本（denoised samples）的函数，但梯度必须在加噪样本（noised samples）的基础上计算。我们首先从*<em>方程（2）</em>***中获得 扩散步骤t 的去噪样本<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713758641241.png" alt="1713758641241">，<img src="https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/yuanyuan/iCloudDrive/%E8%AF%BE%E4%BB%B6%E5%8F%8A%E4%BD%9C%E4%B8%9A/learning-notes/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713758603785.png" alt="1713758603785"></p>
<p>然后根据权重因子α修改<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713758717440.png" alt="1713758717440">中的相应节点：</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713758783100.png" alt="1713758783100"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>总而言之，所提出的框架GSC分为三个部分:</p>
<p>(1)在不了解任何其他技能的情况下，使用所提出的架构训练<strong>单独的技能扩散模型;</strong></p>
<p>(2)chain skill diffusion models链化技能扩散模型，在推理过程中，根据一个看不见的任务骨架，使用带有依赖因子（dependency factor）的个体学习分布的<strong>概率线性链</strong>的<strong>链化技能扩散模型</strong>;</p>
<p>(3)incorporate classifier-based guidance结合基于分类器的指导，     在推理过程中添加任何看不见的规划约束时，结合基于分类器的指导。</p>
<p>遵循标准的反向去噪，我们考虑从所有个体模型而不是一个模型中并行采样，因此我们提出的方法既与任务骨架无关，也与骨架长度无关。</p>
<p>此外，依赖性因素（dependency factor）有助于做出灵活的设计选择，以满足期望的目标条件。</p>
<p>虽然γ &#x3D; 0.5的恒定值是足够的，但它可以对每个技能进行微调。</p>
<p>除此之外，我们还收集失败数据，为每个技能训练成功概率预测模块<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713759171719.png" alt="1713759171719">，这是在当前和过渡状态下衡量技能成功执行的指标。</p>
<p>该模型用于从采样的候选解中考虑最佳参数序列。我们在附录A中说明了整个算法。</p>
<h2 id="5、结果"><a href="#5、结果" class="headerlink" title="5、结果"></a>5、结果</h2><p>我们通过实验验证了GSC在以下方面的有效性:</p>
<p>(1)对任意长度的未见任务的长期规划，</p>
<p>(2)约束处理和满足，</p>
<p>(3)最大化行动依赖视界，</p>
<p>最后(4)对扰动的泛化。</p>
<p>首先，我们展示了GSC在Toy领域的组合和约束处理性能。其次，我们评估了链式训练技能扩散模型在先前工作[11]引入的9个标准TAMP任务上的性能。这些任务包含范围广泛的骨架长度，并在不同级别的长期依赖关系上对方法提出挑战。</p>
<p>最后，我们讨论了GSC对扰动的响应，然后讨论了依赖因子γ在GSC成功中的重要性。</p>
<h3 id="基线和指标"><a href="#基线和指标" class="headerlink" title="基线和指标"></a>基线和指标</h3><p>在技能链的背景下，我们主要考虑基于搜索的方法与CEM优化策略。</p>
<p>我们的<strong>主要基准</strong>是具有统一先验(Random CEM)和学习策略先验(STAP)的CEM方法。</p>
<p>此外，为了显示与在任务序列上训练相比性能的提高，并期望将其推广到新的任务序列，我们在Agia等人[11]之后添加了DAF[30]的性能。</p>
<p><strong>另一个潜在的基线</strong>是仅对状态使用扩散，对基于Decision Diffuser的动作使用逆动态模型[40]。然而，由于存在相当大的分布偏移和状态预测的级联误差，这种方法的<strong>性能并不好</strong>(参见附录B)。我们使用100次随机环境执行中满足目标条件的成功率作为比较指标。</p>
<h3 id="Toy领域。"><a href="#Toy领域。" class="headerlink" title="Toy领域。"></a>Toy领域。</h3><p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713777218443.png" alt="1713777218443"></p>
<p>二维域由点样本的<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713777096263.png" alt="1713777096263">位置的状态和单位向量(u, v)的方向和幅度(a)的作用<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713777114544.png" alt="1713777114544">组成。????</p>
<h2 id="算法总结"><a href="#算法总结" class="headerlink" title="算法总结"></a>算法总结</h2><p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748545655.png" alt="1713748545655">技能链骨架</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748418413.png" alt="1713748418413">为技能，或者说<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748523132.png" alt="1713748523132">，o为技能操作的对象集</p>
<p>每个技能π ∈ Π由<strong>一组连续的可行技能参数a ∈ Aπ参数化，所以a为参数，这些</strong>参数<strong>控制着在</strong>状态空间S<strong>中的某个状态s执行技能时所需的</strong>期望运动****</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748638757.png" alt="1713748638757">为当前状态，<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748663784.png" alt="1713748663784">为经过技能π得到后的下一个参数，而这个技能派对应的参数为<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748729021.png" alt="1713748729021">，这些转换遵循<strong>转换模型<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713748765988.png" alt="1713748765988"></strong></p>
<p>附加约束<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713757095834.png" alt="1713757095834"></p>
<p><strong>掩蔽采样得分模型</strong>（masked sampling score model）为<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713754193297.png" alt="1713754193297">和<strong>扩散模型得分函数</strong><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713753195186.png" alt="1713753195186"></p>
<h3 id="算法1-生成技能链-GSC-算法"><a href="#算法1-生成技能链-GSC-算法" class="headerlink" title="算法1:生成技能链(GSC)算法"></a>算法1:生成技能链(GSC)算法</h3><p>超参数:</p>
<p>反向扩散step t</p>
<p>正向向后依赖因子 γ</p>
<p>梯度评分函数权值 α</p>
<p>输入:</p>
<p>预定义技能库<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713781196258.png" alt="1713781196258"></p>
<p>individual技能扩散得分函数<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713781226027.png" alt="1713781226027"></p>
<p>任务骨架<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713781334447.png" alt="1713781334447">是长度为K的技能序列</p>
<p>初始状态<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713781423766.png" alt="1713781423766"></p>
<p>目标条件 g</p>
<p>约束<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713781481633.png" alt="1713781481633">：设<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713781516005.png" alt="1713781516005">为受约束影响的节点</p>
<p>初始框架solution：<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713781614101.png" alt="1713781614101">是由<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713781629277.png" alt="1713781629277">采样得到的</p>
<p>初始化t &#x3D; T</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713781884195.png" alt="1713781884195"></p>
<h3 id="初始化："><a href="#初始化：" class="headerlink" title="初始化："></a>初始化：</h3><ol start="10">
<li>xT：从正态分布 N (0,I) 中采样的初始骨架解。</li>
<li>初始化 t 为 T。</li>
</ol>
<h3 id="算法过程："><a href="#算法过程：" class="headerlink" title="算法过程："></a>算法过程：</h3><ol start="12">
<li>当 t ≥ 0 时，进行以下循环：</li>
<li>初始化整个骨架序列的分数为 0。</li>
<li></li>
<li>- <ul>
<li>对于 i &#x3D; 1 到 K，更新 ϵΦ 的子向量，将每个技能的分数函数加到整个骨架的分数上。</li>
</ul>
</li>
<li>对于每个技能 ( pii )，我们计算它在时间步 ( t ) 的分数函数 ( epsilon{pii}(s{(i-1)}^t, a{(i-1)}^t,s{(i)}^t, t) )，然后将这个分数加到整个骨架序列的分数函数 ( epsilonPhi ) 上。这样，我们就可以得到考虑了当前技能 ( pii ) 的整个序列的累积分数。</li>
</ol>
<p>考虑了前向-后向依赖因子 ( gamma )，它是用来平衡前一个技能 ( pii ) 和后一个技能 ( pi{i+1} ) 对当前状态 ( s{(i)}^t ) 的影响。我们从当前的 ( epsilonPhi(s{(i)}^t, t) ) 减去一个加权和，其中包括当前技能 ( pii ) 的分数函数和下一个技能 ( pi_{i+1} ) 的分数函数，这两个分数函数分别乘以 ( gamma ) 和 ( 1 - gamma )。这种更新确保了技能序列中的每个状态都能够平滑地过渡到下一个状态，同时考虑到了技能之间的依赖关系。</p>
<p>简而言之，这些更新步骤是在构建一个整合了所有单个技能分数函数的综合分数函数，这个综合分数函数 ( epsilon_Phi ) 反映了整个任务骨架序列在时间步 ( t ) 的得分。这个得分随后用于指导逆向扩散过程中的样本生成，以便最终产生一个符合约束并满足目标条件的解决方案</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713973497660.png" alt="1713973497660"></p>
<ul>
<li>对于受约束的节点 v，使用梯度下降来更新 ϵΦ 的子向量，以处理约束。</li>
<li>循环每个受约束的节点：对于每个受约束的节点 ( v )，这些节点可以是状态 ( s ) 或动作 ( a )，我们将对它们执行更新。这里的 ( v ) 指的是序列中的受约束状态和动作的集合，例如 ( s^{(1)}, a^{(2)}, s^{(K)} )。</li>
<li>更新 ( epsilon_Phi ) 的子向量：对于每个受约束的节点 ( v )，我们需要更新它在分数函数 ( epsilonPhi ) 中对应的部分。这个更新是通过从当前的分数函数 ( epsilonPhi(v^t, t) ) 中减去一个项来实现的，这个项是约束函数 ( h ) 对数的梯度 ( nabla_{v^t} log h(tilde{s}^{(1)}, tilde{a}^{(2)}, tilde{s}^{(K)}) ) 乘以梯度分数函数权重 ( alpha )。</li>
<li>梯度和权重：这里的 ( nabla_{v^t} ) 表示对 ( v ) 在时间步 ( t ) 的梯度，而 ( log h ) 是约束函数 ( h ) 的对数。这个梯度指示了如何调整 ( v ) 以最大程度地满足约束 ( h )。权重 ( alpha ) 决定了这个调整的强度。、</li>
<li>为什么要进行这个更新？<br>目的是确保生成的技能序列不仅遵循每个技能的内在逻辑（即通过 ( epsilonPhi ) 和 ( epsilon{pi} ) 的更新实现），而且还符合外部施加的约束条件 ( h )。通过在分数函数中考虑这些约束，算法能够指导逆向扩散过程，以便生成的序列在满足目标条件的同时，也不违反任何约束。</li>
<li>总之，这个步骤是对生成过程中的样本进行调整，以确保它们遵守特定的约束规则，从而使最终的技能链既有效又合规。</li>
</ul>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713973643566.png" alt="1713973643566"></p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713973658227.png" alt="1713973658227"></p>
<ul>
<li>生成去噪后的样本 ( tilde{x}_0 )。</li>
<li></li>
</ul>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713973672917.png" alt="1713973672917"></p>
<ul>
<li>获取更新后的带噪样本 ( q0(t-1)(x{t-1}|tilde{x}0) )，这是一个以 ( tilde{x}0 ) 为均值，方差为 ( sigma^2_{t-1} I ) 的正态分布。</li>
<li>t 减 1，进行下一个逆向扩散步骤。</li>
</ul>
<h3 id="结果："><a href="#结果：" class="headerlink" title="结果："></a>结果：</h3><ol start="13">
<li>返回最终的去噪样本 x0。</li>
</ol>
<h3 id="超参数及其计算"><a href="#超参数及其计算" class="headerlink" title="超参数及其计算"></a>超参数及其计算</h3><p>反扩散的时间步数是一个重要的参数，它决定了在保证生成样本质量的前提下完成采样所需的时间。虽然较低的步数减少了采样所花费的时间，但较高的步数可以得到精细去噪的高质量样本。</p>
<p>我们尝试使用多个值(256、128、64和50)，并收敛到对大多数任务使用128个扩散步骤。根据第4节和第5节(图6)中描述的解释，依赖因子γ被设置为0.5。γ &#x3D; 1的值使GSC与一个平凡的策略推出方法相同。</p>
<p>最后，在梯度的情况下，我们对权重进行微调，以平衡反向扩散过程中约束的影响。虽然由于反向过程的复杂性，很难彻底改变采样轨迹，但我们对给定计划约束的所有任务使用α &#x3D; 1</p>
<p>我们遵循DiT[44]的分数-网络架构，并采用他们的开源实现:github.com&#x2F;facebookresearch&#x2F;DiT。我们使用以下超参数来构建分数</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713974964582.png" alt="1713974964582"></p>
<h2 id="额外的讨论"><a href="#额外的讨论" class="headerlink" title="额外的讨论"></a>额外的讨论</h2><h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节:"></a>实现细节:</h3><p>所提出的技能排序框架的性能取决于专家数据集的多样性、动作在未知骨架中的最大水平依赖性以及训练的技能扩散模型的质量。此外，只有真实的分布被估计并用于对候选解进行抽样。为了确保所有任务的高成功概率，我们考虑对<strong>多个候选序列****解决方案</strong>进行采样(其中两个如图7所示)，并考虑基于个人技能成功概率度量的乘积的最佳可能解决方案。</p>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713975051044.png" alt="1713975051044"></p>
<p>图7:对于拾取和放置青色盒子的“拾取”任务，这样它就可以被推入机架中:左是正确采样的状态序列，而右是不正确的。因此，筛选候选解是必要的。</p>
<h3 id="决策扩散器方法-具有逆动力学作用的状态扩散模型。"><a href="#决策扩散器方法-具有逆动力学作用的状态扩散模型。" class="headerlink" title="决策扩散器方法:具有逆动力学作用的状态扩散模型。"></a>决策扩散器方法:具有逆动力学作用的状态扩散模型。</h3><p>扩散模型已被用于机器人的规划。其中一个框架是<strong>决策扩散器</strong>[40]，它对期望的状态轨迹进行采样，并使用逆动力学模型来找到最佳动作序列。我们的框架可以通过从样本中删除action来实现这一点。</p>
<p>然而，这导致扩散模型生成的分布与逆动力学模型给出的作用不相交。这种分布位移对采样状态的质量很敏感，因此会导致级联误差。考虑状态-动作-状态转换的联合分布是有利的，因为它对这种状态扰动不太敏感。</p>
<h3 id="C技能描述和参数化"><a href="#C技能描述和参数化" class="headerlink" title="C技能描述和参数化"></a>C技能描述和参数化</h3><p>我们有一个固定的技能库，包括:捡、放、推和拉。根据以下设置，每个技能都是根据感兴趣的对象参数化的:</p>
<p>pick:参数化的(x, y, z， θ)作为pick的位置和抓手围绕z轴的方向。这些参数是根据感兴趣的(要挑选的)对象的起源计算的。例如，拾取块的位置是块的原点，即质心。同样，对于挑钩，原点是矩形的中心，钩是矩形的一个L-segment。</p>
<p>place:参数为(x, y, z， θ)作为放置位置和夹持器绕z轴的方向。参数是根据感兴趣的对象(被选中的对象将被放置在其上)的原点计算的。</p>
<p>push:参数为(x, y， θ， r)，表示工具(挂钩)在工作台(z &#x3D; 0)上放置的位置和方向，r表示挂钩将被移离臂座的长度。参数x, y， θ是相对于感兴趣的对象(要推送的)原点。推距r是刀具在θ方向上的位置位移。</p>
<p>拉力:参数为(x, y， θ， r)，表示工具(挂钩)在工作台(z &#x3D; 0)上放置的位置和方向，r表示挂钩向臂座移动的长度。参数x, y， θ是相对于感兴趣的对象(要拉的)原点。拉力距离r是刀具在θ方向上的位置位移。</p>
<h2 id="参考知识："><a href="#参考知识：" class="headerlink" title="参考知识："></a>参考知识：</h2><h3 id="蒙特卡洛采样（MCS）https-zhuanlan-zhihu-com-p-81806149"><a href="#蒙特卡洛采样（MCS）https-zhuanlan-zhihu-com-p-81806149" class="headerlink" title="蒙特卡洛采样（MCS）https://zhuanlan.zhihu.com/p/81806149"></a><strong>蒙特卡洛采样（MCS）<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/81806149">https://zhuanlan.zhihu.com/p/81806149</a></strong></h3><h4 id="动机"><a href="#动机" class="headerlink" title="动机"></a><strong>动机</strong></h4><p>机器学习中最常见的优化算法是  <strong>基于梯度的优化方法</strong>，当<strong>目标函数</strong>是一个类似如下结构的随机函数 F(θ) 时：</p>
<p><img src="https://pic2.zhimg.com/80/v2-6c668a149b543ec66ba01e2e91af2ad1_720w.webp"></p>
<p>优化该类目标函数，<strong>最核心的计算问题</strong>是<strong>对随机函数 F(θ) 的梯度进行估计</strong>，即：</p>
<p><img src="https://pic1.zhimg.com/80/v2-55a904bf45438d32830c5074c7f163a0_720w.webp"></p>
<p>随机函数梯度估计在机器学习以及诸多领域都是核心计算问题，比如：<strong>变分推断</strong>，一种常见的近似贝叶斯推断方法；<strong>强化学习中的策略梯度算法</strong>；实验设计中的<strong>贝叶斯优化和主动学习方</strong>法等。</p>
<p>其中，<strong>对于函数期望类目标问题，最常见的是基于蒙特卡洛采样的方法。</strong></p>
<h4 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a><strong>背景知识</strong></h4><p>要了解基于蒙特卡洛采样的梯度估计方法，首先先了解<strong>蒙特卡洛采样方法</strong>和<strong>随机优化方法。</strong></p>
<p>MCS 是一种经典的求解积分方法，公式（1）中的问题通常可以用 MCS 近似求解如下（感觉就是概率论的矩估计法）：</p>
<p><img src="https://pic3.zhimg.com/80/v2-78104e5848ba9eaea261d8469f56966e_720w.webp"></p>
<p>其中， <img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713844936139.png" alt="1713844936139">采样自分布 p(x;θ)，由于采样的不确定性和有限性，这里<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713844959039.png" alt="1713844959039">是一个随机变量，公式（3）是公式（1）的<strong>蒙特卡洛估计器（MCE）</strong>。</p>
<p>这类方法非常适用于求解形式如公式（1）的积分问题，尤其是当分布 p(x;θ) 非常容易进行采样的时候。</p>
<p><strong>在使用 MCE 时，往往关注其以下四个性质：</strong></p>
<ol>
<li>一致性，根据大数定理，当所采样的样本数量非常多时，MCE 的估计值将会收敛到积分的真值处。</li>
<li>无偏性，MCE 是对所求积分的一个无偏估计，简单推导如下：</li>
</ol>
<p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713845061223.png" alt="1713845061223"></p>
<p>MCE 的无偏性是随机优化算法收敛的重要保证。</p>
<ol start="3">
<li>小方差，当几个估计方法都是无偏估计时，我们通常会选择方差较小的 MCE，因为更小方差的 MCE 会估计地更准，从而使得优化地效率更高、准确性更好。</li>
<li>可计算性，很多机器学习问题都是高维问题，如何提高 MCE 的可计算性，比如：减少采样、提高并行能力等变得十分重要。</li>
</ol>
<p><strong>随机优化（SO）</strong></p>
<p><img src="https://pic2.zhimg.com/80/v2-9ae8ce5c6063f6ef0d28da63a0929615_720w.webp"></p>
<p>▲ 图1. 随机优化</p>
<p>如图 1 所示，随机优化问题通常包含两个过程，一是仿真过程，输入优化变量，获得响应值 F(θ)，然后计算出 <img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713845127984.png" alt="1713845127984">，其中是个随机变量 ；二是优化过程，基于梯度，迭代更新优化变量。</p>
<p>不同于确定性优化，<strong>随机优化算法包含两个部分的随机性：</strong></p>
<ul>
<li>仿真过程中，由于系统响应 F(θ) 是随机变量，因此其梯度以及 Hessian 矩阵等都是随机的，需要近似估计；</li>
<li>优化过程中，由于采用一些近似处理手段，比如用 mini batch 来估计梯度会产生随机性。</li>
</ul>
<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a><strong>应用</strong></h4><p>基于蒙特卡洛采样的梯度估计方法（MCGE）在很多研究领域都起到了核心作用，本节总结一下其在机器学习领域中的典型应用。</p>
<h5 id="变分推断（Variational-Inference-VI）"><a href="#变分推断（Variational-Inference-VI）" class="headerlink" title="变分推断（Variational Inference, VI）"></a><strong>变分推断（Variational Inference, VI）</strong></h5><p><img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713845182202.png" alt="1713845182202"></p>
<p>VI 是贝叶斯推断中的一大类方法，在统计机器学习（贝叶斯视角）中具有广泛的应用。从上图中可以看出，变分推断 (VI) 的思想非常简单。假设一个变分分布簇，在概率空间中找到一个离真实分布最近的分布。VI 巧妙地将一个推断问题转化为了优化问题，优化目标是</p>
<p>(Q||P)，即待求分布 Q 和真实后验分布 P 的距离，优化的变量是分布 Q 的描述参数。</p>
<p>VI 方法综述将在另外一篇文章中详细介绍，本文只简单说明其目标函数是一个形如公式（1）的问题。考虑一个生成模型问题 p(z)p(x|z)，其中 z 是隐变量，x 是观测变量，p(z) 是先验分布，p(x|z) 是似然函数。根据贝叶斯公式：</p>
<p><img src="https://pic3.zhimg.com/80/v2-3fb255b75b170577588342327d5f5442_720w.webp"></p>
<p>其中 p(x)&#x3D;ʃp(z)p(z|x)，称为 evidence，通常 p(x) 是一个不可积的多重积分，导致后验分布 p(z|x) 无法获得解析解。如上述思路所述，假设后验分布用一个变分分布 q(z|x;θ) 来近似，通过构造如下优化问题：</p>
<p><img src="https://pic4.zhimg.com/80/v2-31a9eedfa9ffbf7ced619a0047a0412f_720w.webp"></p>
<p>来求解使得两个分布距离最小的变分分布参数 θ，从而得到近似后验分布。</p>
<p>因为真实后验分布是未知的，直接优化公式（6）是一件比较有挑战的事情，VI 巧妙地将其转化为优化 ELBO 的问题。</p>
<p>简单的推导过程如下：</p>
<p><img src="https://pic1.zhimg.com/80/v2-fedc55ba5501cf4c1c8e633b89c4f9cc_720w.webp"></p>
<p>等号两边移动一下可得：</p>
<p><img src="https://pic3.zhimg.com/80/v2-413cba54dadb4a89497e3a295b78a482_720w.webp"></p>
<p>由 KL 散度的定义可知，KL(q(z|x;ф)||p(z|x;θ))≥0，同时 logp(x;θ) 是个常数，所以求优化问题（6）等价于求如下优化问题：</p>
<p><img src="https://pic3.zhimg.com/80/v2-fb8812c91a19e34ca526e8dcfd3ca642_720w.webp"></p>
<p>相当于求解 log evidence lower bound，即 eblo。继续推导如下：</p>
<p><img src="https://pic4.zhimg.com/80/v2-d20e610c98dd8b74bf6aef95a9dbdc37_720w.webp"></p>
<p>公式（10）的形式如公式（1），可以用 MCGE 进行梯度估计，从而优化求解。</p>
<p>变分推断方法是一个热门研究领域，而核心问题是如何高效求解 elbo 优化问题，在统计物理、信息论、贝叶斯推断、机器学习等诸多领域由广泛的应用。</p>
<h5 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a><strong>强化学习</strong></h5><p>强化学习是机器学习中一大类热门研究领域，尤其是 AlphaGo 的横空出世，为强化学习带来了更多的关注和更多的研究人员。本文将不对强化学习的任务和各种概念进行赘述，强化学习中的一大类问题是无模型的策略搜索问题，即通过优化累计回报的均值学习到最优策略。所谓累计回报的均值形式如下：</p>
<p><img src="https://pic1.zhimg.com/80/v2-dc3ce6ad692d6b3e76263e20eefbdd0c_720w.webp"></p>
<p>公式（11）形式亦如公式（1），可以用 MCGE 进行梯度估计，从而优化求解。</p>
<p><strong>实验设计</strong></p>
<p>实验设计是个非常广泛的领域，主要是研究如何为实验设置合适的配置，比如：自动机器学习中的超参数调优（HPO）、神经架构搜索（NAS），通过主动学习（Active Learning）选择更加合适的样本进行标注，老虎机问题的求解（Bandit）等等。</p>
<p>这类任务中经常会遇到一个问题，如何选择下一个更好的配置，使得选择之后比选择之前性能的概率会有所提升。因此需要优化如下问题：</p>
<p><img src="https://pic1.zhimg.com/80/v2-772979b5d4f767cb696a4f888a00a06c_720w.webp"></p>
<p>公式（12）形式亦如公式（1），可以用 MCGE 进行梯度估计，从而优化求解。</p>
<p>简单总结一下，优化是机器学习训练中最重要的部分，而其中很多优化问题都是形如公式（1）的问题，而 MCGE 是解决这类问题的有效手段，接下来介绍两种经典的 MCGE 方法。</p>
<h2 id="方法综述"><a href="#方法综述" class="headerlink" title="方法综述"></a><strong>方法综述</strong></h2><p>公式（1）中的积分内是一个分布和代价函数的乘积，在对其梯度进行近似估计时，可以从两个方面进行求导。由此，<strong>可以将梯度估计方法大致分为两类：</strong></p>
<ul>
<li><strong>求解分布测度的导数</strong> ，包括本文介绍的 score function gradient estimator</li>
<li><strong>求解代价函数的导数</strong> ，包括本文介绍的 pathwise gradient estimator</li>
</ul>
<p>根据公式（2）待估计的梯度是：</p>
<p><img src="https://pic2.zhimg.com/80/v2-a650d880f323ca0ff3b47dd826b4bcd5_720w.webp"></p>
<p>直接计算会非常困难，一个直观的思路是研究如何将期望的梯度转化为梯度的期望，从而可以利用 MCS 做无偏近似估计。本文将会介绍两种经典的方法，来解决这个问题。</p>
<p><strong>Score Function Gradient Estimator (SFGE)</strong></p>
<p>SFGE 是最经典的方法，也是适用性最好的方法，在强化学习中的策略梯度优化问题里，有一个算法叫做 REINFORCE，正是基于 SFGE 来做的。SFGE 也常常被用于解决目标函数不可导的优化问题以及一些黑盒优化问题。</p>
<p><strong>Score Function 简介</strong></p>
<p>所谓的 score function 是：</p>
<p><img src="https://pic4.zhimg.com/80/v2-71afdd29ae251a3008899fbc7894cfaf_720w.webp"></p>
<p>之所以选择这个函数，是因为以下两点原因：</p>
<ol>
<li>score function 的期望为 0，证明如下：</li>
</ol>
<p><img src="https://pic2.zhimg.com/80/v2-10c6a8759c02a0e0a3ac13532d0611d5_720w.webp"></p>
<p>这样会带来非常多的便利，比如：一种降低估计方差的思路，将代价函数 f(x) 改造为 f(x)-b，其中 b 是所谓的 baseline。因为 score function 的期望为 0，所以：</p>
<p><img src="https://pic1.zhimg.com/80/v2-72d2c05eaabdb83d07ab3c06a19c2418_720w.webp"></p>
<ol start="2">
<li>score function 的方差是 Fisher 信息量。</li>
</ol>
<p><strong>SFGE（Score Function Gradient Estimator）的推导过程</strong></p>
<p><img src="https://pic2.zhimg.com/80/v2-42f2a17a680520e983bade70d3ac729d_720w.webp"></p>
<p>推导中，用到了一个复合函数求导的公式，如下：</p>
<p><img src="https://pic2.zhimg.com/80/v2-d4de48e7e4c07decdd58055559baa9fd_720w.webp"></p>
<p>利用 MC 采样可以估计出梯度，如下：</p>
<p><img src="https://pic2.zhimg.com/80/v2-2b90dc47cfa8f8862ef8e76bacc10d59_720w.webp"></p>
<p>其中：</p>
<p><img src="https://pic4.zhimg.com/80/v2-c66286b3e5df9ffb0a0660180794199b_720w.webp"></p>
<p>从上述推导中可以看到，通过引入 score function，可以成功地将期望的梯度<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713888479218.png" alt="1713888479218">变换为梯度的期望<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713888496031.png" alt="1713888496031">或<img src="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/1713888720195.png" alt="1713888720195">，从而实现梯度的近似估计。</p>
<p>这中间有一个过程是将积分和微分操作的位置进行了对换，此操作并非可以随意进行，需要满足一定的条件，但一般的机器学习问题都会满足。</p>
<p><strong>SFGE（Score Function Gradient Estimator）的性质</strong></p>
<ul>
<li>代价函数 <em>f(x)</em> 可以是任意函数。比如可微的，不可微的；离散的，连续的；白箱的，黑箱的等。这个性质是其最大的优点，使得很多不可微的甚至没有具体函数的黑箱优化问题都可以利用梯度优化求解。</li>
<li>分布函数 <em>p(x;θ)</em> 必须对 <em>θ</em> 是可微的，从公式中也看得出来。</li>
<li>分布函数必须是便于采样的，因为梯度估计都是基于 MC 的，所以希望分布函数便于采样。</li>
<li>SFGE 的方差受很多因素影响，包括输入的维度和代价函数。</li>
</ul>
<p><strong>SFGE（Score Function Gradient Estimator）的典型应用</strong></p>
<p>SFGE 由于其对代价函数没有限制，具有非常广阔的应用场景，以下是几个非常热门的应用：</p>
<ul>
<li>策略梯度优化算法 REINFORCE 及其变种</li>
<li>基于 GAN 的自然语言生成</li>
<li>基于自动微分的黑盒变分推断</li>
</ul>
<p>这些典型的应用，后续可专门写一篇文章进行介绍。</p>
<p><strong>Pathwise Gradient Estimator (PGE)</strong></p>
<p>不同于 SFGE 对代价函数没有任何约束，PGE 要求代价函数可微，虽然 SFGE 更具一般性，但 PGE 会有更好的性质。PGE在机器学习领域有一个重要的方法是 reparameterization trick，它是著名的深度生成模型 VAE 中一个重要的步骤。</p>
<p><strong>PGE简介</strong></p>
<p>PGE 的思路是将待学习的参数从分布中变换到代价函数中，核心是做分布变换（即所谓的 reparameterization ，重参数化），计算原来分布下的期望梯度时，由于变换后的分布不包含求导参数，可将求导和积分操作进行对换，从而基于 MC 对梯度进行估计。</p>
<p><img src="https://pic4.zhimg.com/80/v2-abb14177f5a1a38549cf966bf2075c53_720w.webp"></p>
<p>如上述公式，从一个含参 <em>θ</em> 分布中采样，等同于从一个简单无参分布中采样，然后进行函数变换，并且此函数的参数也是  <em>θ</em> 。变换前，采样是直接从所给分布中进行，而采用重参数化技巧后，采样是间接地从一个简单分布进行，然后再映射回去，这个映射是一个确定性的映射。其中，映射有很多中思路，比如：逆函数、极变换等方法。</p>
<p>PGE 的一个重要理论依据是 Law of the Unconscious Statistician (LOTUS) ，即：</p>
<p><img src="https://pic3.zhimg.com/80/v2-a796c271a30eb8bd3884e1b123fe13c2_720w.webp"></p>
<p>从定理中可以看到，计算一个函数的期望，可以不知道其分布，只需要知道一个简单分布，以及从简单分布到当前分布的映射关系即可。</p>
<p><strong>PGE推导过程</strong></p>
<p>基于 Law of the Unconscious Statistician (LOTUS) 对 PGE 进行推导，如下：</p>
<p><img src="https://pic1.zhimg.com/80/v2-7d4173cd947d5a75871b6f089adb26e4_720w.webp"></p>
<p>利用 MC 可以估计出梯度为：</p>
<p><img src="https://pic3.zhimg.com/80/v2-f0c0dc31504cf0277727d2e143d26c5a_720w.webp"></p>
<p>其中：</p>
<p><img src="https://pic4.zhimg.com/80/v2-a5b145797fba25088eef9aa6fdaddcd3_720w.webp"></p>
<p>从推导中可以看出，分布中的参数被 push 到了代价函数中，从而可以将求导和积分操作进行对换。</p>
<p>分布变换是统计学中一个基本的操作，在计算机中实际产生各种常见分布的随机数时，都是基于均匀分布的变换来完成的。有一些常见的分布变换可参见下表：</p>
<p><img src="https://pic1.zhimg.com/80/v2-55dec2c6a19ade14da2d5f18e290a8fc_720w.webp"></p>
<p>▲ 图3. 常见分布变换</p>
<p><strong>PGE的性质</strong></p>
<ul>
<li>代价函数要求是可微的，比 SFGE 更严格</li>
<li>在使用 PGE 时，并不需要显式知道分布的形式，只需要知道一个基础分布和从该基础分布到原分布的一个映射关系即可，这意味着，不管原来分布多么复杂，只要能获取到以上两点信息，都可以进行梯度估计；而 SFGE 则需要尽量选择一个易采样的分布</li>
<li>PGE 的方差受代价函数的光滑性影响</li>
</ul>
<p><strong>PGE的典型应用</strong></p>
<ul>
<li>深度生成模型 VAE 和 GAN 的训练</li>
<li>基于 Normalising Flow 的变分推断</li>
<li>用于连续控制问题的强化学习</li>
</ul>
<h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>蒙特卡洛采样（MCS）是求解函数期望的常用近似方法，优点是简单易用，通过一定的变换，可以对期望的梯度进行估计，从而完成对代价函数的优化，实现很多任务。</p>
<p>但 MCS 的缺点也非常明显，为了保证一定的估计效果，往往需要很大量的采样规模，对于大数据、高维度等实际问题来说，过多的采样会导致算法效率极低，从而降低了算法的实用性。从这个角度来说，如何研究一些新方法，来提高期望或者期望梯度的近似估计效率是一个非常重要的问题。最后，推荐两篇 2019 年的工作[4] [5]，旨在尝试解决这个问题。</p>
<p>上述研究虽然有一定的局限性，但尝试了新的思路来解决这一问题。其中第[5]篇，尝试用一些 Uncertainty Qualification (UQ) 的方法，比如用一些不确定性传播的估计方法，对期望进行确定性估 计，而非随机采样估计，在一定的假设下，确实有非常显著的效果。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a><strong>参考文献</strong></h2><p>[1] Mohamed, S., Rosca, M., Figurnov, M., &amp; Mnih, A. (2019). Monte Carlo Gradient Estimation in Machine Learning. ArXiv Preprint ArXiv:1906.10652.</p>
<p>[2] Fu, M. C. (2005). Stochastic Gradient Estimation, 105–147.</p>
<p>[3] Shakir’s Machine Learning Blog <a href="https://link.zhihu.com/?target=http://blog.shakirm.com">http:&#x2F;&#x2F;<strong>blog.shakirm.com</strong></a></p>
<p>[4] Postels, J., Ferroni, F., Coskun, H., Navab, N., &amp; Tombari, F. (2019). Sampling-free Epistemic Uncertainty Estimation Using Approximated Variance Propagation. ArXiv Preprint ArXiv:1908.00598.</p>
<p>[5] Wu, A., Nowozin, S., Meeds, T., Turner, R. E., Lobato, J. M. H., &amp; Gaunt, A. (2019). Deterministic Variational Inference for Robust Bayesian Neural Networks. In ICLR 2019 : 7th International Conference on Learning Representations.</p>
<h2 id="本文总结"><a href="#本文总结" class="headerlink" title="本文总结"></a>本文总结</h2><p><strong>《Diffusion Policy: Visuomotor Policy Learning via Action Diffusion》</strong><br>作者为来自哥伦比亚大学、麻省理工学院等的Cheng Chi、Zhenjia Xu、Siyuan Feng等人。<br>文章提出了一种新的机器人视觉运动策略表示方法Diffusion Policy，它将机器人的视觉运动策略表示为一个条件去噪扩散过程，通过学习动作分布的梯度来生成机器人的行为。实验结果表明，Diffusion Policy在多个机器人操作基准测试中均取得了显著的性能提升，并且具有良好的泛化能力和可解释性。</p>
<ul>
<li><strong>研究背景</strong>：传统的机器人行为克隆方法存在一些问题，如难以处理多模态动作分布、高维动作空间和训练不稳定等。为了解决这些问题，作者提出了一种新的机器人视觉运动策略表示方法Diffusion Policy，它将机器人的视觉运动策略表示为一个条件去噪扩散过程，通过学习动作分布的梯度来生成机器人的行为。</li>
<li><strong>方法</strong>：<ul>
<li><strong>条件去噪扩散模型</strong>：将机器人的动作生成过程建模为一个条件去噪扩散过程，通过学习动作分布的梯度来生成机器人的行为。</li>
<li><strong>策略学习</strong>：通过最小化策略损失函数来学习策略参数，使得策略能够生成最优的动作序列。</li>
<li><strong>模型训练</strong>：使用随机梯度下降算法来训练模型参数，同时使用正则化技术来防止过拟合。</li>
<li><strong>实验结果</strong>：在多个机器人操作基准测试中均取得了显著的性能提升，并且具有良好的泛化能力和可解释性。</li>
</ul>
</li>
<li><strong>实验</strong>：<ul>
<li><strong>数据集</strong>：使用了四个不同的机器人操作基准测试数据集，包括Robomimic、Franka Kitchen、Realworld Push-T和Realworld Mug Flipping等。</li>
<li><strong>评估指标</strong>：使用了成功率、平均奖励和平均轨迹长度等评估指标来评估模型的性能。</li>
<li><strong>实验结果</strong>：Diffusion Policy在所有的基准测试中均取得了显著的性能提升，并且具有良好的泛化能力和可解释性。</li>
</ul>
</li>
<li><strong>讨论</strong>：<ul>
<li><strong>模型的优势</strong>：Diffusion Policy能够处理多模态动作分布、高维动作空间和训练不稳定等问题，同时具有良好的泛化能力和可解释性。</li>
<li><strong>模型的局限性</strong>：Diffusion Policy需要大量的计算资源和时间来训练，同时对于复杂的环境和任务可能需要进一步的改进和优化。</li>
<li><strong>未来的研究方向</strong>：未来的研究方向包括进一步提高模型的性能和效率、探索模型的可解释性和可视化、将模型应用于更广泛的机器人操作任务等。</li>
</ul>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz">ALTNT</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/">http://blog.705553939.xyz/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/GenerativeSkillChainingLong-HorizonSkillPlanningwithDiffusionModels/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA/">Ros机器人</a></div><div class="post_share"><div class="social-share" data-image="/img/altnt.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/diffusionpolicy/" title="Diffusion Policy"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">Diffusion Policy</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E4%BA%91%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" title="云操作系统"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">云操作系统</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/" title="模仿学习"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-05</div><div class="title">模仿学习</div></div></a></div><div><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/ros%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/ROS%20%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/" title="ROS通信机制"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-05</div><div class="title">ROS通信机制</div></div></a></div><div><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/%E6%8E%A5%E4%B8%8B%E6%9D%A5%E5%8F%AF%E8%83%BD%E7%9C%8B%E7%9A%84%E8%AE%BA%E6%96%87/" title="接下来可能看的论文"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-05</div><div class="title">接下来可能看的论文</div></div></a></div><div><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/ros%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/2.1%E8%AF%9D%E9%A2%98%E9%80%9A%E4%BF%A1/00%20%E6%A6%82%E5%BF%B5/" title="Ros通信机制-话题通信"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-05</div><div class="title">Ros通信机制-话题通信</div></div></a></div><div><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/ros%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/2.1%E8%AF%9D%E9%A2%98%E9%80%9A%E4%BF%A1/01.base%20example/" title="Ros通信机制-话题通信2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-05</div><div class="title">Ros通信机制-话题通信2</div></div></a></div><div><a href="/2024/06/05/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/MOMA-Force:%20Visual-Force%20Imitation%20for%20Real-World%20Mobile%20Manipulation/" title="MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-05</div><div class="title">MOMA-Force: Visual-Force Imitation for Real-World Mobile Manipulation</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/altnt.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ALTNT</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">17</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ALTNT"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Generative-Skill-Chaining-Long-Horizon-Skill-Planning-with-Diffusion-Models"><span class="toc-number">1.</span> <span class="toc-text">Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E6%91%98%E8%A6%81"><span class="toc-number">2.</span> <span class="toc-text">1、摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E4%BB%8B%E7%BB%8D"><span class="toc-number">3.</span> <span class="toc-text">2、介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">4.</span> <span class="toc-text">3、相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Task-and-Motion-Planning"><span class="toc-number">4.1.</span> <span class="toc-text">Task and Motion Planning</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A"><span class="toc-number">4.1.0.0.1.</span> <span class="toc-text">解释:</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Learning-to-solve-long-horizon-tasks"><span class="toc-number">4.2.</span> <span class="toc-text">Learning to solve long-horizon tasks.</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A-1"><span class="toc-number">4.2.0.0.1.</span> <span class="toc-text">解释:</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Generative-models-for-planning"><span class="toc-number">4.3.</span> <span class="toc-text">Generative models for planning.</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A-2"><span class="toc-number">4.3.0.0.1.</span> <span class="toc-text">解释:</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-number">5.</span> <span class="toc-text">4、准备工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E8%A1%A8%E8%BF%B0"><span class="toc-number">5.1.</span> <span class="toc-text">问题表述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E8%AE%BE%E7%BD%AE"><span class="toc-number">5.2.</span> <span class="toc-text">环境设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B-%E6%98%AF%E4%B8%80%E7%A7%8D%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%EF%BC%8C%E7%94%A8%E4%BA%8E%E6%A8%A1%E6%8B%9F%E5%92%8C%E5%AD%A6%E4%B9%A0%E6%95%B0%E6%8D%AE%E7%9A%84%E5%88%86%E5%B8%83"><span class="toc-number">5.3.</span> <span class="toc-text">扩散模型:(是一种生成模型，用于模拟和学习数据的分布)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A-3"><span class="toc-number">5.3.1.</span> <span class="toc-text">解释</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81Methodology%E6%96%B9%E6%B3%95%E8%AE%BA"><span class="toc-number">6.</span> <span class="toc-text">5、Methodology方法论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A8%E4%BD%9C%E5%8E%9F%E8%AF%AD%E4%BD%9C%E4%B8%BA%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.1.</span> <span class="toc-text">动作原语作为扩散模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sequencing-skill-diffusion-models%E8%AE%AD%E7%BB%83%E6%8A%80%E8%83%BD%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B"><span class="toc-number">6.2.</span> <span class="toc-text">Sequencing skill diffusion models训练技能扩散模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E7%BA%A6%E6%9D%9F%E6%BB%A1%E8%B6%B3%E6%8C%87%E5%AF%BC"><span class="toc-number">6.3.</span> <span class="toc-text">基于分类器的约束满足指导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">6.4.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81%E7%BB%93%E6%9E%9C"><span class="toc-number">7.</span> <span class="toc-text">5、结果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%BA%BF%E5%92%8C%E6%8C%87%E6%A0%87"><span class="toc-number">7.1.</span> <span class="toc-text">基线和指标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Toy%E9%A2%86%E5%9F%9F%E3%80%82"><span class="toc-number">7.2.</span> <span class="toc-text">Toy领域。</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93"><span class="toc-number">8.</span> <span class="toc-text">算法总结</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%951-%E7%94%9F%E6%88%90%E6%8A%80%E8%83%BD%E9%93%BE-GSC-%E7%AE%97%E6%B3%95"><span class="toc-number">8.1.</span> <span class="toc-text">算法1:生成技能链(GSC)算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%EF%BC%9A"><span class="toc-number">8.2.</span> <span class="toc-text">初始化：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E8%BF%87%E7%A8%8B%EF%BC%9A"><span class="toc-number">8.3.</span> <span class="toc-text">算法过程：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C%EF%BC%9A"><span class="toc-number">8.4.</span> <span class="toc-text">结果：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E5%8F%8A%E5%85%B6%E8%AE%A1%E7%AE%97"><span class="toc-number">8.5.</span> <span class="toc-text">超参数及其计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A2%9D%E5%A4%96%E7%9A%84%E8%AE%A8%E8%AE%BA"><span class="toc-number">9.</span> <span class="toc-text">额外的讨论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="toc-number">9.1.</span> <span class="toc-text">实现细节:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%89%A9%E6%95%A3%E5%99%A8%E6%96%B9%E6%B3%95-%E5%85%B7%E6%9C%89%E9%80%86%E5%8A%A8%E5%8A%9B%E5%AD%A6%E4%BD%9C%E7%94%A8%E7%9A%84%E7%8A%B6%E6%80%81%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E3%80%82"><span class="toc-number">9.2.</span> <span class="toc-text">决策扩散器方法:具有逆动力学作用的状态扩散模型。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#C%E6%8A%80%E8%83%BD%E6%8F%8F%E8%BF%B0%E5%92%8C%E5%8F%82%E6%95%B0%E5%8C%96"><span class="toc-number">9.3.</span> <span class="toc-text">C技能描述和参数化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E7%9F%A5%E8%AF%86%EF%BC%9A"><span class="toc-number">10.</span> <span class="toc-text">参考知识：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E9%87%87%E6%A0%B7%EF%BC%88MCS%EF%BC%89https-zhuanlan-zhihu-com-p-81806149"><span class="toc-number">10.1.</span> <span class="toc-text">蒙特卡洛采样（MCS）https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;81806149</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A8%E6%9C%BA"><span class="toc-number">10.1.1.</span> <span class="toc-text">动机</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86"><span class="toc-number">10.1.2.</span> <span class="toc-text">背景知识</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8"><span class="toc-number">10.1.3.</span> <span class="toc-text">应用</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%EF%BC%88Variational-Inference-VI%EF%BC%89"><span class="toc-number">10.1.3.1.</span> <span class="toc-text">变分推断（Variational Inference, VI）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">10.1.3.2.</span> <span class="toc-text">强化学习</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E7%BB%BC%E8%BF%B0"><span class="toc-number">11.</span> <span class="toc-text">方法综述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-number">12.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">13.</span> <span class="toc-text">参考文献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E6%80%BB%E7%BB%93"><span class="toc-number">14.</span> <span class="toc-text">本文总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/28/vscode%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/vscode%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/" title="vscode插件开发">vscode插件开发</a><time datetime="2024-06-28T06:47:30.251Z" title="Created 2024-06-28 14:47:30">2024-06-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5/" title="强化学习相关概念">强化学习相关概念</a><time datetime="2024-06-28T04:03:18.846Z" title="Created 2024-06-28 12:03:18">2024-06-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/" title="机器学习相关概念">机器学习相关概念</a><time datetime="2024-06-26T09:09:56.000Z" title="Created 2024-06-26 17:09:56">2024-06-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/22/%E8%B5%84%E6%96%99/%E5%91%BD%E4%BB%A4/" title="命令">命令</a><time datetime="2024-06-22T03:44:40.000Z" title="Created 2024-06-22 11:44:40">2024-06-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/05/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%AE%BA%E7%AC%94%E8%AE%B0/" title="数据库系统概论笔记">数据库系统概论笔记</a><time datetime="2024-06-05T15:46:23.990Z" title="Created 2024-06-05 23:46:23">2024-06-05</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By ALTNT</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>