<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习相关概念 | ALTNT's Hexo Blog</title><meta name="author" content="ALTNT"><meta name="copyright" content="ALTNT"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="机器学习分类 根据训练数据是否有标签，可以分为： 监督学习：所有训练数据均具有标签（典型的问题有回归：模型输出的是一个具体数值；分类：模型的输出是某一类别） 在监督学习中，常用的模型种类可以分为：线性模型 和 非线性模型。其中，非线性模型应用更加广泛，表达能力也更强，包括深度学习，支持向量机（SVM），决策树，K-NN算法等。 半监督学习：训练数据中，部分具有标签，另一部分没有标签（但是没有标签的">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习相关概念">
<meta property="og:url" content="http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/index.html">
<meta property="og:site_name" content="ALTNT&#39;s Hexo Blog">
<meta property="og:description" content="机器学习分类 根据训练数据是否有标签，可以分为： 监督学习：所有训练数据均具有标签（典型的问题有回归：模型输出的是一个具体数值；分类：模型的输出是某一类别） 在监督学习中，常用的模型种类可以分为：线性模型 和 非线性模型。其中，非线性模型应用更加广泛，表达能力也更强，包括深度学习，支持向量机（SVM），决策树，K-NN算法等。 半监督学习：训练数据中，部分具有标签，另一部分没有标签（但是没有标签的">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://blog.705553939.xyz/img/altnt.jpeg">
<meta property="article:published_time" content="2024-06-26T09:09:56.000Z">
<meta property="article:modified_time" content="2024-08-13T10:41:44.372Z">
<meta property="article:author" content="ALTNT">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.705553939.xyz/img/altnt.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习相关概念',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-08-13 18:41:44'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/altnt.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="ALTNT's Hexo Blog"><span class="site-name">ALTNT's Hexo Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习相关概念</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-06-26T09:09:56.000Z" title="Created 2024-06-26 17:09:56">2024-06-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-08-13T10:41:44.372Z" title="Updated 2024-08-13 18:41:44">2024-08-13</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习相关概念"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="机器学习分类">机器学习分类</h2>
<p>根据训练数据是否有标签，可以分为：</p>
<p>监督学习：所有训练数据均具有标签（典型的问题有回归：模型输出的是一个具体数值；分类：模型的输出是某一类别）
在监督学习中，常用的模型种类可以分为：线性模型 和
非线性模型。其中，非线性模型应用更加广泛，表达能力也更强，包括深度学习，支持向量机（SVM），决策树，K-NN算法等。</p>
<p>半监督学习：训练数据中，部分具有标签，另一部分没有标签（但是没有标签的数据，对于模型的学习也是有用处的）。
迁移学习：使用与当前任务无关的数据（可能有标签，可能没有标签）来促进当前模型的学习。
无监督学习：训练数据都没有标签。
无监督学习存在的原因是，现实世界中，为训练数据进行标注成本较高，当训练数据都没有标签时，如果我们想要为数据进行分类，只能根据数据的特征进行划分，比如聚类算法。</p>
<p>强化学习：训练数据没有标签，智能体从环境交互中进行学习，来更新自身的策略，根据最终环境的反馈（获得的奖励）来调整自身行为。</p>
<h3 id="回归问题">回归问题</h3>
<pre><code>机器学习笔记的第二篇博客，来介绍机器学习中最基础的回归任务，上一篇博客中有提到回归任务和分类任务的差别在于，回归任务中模型的输出是一个具体的数值， 而分类任务中模型的输出是某一类别。其实，许多问题我们都可以视为回归问题：![alt text](机器学习相关概念/image-4.png)</code></pre>
<p>例如：根据股票市场的历史数据预测明天的股票走势；自动驾驶中根据传感器获取的信息输出方向盘的转动角度；推荐系统中，输入用户和商品的特征，模型输出一个[0,1]之间的数值，表示购买的可能性。</p>
<h3 id="回归模型的建立">回归模型的建立</h3>
<p>机器学习模型建立的三个步骤：</p>
<ol type="1">
<li>我们准备许多备选的函数 <strong><em>f </em></strong>
，构成一个集合，也就是机器学习中的模型（Model）。</li>
<li>使用训练数据来衡量这些备选函数的好坏程度。</li>
<li>根据训练数据选出拟合最好的函数，作为最终的拟合函数。</li>
</ol>
<h4 id="第一步确定用于回归任务的模型">第一步：确定用于回归任务的模型</h4>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-5.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>这里选择一元线性函数 <strong><em>y = wx +
b</em></strong>即线性模型，一元表示我们使用的特征是一个（宝可梦的原CP值）；这样我们就构造了一个函数集合，由于参数<strong><em>w</em></strong>和<strong><em>b</em></strong>的取值是无穷的，所以函数集合中函数的个数是无穷个，接下来在函数集合中选择最好的函数的过程实际上就是为函数确定参数值的过程。</p>
<h4 id="第二步使用训练数据来衡量这些备选函数的好坏程度确定参数">第二步：使用训练数据来衡量这些备选函数的好坏程度（确定参数）</h4>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-6.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>接下来我们需要根据训练数据来从备选函数中选择一个效果最好的函数（即确定函数参数部分），这里需要使用损失函数，损失函数的输入是
“用来进行回归任务的函数” 和 “真实标签” ，输出是
进行回归任务的函数的好坏程度（Loss的值越小，认为该函数的效果越好）。如上图所示，我们使用
平方差之和 作为损失函数。</p>
<h4 id="第三步根据训练数据选出拟合最好的函数作为最终的拟合函数">第三步：根据训练数据选出拟合最好的函数，作为最终的拟合函数</h4>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-7.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>第三步，我们在训练数据上根据损失函数来评估拟合函数的好坏，找到使得损失函数最小的一组参数，作为我们最终的拟合函数。其中，寻找参数时，使用的方法为
<strong><em>梯度下降法</em></strong> 。</p>
<h2 id="梯度下降">梯度下降</h2>
<p>梯度下降算法是机器学习领域最广为人知、用途最广的优化算法，用来确定模型的参数（包括随机梯度下降SGD，Momentum，Adam等）。梯度下降算法的一个简单介绍如下：</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-8.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720165442054.png" alt="1720165442054">
<figcaption aria-hidden="true">1720165442054</figcaption>
</figure>
<p>1、在机器学习中，只要损失函数是可微分（可求导）的，就可以使用梯度下降算法进行参数的求解，那么怎么判断损失函数是否可微？（后面解释）</p>
<p>上面图片展示的是，模型当中只有一个参数（所以直接对该参数求导就可以），如果模型中存在两个及以上的参数，那么就需要分别对每个参数计算偏导数，然后根据参数更新公式进行每个参数的更新，如下图所示:
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-9.png" alt="alt text"></p>
<p>梯度下降算法的原理已经清楚，其实就是沿着损失函数降低的方向更新模型的参数，但是如果损失函数很复杂，比如下面图片所示，
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-10.png" alt="alt text"></p>
<p>我们很可能在更新参数的过程中，走到导数为0的点（第四个红点位置），这时因为不知道更新的方向，就陷入了局部最优点（其实真正的全局最优点还在右边）。</p>
<p>2、不过对于上面回归问题中，损失函数为平方差之和，该损失函数为凸函数，没有局部最优点，只有全局最优点。那么如何判断一个函数是否为凸函数？（后面解释）</p>
<h3 id="预测结果分析">预测结果分析</h3>
<p>我们上面的线性模型，经过梯度下降算法，寻得一组最优参数，其结果表现如下：
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-11.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-12.png" alt="alt text"></p>
<p>训练数据上面的损失函数值为31.9，测试数据上面损失函数值为35。在实际问题中，我们更加关注的是模型在测试集上面的性能表现，也就是模型的
<strong><em>泛化能力</em></strong>
，线性模型在测试集上面的误差较大，所以如果我们重新设计预测模型，使用更加复杂的模型，会不会得到更好的效果？
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-13.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-14.png" alt="alt text"></p>
<p>随着使用更加复杂的二次模型，三次模型，无论是在训练集还是测试集上面，效果都有提升。可是当继续增加模型的复杂度，使用四次模型的时候，虽然在训练集上面的<strong><em>loss</em></strong>更小，但是测试集上面的效果却变糟了。使用五次模型的时候，这一趋势更加明显：
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-15.png" alt="alt text">
虽然复杂的模型对于训练数据的拟合程度会更好，但是很容易出现过拟合的现象（过于严格的去拟合训练数据，当面对新数据的时候没有办法做出准确的预测，即无法泛化到其他数据）。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-16.png" alt="alt text">
在机器学习模型训练过程中，我们要尽量避免过拟合的现象，一方面要选择合适的模型，模型不是越复杂越好，可以通过交叉验证来选择合适的模型；另一方面，<strong>可以通过一些技术手段来帮助我们避免过拟合，比如正则化，early
stopping等等。</strong> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-17.png" alt="alt text">
如上图所示，<strong>蓝色线代表训练集上的损失函数，红色线代表验证集的损失函数</strong>，当训练进行到中间垂直的线段时，模型应该是最优的；如果继续训练，就会造成过拟合现象。</p>
<h3 id="正则化">正则化</h3>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-18.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>简单来说，正则化是一种为了减小<strong>测试误差</strong>的行为(有时候会增加训练误差)。我们在构造机器学习模型时，最终目的是让模型在面对新数据的时候，可以有很好的表现。当使用比较复杂的模型比如神经网络，去拟合数据时，很容易出现过拟合现象(训练集表现很好，测试集表现较差)，这会导致模型的泛化能力下降，这时候，就需要使用正则化，降低模型的复杂度。</p>
<p><strong>具体而言，正则化就是在损失函数后面增加一项惩罚项（对某些参数进行限制），使得我们的模型更加平滑。以上图为例，我们在损失函数后面增加一项关于参数w的正则项，限制参数w不要过大，这样模型会有更好的泛化能力。因为，这样对于测试数据中存在的噪声，会不那么敏感，即噪声对于预测的结果影响会降低。</strong>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720515070068.png" alt="1720515070068"></p>
<p>加入正则化技术之后，训练集和测试集上面的误差如下： <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-19.png" alt="alt text"></p>
<p>和我们的预期分析是一致的，加入正则化之后，参数<embed src="https://private.codecogs.com/gif.latex?%5Clambda"></p>
<p>越大，表示我们越关注模型的平滑程度（也就是模型的泛化能力），相对于训练误差考虑较少，所以训练集上面的loss增大，测试集上面的loss降低。但是，参数<embed src="https://private.codecogs.com/gif.latex?%5Clambda"></p>
<p>不是越大越好，我们希望得到一个比较平滑的函数，但是不能过于平滑（会丧失其预测能力）。</p>
<p><strong>机器学习中的正则化概念</strong></p>
<p>在机器学习中，正则化（Regularization）是一种用于防止过拟合（Overfitting）的技术。</p>
<p>过拟合是指模型在训练数据上表现得非常好，但在新的、未见过的数据上表现不佳。这通常是因为模型过于复杂，学习到了训练数据中的噪声和特定的细节，而不是一般性的模式。</p>
<p>正则化通过在损失函数中添加一个惩罚项来限制模型的复杂度。常见的正则化方法有
L1 正则化和 L2 正则化。</p>
<p><strong>L1 正则化</strong>：也称为 Lasso
正则化，它在损失函数中添加的惩罚项是模型参数的绝对值之和。L1
正则化具有特征选择的效果，因为它可能会将一些不重要的特征对应的参数压缩至零。</p>
<p>例如，在线性回归中，假设模型的预测函数为
<code>y = w1 * x1 + w2 * x2 +... + wn * xn + b</code> ，L1 正则化项就是
<code>λ * |w1| + λ * |w2| +... + λ * |wn|</code> ，其中 <code>λ</code>
是正则化参数，控制正则化的强度。</p>
<p><strong>L2 正则化</strong>：也称为 Ridge
正则化，它在损失函数中添加的惩罚项是模型参数的平方和。L2
正则化会使模型的参数值趋向于较小的值，但不太会将参数压缩至零。</p>
<p>同样在线性回归中，L2 正则化项就是
<code>λ * (w1^2 + w2^2 +... + wn^2)</code> 。</p>
<p><strong>正则化的作用</strong>：</p>
<ol type="1">
<li>控制模型复杂度：通过限制模型的参数大小，防止模型过于复杂。</li>
<li>提高模型泛化能力：使模型能够更好地应对新的数据，减少过拟合的风险。</li>
</ol>
<p><strong>举例说明</strong>：
假设我们正在训练一个神经网络来识别图像中的猫和狗。如果没有正则化，模型可能会过度学习训练数据中的细微特征，比如图片中的背景颜色或微小的噪声，导致在新的图像上识别准确率下降。</p>
<p>当我们应用 L2
正则化时，模型的参数会受到一定的约束，不会变得过大。这可能会导致模型在训练数据上的准确率稍微降低，但在测试数据上的表现会更好，因为它学习到了更通用的特征，而不是过度依赖于特定的训练样本。</p>
<p>总之，正则化是机器学习中非常重要的技术，有助于提高模型的性能和稳定性。</p>
<h3 id="遗留问题">遗留问题</h3>
<p>1、在机器学习中，只要损失函数是可微的，就可以使用梯度下降算法进行参数的求解，那么怎么判断损失函数是否可微？（后面解释）</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720166051610.png" alt="1720166051610">
<figcaption aria-hidden="true">1720166051610</figcaption>
</figure>
<p>2、不过对于上面回归问题中，损失函数为平方差之和，该损失函数为凸函数，没有局部最优点，只有全局最优点。那么如何判断一个函数是否为凸函数？（后面解释）</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720166067204.png" alt="1720166067204">
<figcaption aria-hidden="true">1720166067204</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-20.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>判断一个矩阵是不是半正定矩阵，方法之一是判断该矩阵的所有主子式是不是非负；对于上面的海森矩阵，其所有主子式为：</p>
<p><embed src="https://private.codecogs.com/gif.latex?2x%5E%7B2%7D">，2,
0，均为非负，所以该海森矩阵为半正定矩阵。所以该损失函数为凸函数，只有全局最优值。</p>
<p><strong>半正定矩阵</strong></p>
<p>半正定矩阵是矩阵理论中的一个重要概念。</p>
<p>一个实对称矩阵 <code>A</code>
被称为半正定矩阵，如果对于任意的非零实向量 <code>x</code> ，都有
<code>x^T Ax ≥ 0</code> 。</p>
<p><strong>性质</strong> ：</p>
<ol type="1">
<li>半正定矩阵的所有特征值都是非负的。</li>
<li>半正定矩阵的主子式都非负。</li>
<li>半正定矩阵与另一个半正定矩阵的和仍是半正定矩阵。</li>
</ol>
<p>在实际的机器学习和优化问题中，Hessian 矩阵具有重要的地位。</p>
<p><strong>计算方面</strong>： 计算 Hessian
矩阵可能是计算密集型的，特别是对于具有大量参数的模型。在深度学习中，直接计算完整的
Hessian
矩阵通常是不现实的。然而，可以使用近似方法或针对特定结构的模型进行简化计算。例如，对于一些具有简单结构的神经网络，可能通过一些技巧来估计
Hessian 矩阵的部分元素。</p>
<p><strong>应用方面</strong>：</p>
<ol type="1">
<li>优化算法：如牛顿法及其变体，利用 Hessian
矩阵来确定搜索方向和步长。相比于梯度下降法只依赖一阶导数（梯度），牛顿法考虑了二阶导数信息，能够在一些情况下更快地收敛到最优解。
<ul>
<li>举例来说，在求解一个二次函数的最小值时，牛顿法通过一次迭代就可以直接到达最小值点，因为它准确地利用了
Hessian 矩阵的信息。</li>
</ul></li>
<li>模型分析：帮助理解模型的性质和行为。通过分析 Hessian
矩阵的特征值和特征向量，可以了解模型在不同方向上的敏感度和曲率，从而洞察模型的稳定性和鲁棒性。
<ul>
<li>例如，在图像分类任务中，如果 Hessian
矩阵的某些特征值很大，说明模型在对应的特征方向上变化剧烈，可能对输入的微小变化非常敏感。</li>
</ul></li>
<li>正则化：可以用于设计一些基于二阶信息的正则化方法，以防止过拟合。
<ul>
<li>比如，通过对 Hessian
矩阵进行某种变换或约束，使得模型的复杂度得到控制。</li>
</ul></li>
</ol>
<p>总之，尽管在实际中直接处理 Hessian
矩阵存在困难，但通过巧妙的近似和应用，它仍然为解决机器学习和优化问题提供了有价值的见解和工具。</p>
<h3 id="梯度下降算法">梯度下降算法</h3>
<p>梯度下降算法是机器学习领域最广为人知、用途最广的优化算法，用来确定模型的参数（包括随机梯度下降SGD，Momentum，Adam等）。首先回顾一下梯度下降的计算过程：</p>
<h4 id="梯度下降中常用技巧tips">梯度下降中常用技巧（Tips）</h4>
<h5 id="一调整学习率">一、调整学习率</h5>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-28.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720543595377.png" alt="1720543595377">
<figcaption aria-hidden="true">1720543595377</figcaption>
</figure>
<p>所以，我们可以根据Loss曲线的变化情况，对我们的学习率进行一个合理的调整。可以绘制上图右边部分所示的曲线图，横轴代表参数更新次数，纵轴代表Loss值，学习率的大小分为四种情况：</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720543613512.png" alt="1720543613512">
<figcaption aria-hidden="true">1720543613512</figcaption>
</figure>
<p>在学习率的设置过程中，常见的做法是，进行
<strong><em>学习率的衰减</em></strong> 。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-29.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>在模型训练初期，距离全局最优点较远，可以设置一个相对大一些的学习率，随着训练的进行，距离全局最优点的距离越来越小，此时应该减小学习率；所以让学习率随着时间或者更新的次数进行衰减。</p>
<p>除了学习率的衰减之外，另外一个做法是：为不同的参数设置不同的学习率。也就是接下来要介绍的一种优化算法AdaGrad。</p>
<h6 id="adagrad优化器">AdaGrad优化器</h6>
<p>AdaGrad（Adaptive Gradient
Algorithm）是一种自适应学习率的方法，用于优化机器学习模型，特别是在处理稀疏数据和高维数据时表现出色。AdaGrad的主要特点是它为每个参数维护一个单独的学习率，并根据之前的梯度信息调整这些学习率。</p>
<p>AdaGrad根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题,即为每个参数设置不同的学习率。具体来说，对于每个参数
<strong>θ</strong>i，AdaGrad计算其所有历史梯度的平方和，并用这个和来缩放当前的梯度，从而得到更新步长。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-30.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720543759855.png" alt="1720543759855">
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-31.png" alt="alt text"></p>
<p>这里需要注意的一点为： <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-32.png" alt="alt text"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-33.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-34.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-35.png" alt="alt text">
而在AdaGrad中，正是这一思想的体现，不过为了减少计算量，增加运算速度，AdaGrad中使用过去一阶偏导数的均方根作为分母项（二阶偏导数）的近似。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-36.png" alt="alt text">
过去一阶偏导数的均方根可以在一定程度上反应二阶偏导的大小情况。如上图所示，二阶偏导小的函数，其采样的一阶偏导的值也相对较小。</p>
<p><strong>优势</strong></p>
<ol type="1">
<li><strong>自适应学习率</strong>
：不同参数有不同的学习率，能够更好地处理稀疏数据，使得更新幅度较大的参数的学习率更小，而更新幅度较小的参数的学习率更大。</li>
<li><strong>简化调参</strong>
：由于学习率是自适应的，通常不需要频繁调整学习率超参数。</li>
</ol>
<p><strong>局限性</strong></p>
<ol type="1">
<li><strong>学习率过小</strong>
：随着时间的推移，累积梯度平方和不断增大，导致学习率逐渐缩小，可能会导致算法在后期学习变得非常缓慢。</li>
<li><strong>不适用于所有问题</strong>
：虽然AdaGrad在处理稀疏数据上有优势，但对于某些问题，其性能可能不如其他优化算法，如RMSprop或Adam。</li>
</ol>
<p><strong>典型应用</strong></p>
<p>AdaGrad常用于处理自然语言处理中的词嵌入、推荐系统中的用户行为数据等高维、稀疏数据场景。</p>
<p>总结来说，AdaGrad通过自适应地调整每个参数的学习率，在处理稀疏数据和高维数据时提供了显著的优势，但其逐渐减小的学习率可能在某些情况下限制其性能。</p>
<h5 id="二随机梯度下降sgd">二、随机梯度下降（SGD）</h5>
<p>随机梯度下降（SGD, Stochastic Gradient
Descent）是一种用于优化机器学习模型参数的算法，特别适用于大规模数据集的训练。SGD与传统的批量梯度下降（Batch
Gradient
Descent）不同，它在每次迭代中仅使用一个样本或一个小批量的样本（mini-batch）来计算梯度和更新参数。这种方法具有较快的更新速度和更好的内存效率，特别是在处理大数据集时。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720599100421.png" alt="1720599100421">
<figcaption aria-hidden="true">1720599100421</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-37.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-38.png" alt="alt text">
之前梯度下降算法中，Loss函数是对所有训练样本的Loss之和，而在随机梯度下降中，每次只采样一个样本，根据这一个样本进行一次梯度下降，所以随机梯度下降算法更新参数的过程更快。</p>
<p>随机梯度下降（SGD, Stochastic Gradient
Descent）是一种用于优化机器学习模型参数的算法，特别适用于大规模数据集的训练。SGD与传统的批量梯度下降（Batch
Gradient
Descent）不同，它在每次迭代中仅使用一个样本或一个小批量的样本（mini-batch）来计算梯度和更新参数。这种方法具有较快的更新速度和更好的内存效率，特别是在处理大数据集时。</p>
<p><strong>优势</strong></p>
<ol type="1">
<li><strong>快速收敛</strong>：由于每次迭代只使用一个样本或小批量样本，更新频繁，收敛速度快。</li>
<li><strong>内存效率</strong>：每次只需要加载一个样本或小批量样本，内存占用低，适合处理大规模数据集。</li>
<li><strong>逃离局部最优</strong>：由于引入了随机性，SGD有助于跳出局部最优解，更容易找到全局最优解。</li>
</ol>
<p><strong>局限性</strong></p>
<ol type="1">
<li><strong>收敛波动</strong>：由于每次更新基于单个样本或小批量样本，导致参数更新不稳定，损失函数可能会剧烈波动。</li>
<li><strong>调参困难</strong>：学习率的选择对SGD的性能影响很大，通常需要进行超参数调优。</li>
</ol>
<p><strong>改进方法</strong></p>
<p>为了解决SGD的一些局限性，提出了多种改进算法，如：</p>
<ol type="1">
<li><strong>Mini-batch
SGD</strong>：在每次迭代中使用一个小批量样本，而不是单个样本，平衡了收敛速度和稳定性。</li>
<li><strong>动量（Momentum）</strong>：在参数更新时引入动量项，利用之前梯度的指数加权平均来加速收敛。</li>
<li><strong>RMSprop</strong>：自适应调整学习率，缓解学习率逐渐减小的问题。</li>
<li><strong>Adam</strong>：结合了动量和RMSprop的优点，自适应地调整学习率。</li>
</ol>
<p><strong>应用场景</strong></p>
<p>SGD广泛应用于深度学习和机器学习的各种模型训练中，包括神经网络、线性回归、逻辑回归等。它特别适用于大规模数据集和在线学习场景。</p>
<p>总结来说，SGD通过随机选择样本来进行参数更新，提供了快速且内存高效的优化方法，但其波动性和学习率调优是需要注意的问题。改进的变种算法如Mini-batch
SGD、动量、RMSprop和Adam在实践中被广泛采用，以提高SGD的性能和稳定性。</p>
<h5 id="三特征缩放feature-scaling">三、特征缩放（Feature Scaling）</h5>
<p>特征缩放是用来标准化数据特征的范围，减少特征中特异值的影响。</p>
<p>例如： <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-39.png" alt="alt text"></p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720544009279.png" alt="1720544009279">
<figcaption aria-hidden="true">1720544009279</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720544026157.png" alt="1720544026157">
<figcaption aria-hidden="true">1720544026157</figcaption>
</figure>
<h2 id="梯度上升和梯度下降的区别是什么">梯度上升和梯度下降的区别是什么？</h2>
<p><strong>!!!!!!!
简单来说就是正常的机器学习算法是需要计算loss函数的梯度,往梯度下降的方向走,需要loss函数的最小值</strong></p>
<pre><code>**但是梯度上升时,这个函数不是loss函数,而是一个真正的func函数,需要找这个func函数的最大值**</code></pre>
<p>梯度上升和梯度下降是两种相反的优化方法，主要区别如下：</p>
<p><strong>目标方向</strong>：</p>
<p>梯度下降的目标是找到函数的最小值，因此沿着函数梯度的负方向更新参数。</p>
<p>梯度上升的目标是找到函数的最大值，所以沿着函数梯度的正方向更新参数。</p>
<p><strong>更新参数的方式</strong>：</p>
<p>假设函数 <code>f</code> 关于参数 <code>w</code> 的梯度为
<code>∇f(w)</code> ，学习率为 <code>α</code> 。</p>
<p>在梯度下降中，参数的更新公式为：<code>w = w - α * ∇f(w)</code> 。</p>
<p>在梯度上升中，参数的更新公式为：<code>w = w + α * ∇f(w)</code> 。</p>
<p><strong>应用场景</strong>：</p>
<p>梯度下降常用于损失函数的最小化，比如在机器学习中，通过最小化预测值与真实值之间的差异来优化模型的参数。</p>
<p>梯度上升则常用于需要最大化某个目标函数的情况，比如强化学习中最大化奖励，或者在某些特定的优化问题中找到使某个函数达到最大值的参数配置。</p>
<p><strong>示例</strong>：</p>
<p>假设有一个简单的二次函数 <code>f(w) = w^2</code> ，其梯度为
<code>∇f(w) = 2w</code> 。</p>
<p>如果使用梯度下降，学习率为 <code>0.1</code> ，初始参数
<code>w = 2</code> ，则第一次更新为
<code>w = 2 - 0.1 * 2 * 2 = 1.6</code> 。</p>
<p>如果使用梯度上升，同样学习率为 <code>0.1</code> ，初始参数
<code>w = 2</code> ，则第一次更新为
<code>w = 2 + 0.1 * 2 * 2 = 2.4</code> 。</p>
<p>总的来说，梯度上升和梯度下降的核心区别在于更新参数的方向，一个朝着梯度正方向，一个朝着梯度负方向，以分别实现最大化和最小化的目标。</p>
<h2 id="偏差和方差">偏差和方差</h2>
<p>这篇博客介绍机器学习中误差（error）的来源，知道我们的模型中产生的误差来自于哪一部分，才能更好地进行模型的调整。一般来说，误差的来源有两部分：偏差（bias）和方差（variance）。偏差和方差——用来衡量模型泛化能力的工具，所以我的理解是在测试集上面根据偏差和方差来对模型进行一个评估。</p>
<p>回顾之前回归问题中的例子，简单模型对于数据的拟合能力比较差，在训练集和测试集上面效果均不好；但同时不是越复杂的模型越好，因为有可能产生过拟合的现象，所以需要选择合适的模型。偏差-方差分析可以帮我们诊断模型中存在的问题（过于复杂或者过于简单）。</p>
<p><strong>偏差就是：预测输出的期望值 -
真实值，（描述模型的拟合能力）</strong></p>
<p><strong>方差就是：（每个模型实例的预测输出 - 模型预测输出的期望值）^
2 （描述模型的稳定性，即受数据扰动的影响程度）</strong></p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-21.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>还是以宝可梦进化之后的CP值预测为例，如果我们有一些不同的训练数据（也就是李宏毅老师PPT中所说从若干个平行世界中收集的不同的宝可梦），</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-22.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>实质上是指有几个不同的训练集（TrainData_1，TrainData_2，TrainData_3），模型分别在不同的训练集上面训练，然后在同样的测试集（TestData）上面测试。对于不同的训练集，我们会得到一个模型的实例，比如有一次模型和五次模型，训练结果：(这里，“模型”表示具体的模型类别（比如一次模型，二次模型）；“模型实例”表示一个模型在不同训练集上面训练得到的最终模型，有几个训练集就会有几个模型实例。)
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-23.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-24.png" alt="alt text"></p>
<p>图片中，红色线表示在100个不同的训练集上面得到的模型的实例，蓝色线表示模型的预测输出的期望，黑色线是真实值。</p>
<p>可以看出，一次模型的偏差较大，方差较小；五次模型的偏差较小，方差较大。</p>
<h3 id="数学定义">数学定义</h3>
<p>上面是偏差和方差一个比较直观的理解，接下来给出数学形式的定义：</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720518028206.png" alt="1720518028206">
<figcaption aria-hidden="true">1720518028206</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720518369317.png" alt="1720518369317">
<figcaption aria-hidden="true">1720518369317</figcaption>
</figure>
<h3 id="偏差-方差与模型复杂度">偏差-方差与模型复杂度</h3>
<p>偏差和方差的几种情况： <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-25.png" alt="alt text"></p>
<p>偏差小，方差小：追求的目标，理想的模型。
偏差小，方差大：模型比较复杂，在训练集上面过拟合，导致在测试集上面泛化效果不好。
偏差大，方差小：模型比较简单，拟合能力较差，在训练集上面欠拟合，导致在测试集上面泛化效果不好。
偏差大，方差大：最糟的情况，模型需要重新进行设计，不适合于现有数据集。</p>
<p>很直观的一个解释，因为偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力，所以偏差大的模型拟合能力差，模型简单，容易欠拟合；方差度量数据扰动所造成的影响（在不同的训练集上面训练得到的模型在测试集上面效果表现相差很大），说明模型过于拟合训练集，模型复杂。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-26.png" alt="alt text"></p>
<h3 id="调整方法">调整方法</h3>
<p>综上，根据模型在测试集上的表现，可以得出结论：</p>
<blockquote>
<p>偏差大，方差小：模型欠拟合</p>
<p>偏差小，方差大：模型过拟合</p>
</blockquote>
<p>对于偏差大（欠拟合）的情况，常用的解决方法：</p>
<p>• 重新设计模型，使用更复杂的模型结构</p>
<p>• 输入中使用更多的特征</p>
<p>对于方差大（过拟合）的情况，常用的解决方法：</p>
<p>• 参数正则化（减小模型的复杂程度）</p>
<p>• 使用更多的训练数据</p>
<h3 id="k-折交叉验证">K-折交叉验证</h3>
<p>模型的设计选择需要在偏差和方差之间进行平衡，在选择合适的模型（比如一次模型还是二次模型）时，常用的方法是进行K-折交叉验证。将训练集的数据等分成K份，每次使用（K-1）份数据进行训练，余下的1份数据进行验证，进行K次，保证每份数据均做过验证集，统计K次验证集上面的loss，取loss均值最小的模型作为使用的模型。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-27.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<h2 id="softmax">softmax</h2>
<h3 id="主要用途">主要用途:</h3>
<p><strong>softmax是深度学习任务中常用于计算最终输出类别的函数</strong>。</p>
<p>Softmax
函数主要用于多分类问题中，将多个神经元的输出值转换为概率分布。</p>
<p>Softmax
函数在很多机器学习任务中都有广泛的应用，比如图像分类、文本分类等，它有助于将模型的输出转化为可解释的类别概率。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719916089305.png" alt="1719916089305">
<figcaption aria-hidden="true">1719916089305</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719916111218.png" alt="1719916111218">
<figcaption aria-hidden="true">1719916111218</figcaption>
</figure>
<ul>
<li>softmax和我们普通意义上的max函数不同，每一个元素都有一个概率，而不是其中一个元素为1，其余为0。它的含义是对于输入向量，有多大的概率去选择元素1、元素2、元素3等，主要的目的是使得概率计算过程可导。</li>
</ul>
<p>以下是对 Softmax 函数的一些关键理解点：</p>
<ol type="1">
<li>输出值在 0 到 1 之间：Softmax 函数确保每个输出值都在 0 和 1
之间。</li>
<li>输出值总和为 1：所有输出值的总和为
1，这使得它们可以被解释为概率。</li>
<li>强调相对大小：Softmax 函数会放大输入值之间的差异。较大的输入值在经过
Softmax
计算后会得到更大的概率值，较小的输入值则会得到较小的概率值。</li>
</ol>
<p>例如，假设有一个神经网络的输出为 <code>[1, 2, 0]</code>，经过 Softmax
函数计算后，得到的概率分布可能是
<code>[0.269, 0.731, 0.0]</code>。这意味着模型预测第二个类别是最有可能的，第一个类别有一定的可能性，而第三个类别几乎不可能。</p>
<h2 id="池化层">池化层</h2>
<p>池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合</p>
<p>简而言之，如果输入是图像的话，那么池化层的最主要作用就是压缩图像</p>
<h3 id="池化层的作用">池化层的作用</h3>
<p>主要是两个作用：</p>
<ol type="1">
<li>invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)</li>
<li>保留主要的特征同时减少参数(降维，效果类似PCA)和计算量，防止过拟合，提高模型泛化能力</li>
</ol>
<p>A:
特征不变性，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。</p>
<p>B.
特征降维，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来，这也是池化操作的一大作用</p>
<ol type="1">
<li><p>translation invariance：
这里举一个直观的例子(数字识别)，假设有一个16x16的图片，里面有个数字1，我们需要识别出来，这个数字1可能写的偏左一点(图1)，这个数字1可能偏右一点(图2)，图1到图2相当于向右平移了一个单位，但是图1和图2经过max
pooling之后它们都变成了相同的8x8特征矩阵，主要的特征我们捕获到了，同时又将问题的规模从16x16降到了8x8，而且具有平移不变性的特点。图中的a（或b）表示，在原始图片中的这些a（或b）位置，最终都会映射到相同的位置。</p></li>
<li><p>rotation invariance：
下图表示汉字“一”的识别，第一张相对于x轴有倾斜角，第二张是平行于x轴，两张图片相当于做了旋转，经过多次max
pooling后具有相同的特征 ————————————————</p>
<p>版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA
版权协议，转载请附上原文出处链接和本声明。</p></li>
</ol>
<p>原文链接：https://blog.csdn.net/weixin_38145317/article/details/89310404</p>
<p><strong>池化层用的方法有Max pooling 和 average pooling</strong></p>
<h3 id="max-pooling">Max pooling:</h3>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-40.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-41.png" alt="alt text"></p>
<p>对于每个2<em>2的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个2</em>2窗口中最大的数是6，那么输出矩阵的第一个元素就是6，如此类推。</p>
<h2 id="逻辑回归">逻辑回归</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/2.Logistics%20Regression#1-%E4%BB%80%E4%B9%88%E6%98%AF%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"></a></p>
<h3 id="什么是逻辑回归">1. 什么是逻辑回归</h3>
<p>逻辑回归是用来做分类算法的，大家都熟悉线性回归，一般形式是Y=aX+b，y的取值范围是[-∞,
+∞]，有这么多取值，怎么进行分类呢？不用担心，伟大的数学家已经为我们找到了一个方法。</p>
<p>也就是把Y的结果带入一个非线性变换的<strong>Sigmoid函数</strong>中，即可得到[0,1]之间取值范围的数S，S可以把它看成是一个概率值，如果我们设置概率阈值为0.5，那么S大于0.5可以看成是正样本，小于0.5看成是负样本，就可以进行分类了。</p>
<h3 id="什么是sigmoid函数">2. 什么是Sigmoid函数</h3>
<p>函数公式如下：</p>
<figure>
<img src="https://camo.githubusercontent.com/73bf196741b6133e56170bd6196d3dba2827cd2e3280d0e78bcf1f2889bf5cc5/68747470733a2f2f7778342e73696e61696d672e636e2f6c617267652f30303633304465666c7931673470766b32637461746a333063773062363379712e6a7067" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<p>函数中t无论取什么值，其结果都在[0,-1]的区间内，回想一下，一个分类问题就有两种答案，一种是“是”，一种是“否”，那0对应着“否”，1对应着“是”，那又有人问了，你这不是[0,1]的区间吗，怎么会只有0和1呢？这个问题问得好，我们假设分类的<strong>阈值</strong>是0.5，那么超过0.5的归为1分类，低于0.5的归为0分类，阈值是可以自己设定的。</p>
<p>好了，接下来我们把aX+b带入t中就得到了我们的逻辑回归的一般模型方程：</p>
<p><img src="https://camo.githubusercontent.com/e5f521c76bc20fcfe2eb1ea1513c28f08d30417ebe779d9b0a05fc514cdc0b74/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f4828612c62293d2535436672616325374231253744253742312b652535452537422861582b6229253744253744"></p>
<p>结果P也可以理解为概率，换句话说概率大于0.5的属于1分类，概率小于0.5的属于0分类，这就达到了分类的目的。</p>
<h3 id="损失函数是什么">3. 损失函数是什么</h3>
<p>逻辑回归的损失函数是 <strong>log loss</strong> ，也就是
<strong>对数似然函数</strong> ，函数公式如下：</p>
<figure>
<img src="https://camo.githubusercontent.com/0f83027d44496a9fb9de260dfd72167a89b5c66dd02563ceda33ec2405ec0e2d/68747470733a2f2f7778312e73696e61696d672e636e2f6c617267652f30303633304465666c793167347076747a337477396a333065743034763073772e6a7067" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<p>公式中的 y=1 表示的是真实值为1时用第一个公式，真实 y=0
用第二个公式计算损失。为什么要加上log函数呢？可以试想一下，当真实样本为1是，但h=0概率，那么log0=∞，这就对模型最大的惩罚力度；当h=1时，那么log1=0，相当于没有惩罚，也就是没有损失，达到最优结果。所以数学家就想出了用log函数来表示损失函数。</p>
<p>最后按照梯度下降法一样，求解极小值点，得到想要的模型效果。</p>
<h3 id="可以进行多分类吗">4.可以进行多分类吗？</h3>
<p>可以的，其实我们可以从二分类问题过度到多分类问题(one vs
rest)，思路步骤如下：</p>
<p>1.将类型class1看作正样本，其他类型全部看作负样本，然后我们就可以得到样本标记类型为该类型的概率p1。</p>
<p>2.然后再将另外类型class2看作正样本，其他类型全部看作负样本，同理得到p2。</p>
<p>3.以此循环，我们可以得到该待预测样本的标记类型分别为类型class
i时的概率pi，最后我们<strong>取pi中最大的那个概率对应的样本标记类型作为我们的待预测样本类型</strong>。</p>
<figure>
<img src="https://camo.githubusercontent.com/7d1db9dafb25ace5cf38c61b51be37b119420900bc30868db18ce15f7c388bc2/68747470733a2f2f7778322e73696e61696d672e636e2f6c617267652f30303633304465666c7931673470773131666f31746a3330637630633530746a2e6a7067" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<p>总之还是以二分类来依次划分，并求出最大概率结果.</p>
<h3 id="逻辑回归有什么优点">5.逻辑回归有什么优点</h3>
<ul>
<li>LR能以概率的形式输出结果，而非只是0,1判定。</li>
<li>LR的可解释性强，可控度高(你要给老板讲的嘛…)。</li>
<li>训练快，feature engineering之后效果赞。</li>
<li>因为结果是概率，可以做ranking model。</li>
</ul>
<h3 id="逻辑回归有哪些应用">6. 逻辑回归有哪些应用</h3>
<ul>
<li>CTR预估/推荐系统的learning to rank/各种分类场景。</li>
<li>某搜索引擎厂的广告CTR预估基线版是LR。</li>
<li>某电商搜索排序/广告CTR预估基线版是LR。</li>
<li>某电商的购物搭配推荐用了大量LR。</li>
<li>某现在一天广告赚1000w+的新闻app排序基线是LR。</li>
</ul>
<h3 id="逻辑回归常用的优化方法有哪些">7.
逻辑回归常用的优化方法有哪些</h3>
<h4 id="一阶方法">7.1 一阶方法</h4>
<p>梯度下降、随机梯度下降、mini
随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。</p>
<h4 id="二阶方法牛顿法拟牛顿法">7.2 二阶方法：牛顿法、拟牛顿法：</h4>
<p>这里详细说一下<strong>牛顿法</strong>的基本原理和牛顿法的应用方式。<strong>牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解</strong>。</p>
<p>在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了<strong>二阶求解问题</strong>，<strong>比一阶方法更快</strong>。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。</p>
<p>缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。</p>
<p>拟牛顿法：
不用二阶偏导而是<strong>构造出Hessian矩阵的近似正定对称矩阵</strong>的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、
L-BFGS（可以减少BFGS所需的存储空间）。</p>
<h3 id="逻辑斯特回归为什么要对特征进行离散化">8.
逻辑斯特回归为什么要对特征进行离散化。</h3>
<ol type="1">
<li>非线性！非线性！非线性！逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
离散特征的增加和减少都很容易，易于模型的快速迭代；</li>
<li>速度快！速度快！速度快！稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li>
<li>鲁棒性！鲁棒性！鲁棒性！离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li>
<li>方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</li>
<li>稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；</li>
<li>简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</li>
<li></li>
</ol>
<h3 id="逻辑回归的目标函数中增大l1正则化会是什么结果">9.
逻辑回归的目标函数中增大L1正则化会是什么结果。</h3>
<p>所有的参数w都会变成0。</p>
<h3 id="代码实现">10. 代码实现</h3>
<p>GitHub：<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/2.Logistics%20Regression/demo/CreditScoring.ipynb">https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/2.Logistics%20Regression/demo/CreditScoring.ipynb</a></p>
<p>.</p>
<p>把数据切分成训练集和测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line">x_tran,x_test,y_tran,y_test=model_selection.train_test_split(X,y,test_size=<span class="number">0.2</span>)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape)</span><br></pre></td></tr></table></figure>
<p>使用logistic
regression/决策树/SVM/KNN...等sklearn分类算法进行分类，尝试查sklearn
API了解模型参数含义，调整不同的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment">## https://blog.csdn.net/sun_shengyun/article/details/53811483</span></span><br><span class="line"><span class="comment">#multi_class=&#x27;ovr&#x27; 表示处理多分类问题时采用“One-vs-Rest”策略。</span></span><br><span class="line"><span class="comment">#solver=&#x27;sag&#x27; 选择随机平均梯度下降算法来求解优化问题。</span></span><br><span class="line"><span class="comment">#class_weight=&#x27;balanced&#x27; 用于处理类别不平衡的情况，使模型更关注少数类。</span></span><br><span class="line">lr=LogisticRegression(multi_class=<span class="string">&#x27;ovr&#x27;</span>,solver=<span class="string">&#x27;sag&#x27;</span>,class_weight=<span class="string">&#x27;balanced&#x27;</span>)</span><br><span class="line"><span class="comment">#使用训练数据 x_tran 和对应的标签 y_tran 来拟合模型</span></span><br><span class="line">lr.fit(x_tran,y_tran)</span><br><span class="line"><span class="comment">#计算模型在训练数据上的得分</span></span><br><span class="line">score=lr.score(x_tran,y_tran)</span><br><span class="line"><span class="built_in">print</span>(score) <span class="comment">##最好的分数是1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9323730412572769</span><br></pre></td></tr></table></figure>
<p>.</p>
<p>在测试集上进行预测，计算准确度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="comment">## https://blog.csdn.net/qq_16095417/article/details/79590455</span></span><br><span class="line">train_score=accuracy_score(y_tran,lr.predict(x_tran))</span><br><span class="line">test_score=lr.score(x_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集准确率：&#x27;</span>,train_score)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集准确率：&#x27;</span>,test_score)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">训练集准确率： 0.9323730412572769</span><br><span class="line">测试集准确率： 0.9332719742291763</span><br></pre></td></tr></table></figure>
<p>.</p>
<p>查看sklearn的官方说明，了解分类问题的评估标准，并对此例进行评估。</p>
<p>例如，如果这是一个将水果分为苹果、香蕉、橙子的多分类问题。召回率衡量的是对于每个类别，模型正确预测为该类别的样本数量占实际该类别样本数量的比例。通过计算训练集和测试集的召回率，可以评估模型在不同数据集上对各类别样本的预测能力，从而判断模型是否过拟合或欠拟合，以及模型在新数据上的泛化能力。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">##召回率</span><br><span class="line">from sklearn.metrics import recall_score</span><br><span class="line">train_recall=recall_score(y_tran,lr.predict(x_tran),average=&#x27;macro&#x27;)</span><br><span class="line">test_recall=recall_score(y_test,lr.predict(x_test),average=&#x27;macro&#x27;)</span><br><span class="line">print(&#x27;训练集召回率：&#x27;,train_recall)</span><br><span class="line">print(&#x27;测试集召回率：&#x27;,test_recall)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">训练集召回率： 0.4999938302834368</span><br><span class="line">测试集召回率： 0.4999753463833144</span><br></pre></td></tr></table></figure>
<p>.</p>
<p>银行通常会有更严格的要求，因为fraud带来的后果通常比较严重，一般我们会调整模型的标准。
比如在logistic
regression当中，一般我们的概率判定边界为0.5，但是我们可以把阈值设定低一些，来提高模型的“敏感度”，试试看把阈值设定为0.3，再看看这时的评估指标(主要是准确率和召回率)。</p>
<p>tips:sklearn的很多分类模型，predict_prob可以拿到预估的概率，可以根据它和设定的阈值大小去判断最终结果(分类类别)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">y_pro=lr.predict_proba(x_test) ##获取预测概率值</span><br><span class="line">y_prd2 = [list(p&gt;=0.3).index(1) for i,p in enumerate(y_pro)]   ##设定0.3阈值，把大于0.3的看成1分类。</span><br><span class="line">train_score=accuracy_score(y_test,y_prd2)</span><br><span class="line">print(train_score)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9333179935572941</span><br></pre></td></tr></table></figure>
<h2 id="随机森林">随机森林</h2>
<h3 id="什么是随机森林">1.什么是随机森林</h3>
<h4 id="bagging思想">1.1 Bagging思想</h4>
<p>Bagging是bootstrap
aggregating。思想就是从总体样本当中随机取一部分样本进行训练，通过多次这样的结果，进行投票获取平均值作为结果输出，这就极大可能的避免了不好的样本数据，从而提高准确度。因为有些是不好的样本，相当于噪声，模型学入噪声后会使准确度不高。</p>
<p><strong>举个例子</strong> ：</p>
<p>假设有1000个样本，如果按照以前的思维，是直接把这1000个样本拿来训练，但现在不一样，先抽取800个样本来进行训练，假如噪声点是这800个样本以外的样本点，就很有效的避开了。重复以上操作，提高模型输出的平均值。</p>
<h4 id="随机森林-1">1.2 随机森林</h4>
<p>Random
Forest(随机森林)是一种基于树模型的Bagging的优化版本，一棵树的生成肯定还是不如多棵树，因此就有了随机森林，<strong>解决决策树泛化能力弱的特点</strong>。(可以理解成三个臭皮匠顶过诸葛亮)</p>
<p>而同一批数据，用同样的算法只能产生一棵树，这时<strong>Bagging策略可以帮助我们产生不同的数据集</strong>。<strong>Bagging</strong>策略来源于bootstrap
aggregation：<strong>从样本集（假设样本集N个数据点）中重采样选出Nb个样本（有放回的采样，样本数据点个数仍然不变为N），在所有样本上，对这n个样本建立分类器（ID3）</strong>，重复以上两步m次，获得m个分类器，最后根据这m个分类器的投票结果，决定数据属于哪一类。</p>
<p>.</p>
<p><strong>每棵树的按照如下规则生成：</strong></p>
<ol type="1">
<li>如果训练集大小为N，对于每棵树而言，<strong>随机</strong>且有放回地从训练集中的抽取N个训练样本，作为该树的训练集；</li>
<li>如果每个样本的特征维度为M，指定一个常数m&lt;&lt;M，<strong>随机</strong>地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；</li>
<li>每棵树都尽最大程度的生长，并且没有剪枝过程。</li>
</ol>
<p>一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。</p>
<p>总的来说就是随机选择样本数，随机选取特征，随机选择分类器，建立多颗这样的决策树，然后通过这几课决策树来投票，决定数据属于哪一类(
<strong>投票机制有一票否决制、少数服从多数、加权多数</strong> )</p>
<h3 id="随机森林分类效果的影响因素">2. 随机森林分类效果的影响因素</h3>
<ul>
<li>森林中任意两棵树的相关性：相关性越大，错误率越大；</li>
<li>森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。</li>
</ul>
<p>减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。</p>
<p>.</p>
<h3 id="随机森林有什么优缺点">随机森林有什么优缺点</h3>
<p><strong>优点：</strong></p>
<ul>
<li>在当前的很多数据集上，相对其他算法有着很大的优势，表现良好。</li>
<li>它能够处理很高维度（feature很多）的数据，并且不用做特征选择(因为特征子集是随机选择的)。</li>
<li>在训练完后，它能够给出哪些feature比较重要。</li>
<li>训练速度快，容易做成并行化方法(训练时树与树之间是相互独立的)。</li>
<li>在训练过程中，能够检测到feature间的互相影响。</li>
<li>对于不平衡的数据集来说，它可以平衡误差。</li>
<li>如果有很大一部分的特征遗失，仍可以维持准确度。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>随机森林已经被证明在某些<strong>噪音较大</strong>的分类或回归问题上会过拟合。</li>
<li>对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。</li>
</ul>
<h3 id="随机森林如何处理缺失值">4. 随机森林如何处理缺失值？</h3>
<p>根据随机森林创建和训练的特点，随机森林对缺失值的处理还是比较特殊的。</p>
<ul>
<li>首先，给缺失值预设一些估计值，比如数值型特征，选择其余数据的中位数或众数作为当前的估计值</li>
<li>然后，根据估计的数值，建立随机森林，把所有的数据放进随机森林里面跑一遍。记录每一组数据在决策树中一步一步分类的路径.</li>
<li>判断哪组数据和缺失数据路径最相似，引入一个相似度矩阵，来记录数据之间的相似度，比如有N组数据，相似度矩阵大小就是N*N</li>
<li>如果缺失值是类别变量，通过权重投票得到新估计值，如果是数值型变量，通过加权平均得到新的估计值，如此迭代，直到得到稳定的估计值。</li>
</ul>
<p>其实，该缺失值填补过程类似于推荐系统中采用协同过滤进行评分预测，先计算缺失特征与其他特征的相似度，再加权得到缺失值的估计，而随机森林中计算相似度的方法（数据在决策树中一步一步分类的路径）乃其独特之处。</p>
<h3 id="什么是oob随机森林中oob是如何计算的它有什么优缺点">5.
什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？</h3>
<p><strong>OOB</strong> ：</p>
<p>上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob
error（out-of-bag error）。</p>
<p>bagging方法中Bootstrap每次约有1/3的样本不会出现在Bootstrap所采集的样本集合中，当然也就没有参加决策树的建立，把这1/3的数据称为
<strong>袋外数据oob（out of bag）</strong>
,它可以用于取代测试集误差估计方法。</p>
<p><strong>袋外数据(oob)误差的计算方法如下：</strong></p>
<ul>
<li>对于已经生成的随机森林,用袋外数据测试其性能,假设袋外数据总数为O,用这O个袋外数据作为输入,带进之前已经生成的随机森林分类器,分类器会给出O个数据相应的分类</li>
<li>因为这O条数据的类型是已知的,则用正确的分类与随机森林分类器的结果进行比较,统计随机森林分类器分类错误的数目,设为X,则袋外数据误差大小=X/O</li>
</ul>
<p><strong>优缺点</strong> ：</p>
<p>这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。</p>
<h3 id="随机森林的过拟合问题">6. 随机森林的过拟合问题</h3>
<ol type="1">
<li>你已经建了一个有10000棵树的随机森林模型。在得到0.00的训练误差后，你非常高兴。但是，验证错误是34.23。到底是怎么回事？你还没有训练好你的模型吗？
答：该模型过度拟合，因此，为了避免这些情况，我们要用交叉验证来调整树的数量。</li>
</ol>
<h3 id="代码实现-1">7. 代码实现</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.1%20Random%20Forest/3.1%20Random%20Forest.md#7-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"></a></p>
<p>GitHub：<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.1%20Random%20Forest/RandomForestRegression.ipynb">https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.1%20Random%20Forest/RandomForestRegression.ipynb</a></p>
<h4 id="import工具库">0.import工具库</h4>
<p>In [1]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from sklearn import preprocessing</span><br><span class="line">from sklearn.ensemble import RandomForestRegressor</span><br><span class="line">from sklearn.datasets import load_boston</span><br></pre></td></tr></table></figure>
<h4 id="加载数据">1.加载数据</h4>
<p>In [2]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">boston_house = load_boston()</span><br></pre></td></tr></table></figure>
<p>In [3]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">boston_feature_name = boston_house.feature_names</span><br><span class="line">boston_features = boston_house.data</span><br><span class="line">boston_target = boston_house.target</span><br></pre></td></tr></table></figure>
<p>In [4]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">boston_feature_name</span><br></pre></td></tr></table></figure>
<p>Out[4]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([&#x27;CRIM&#x27;, &#x27;ZN&#x27;, &#x27;INDUS&#x27;, &#x27;CHAS&#x27;, &#x27;NOX&#x27;, &#x27;RM&#x27;, &#x27;AGE&#x27;, &#x27;DIS&#x27;, &#x27;RAD&#x27;,</span><br><span class="line">       &#x27;TAX&#x27;, &#x27;PTRATIO&#x27;, &#x27;B&#x27;, &#x27;LSTAT&#x27;],</span><br><span class="line">      dtype=&#x27;|S7&#x27;)</span><br></pre></td></tr></table></figure>
<h4 id="构建模型">构建模型</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">help(RandomForestRegressor)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Help on class RandomForestRegressor in module sklearn.ensemble.forest:</span><br><span class="line"></span><br><span class="line">class RandomForestRegressor(ForestRegressor)</span><br><span class="line"> |  A random forest regressor.</span><br><span class="line"> |  </span><br><span class="line"> |  A random forest is a meta estimator that fits a number of classifying</span><br><span class="line"> |  decision trees on various sub-samples of the dataset and use averaging</span><br><span class="line"> |  to improve the predictive accuracy and control over-fitting.</span><br><span class="line"> |  The sub-sample size is always the same as the original</span><br><span class="line"> |  input sample size but the samples are drawn with replacement if</span><br><span class="line"> |  `bootstrap=True` (default).</span><br><span class="line"> |  </span><br><span class="line"> |  Read more in the :ref:`User Guide &lt;forest&gt;`.</span><br><span class="line"> |  </span><br><span class="line"> |  Parameters</span><br><span class="line"> |  ----------</span><br><span class="line"> |  n_estimators : integer, optional (default=10)</span><br><span class="line"> |      The number of trees in the forest.</span><br><span class="line"> |  </span><br><span class="line"> |  criterion : string, optional (default=&quot;mse&quot;)</span><br><span class="line"> |      The function to measure the quality of a split. Supported criteria</span><br><span class="line"> |      are &quot;mse&quot; for the mean squared error, which is equal to variance</span><br><span class="line"> |      reduction as feature selection criterion, and &quot;mae&quot; for the mean</span><br><span class="line"> |      absolute error.</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rgs = RandomForestRegressor(n_estimators=15)  ##随机森林模型</span><br><span class="line">rgs = rgs.fit(boston_features, boston_target)</span><br></pre></td></tr></table></figure>
<p>In [10]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rgs</span><br></pre></td></tr></table></figure>
<p>Out[10]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RandomForestRegressor(bootstrap=True, criterion=&#x27;mse&#x27;, max_depth=None,</span><br><span class="line">           max_features=&#x27;auto&#x27;, max_leaf_nodes=None,</span><br><span class="line">           min_impurity_decrease=0.0, min_impurity_split=None,</span><br><span class="line">           min_samples_leaf=1, min_samples_split=2,</span><br><span class="line">           min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,</span><br><span class="line">           oob_score=False, random_state=None, verbose=0, warm_start=False)</span><br></pre></td></tr></table></figure>
<p>In [11]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rgs.predict(boston_features)</span><br></pre></td></tr></table></figure>
<p>Out[11]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">array([ 26.16666667,  22.24      ,  33.76666667,  33.67333333,</span><br><span class="line">        35.46      ,  26.74      ,  21.84      ,  26.59333333,</span><br><span class="line">        20.31333333,  19.88      ,  18.53333333,  19.57333333,</span><br><span class="line">        21.64      ,  19.64666667,  19.37333333,  19.92      ,</span><br><span class="line">        22.3       ,  17.8       ,  19.64      ,  18.73333333,</span><br><span class="line">        13.80666667,  18.42666667,  15.5       ,  14.46666667,</span><br><span class="line">        15.54666667,  14.18      ,  16.34666667,  14.58666667,</span><br><span class="line">        18.25333333,  21.46      ,  13.38      ,  15.86      ,</span><br><span class="line">        14.09333333,  13.8       ,  13.71333333,  19.51333333,</span><br><span class="line">        19.96666667,  21.49333333,  23.56666667,  30.07333333,</span><br><span class="line">        34.7       ,  28.31333333,  25.04      ,  24.64666667,</span><br><span class="line">        21.45333333,  19.42      ,  19.74      ,  18.26666667,</span><br><span class="line">        19.57333333,  20.02666667,  20.74666667,  20.84666667,</span><br><span class="line">        25.42666667,  22.25333333,  19.12666667,  34.66666667,</span><br><span class="line">        24.3       ,  32.18666667,  23.42666667,  19.78      ,</span><br><span class="line">        18.78666667,  17.82666667,  23.03333333,  25.81333333,</span><br><span class="line">        32.30666667,  23.85333333,  20.3       ,  21.85333333,</span><br><span class="line">        18.59333333,  21.10666667,  24.42      ,  21.25333333,</span><br><span class="line">        23.2       ,  23.56666667,  24.22      ,  22.24      ,</span><br><span class="line">        20.08      ,  21.34666667,  21.24      ,  20.49333333,</span><br><span class="line">        27.87333333,  24.4       ,  24.16666667,  22.98      ,</span><br><span class="line">        23.48666667,  27.13333333,  21.23333333,  22.28666667,</span><br><span class="line">        26.63333333,  29.01333333,  22.4       ,  22.06666667,</span><br><span class="line">        22.75333333,  24.77333333,  21.38666667,  26.91333333,</span><br><span class="line">        21.57333333,  40.41333333,  44.06      ,  32.64666667,</span><br><span class="line">        26.9       ,  25.90666667,  19.05333333,  19.85333333,</span><br><span class="line">        20.00666667,  19.73333333,  19.09333333,  20.34666667,</span><br><span class="line">        20.21333333,  19.20666667,  21.24      ,  24.06      ,</span><br><span class="line">        18.76666667,  18.66      ,</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import tree</span><br></pre></td></tr></table></figure>
<p>In [13]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rgs2 = tree.DecisionTreeRegressor()           ##决策树模型，比较两个模型的预测结果！</span><br><span class="line">rgs2.fit(boston_features, boston_target)</span><br></pre></td></tr></table></figure>
<p>Out[13]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DecisionTreeRegressor(criterion=&#x27;mse&#x27;, max_depth=None, max_features=None,</span><br><span class="line">           max_leaf_nodes=None, min_impurity_decrease=0.0,</span><br><span class="line">           min_impurity_split=None, min_samples_leaf=1,</span><br><span class="line">           min_samples_split=2, min_weight_fraction_leaf=0.0,</span><br><span class="line">           presort=False, random_state=None, splitter=&#x27;best&#x27;)</span><br></pre></td></tr></table></figure>
<p>In [14]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rgs2.predict(boston_features)</span><br></pre></td></tr></table></figure>
<p>Out[14]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">array([ 24. ,  21.6,  34.7,  33.4,  36.2,  28.7,  22.9,  27.1,  16.5,</span><br><span class="line">        18.9,  15. ,  18.9,  21.7,  20.4,  18.2,  19.9,  23.1,  17.5,</span><br><span class="line">        20.2,  18.2,  13.6,  19.6,  15.2,  14.5,  15.6,  13.9,  16.6,</span><br><span class="line">        14.8,  18.4,  21. ,  12.7,  14.5,  13.2,  13.1,  13.5,  18.9,</span><br><span class="line">        20. ,  21. ,  24.7,  30.8,  34.9,  26.6,  25.3,  24.7,  21.2,</span><br><span class="line">        19.3,  20. ,  16.6,  14.4,  19.4,  19.7,  20.5,  25. ,  23.4,</span><br><span class="line">        18.9,  35.4,  24.7,  31.6,  23.3,  19.6,  18.7,  16. ,  22.2,</span><br><span class="line">        25. ,  33. ,  23.5,  19.4,  22. ,  17.4,  20.9,  24.2,  21.7,</span><br><span class="line">        22.8,  23.4,  24.1,  21.4,  20. ,  20.8,  21.2,  20.3,  28. ,</span><br><span class="line">        23.9,  24.8,  22.9,  23.9,  26.6,  22.5,  22.2,  23.6,  28.7,</span><br><span class="line">        22.6,  22. ,  22.9,  25. ,  20.6,  28.4,  21.4,  38.7,  43.8,</span><br><span class="line">        33.2,  27.5,  26.5,  18.6,  19.3,  20.1,  19.5,  19.5,  20.4,</span><br><span class="line">        19.8,  19.4,  21.7,  22.8,  18.8,  18.7,  18.5,  18.3,  21.2,</span><br><span class="line">        19.2,  20.4,  19.3,  22. ,  20.3,  20.5,  17.3,  18.8,  21.4,</span><br><span class="line">        15.7,  16.2,  18. ,  14.3,  19.2,  19.6,  23. ,  18.4,  15.6,</span><br><span class="line">        18.1,  17.4,  17.1,  13.3,  17.8,  14. ,  14.4,  13.4,  15.6,</span><br><span class="line">        11.8,  13.8,  15.6,  14.6,  17.8,  15.4,  21.5,  19.6,  15.3,</span><br><span class="line">        19.4,  17. ,  15.6,  13.1,  41.3,  24.3,  23.3,  27. ,  50. ,</span><br><span class="line">        50. ,  50. ,  22.7,  25. ,  50. ,  23.8,  23.8,  22.3,  17.4,</span><br><span class="line">        19.1,  23.1,  23.6,  22.6,  29.4,  23.2,  24.6,  29.9,  37.2,</span><br><span class="line">        39.8,  36.2,  37.9,  32.5,  26.4,  29.6,  50. ,  32. ,  29.8,</span><br><span class="line">        34.9,  37. ,  30.5,  36.4,  31.1,  29.1,  50. ,  33.3,  30.3,</span><br><span class="line">        34.6,  34.9,  32.9,  24.1,  42.3,  48.5,  50. ,  22.6,  24.4,</span><br></pre></td></tr></table></figure>
<h2 id="gbdtgradient-boosting-decision-tree全名叫梯度提升决策树">GBDT(Gradient
Boosting Decision Tree)，全名叫梯度提升决策树</h2>
<p>GBDT(Gradient Boosting Decision
Tree)，全名叫梯度提升决策树，使用的是<strong>Boosting</strong>的思想。</p>
<h3 id="解释一下gbdt算法的过程">1. 解释一下GBDT算法的过程</h3>
<p>GBDT(Gradient Boosting Decision
Tree)，全名叫梯度提升决策树，使用的是<strong>Boosting</strong>的思想。</p>
<h4 id="boosting思想">1.1 Boosting思想</h4>
<p>Boosting方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。</p>
<p>Bagging与Boosting的串行训练方式不同，Bagging方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练</p>
<h4 id="gbdt原来是这么回事">1.2 GBDT原来是这么回事</h4>
<p>GBDT的原理很简单，就是<strong>所有弱分类器的结果相加等于预测值</strong>，然后下一个弱分类器去拟合误差函数对预测值的残差(这个残差就是预测值与真实值之间的误差)。当然了，它里面的弱分类器的表现形式就是各棵树。</p>
<p>举一个非常简单的例子，比如我今年30岁了，但计算机或者模型GBDT并不知道我今年多少岁，那GBDT咋办呢？</p>
<ul>
<li>它会在第一个弱分类器（或第一棵树中）随便用一个年龄比如20岁来拟合，然后发现误差有10岁；</li>
<li>接下来在第二棵树中，用6岁去拟合剩下的损失，发现差距还有4岁；</li>
<li>接着在第三棵树中用3岁拟合剩下的差距，发现差距只有1岁了；</li>
<li>最后在第四课树中用1岁拟合剩下的残差，完美。</li>
<li>最终，四棵树的结论加起来，就是真实年龄30岁（实际工程中，gbdt是计算负梯度，用负梯度近似残差）。</li>
</ul>
<p><strong>为何gbdt可以用用负梯度近似残差呢？</strong></p>
<p>回归任务下，GBDT
在每一轮的迭代时对每个样本都会有一个预测值，此时的损失函数为均方差损失函数，</p>
<p><img src="https://camo.githubusercontent.com/56da71ded4d540860f6dcccfeb57da8706c6ac3be305d35f7a8b21a9b2cd3ee6/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353231343936323033343934343633382e676966"></p>
<p>那此时的负梯度是这样计算的</p>
<p><img src="https://camo.githubusercontent.com/132247f994342d19fabbb36540e1e190b2810596766b958c50d072ab0e961b31/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353231343936323431363637303937332e676966"></p>
<p>所以，当损失函数选用均方损失函数是时，每一次拟合的值就是（真实值 -
当前模型预测的值），即残差。此时的变量是<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/a5daaa4e1550d7aab48422ac1e68305a79bdc608b243c0efff33a1742537e864/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353231343936333633333236373933382e676966"><img src="https://camo.githubusercontent.com/a5daaa4e1550d7aab48422ac1e68305a79bdc608b243c0efff33a1742537e864/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353231343936333633333236373933382e676966"></a>，即“当前预测模型的值”，也就是对它求负梯度。</p>
<p><strong>训练过程</strong></p>
<p>简单起见，假定训练集只有4个人：A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下图所示结果：</p>
<p><img src="https://camo.githubusercontent.com/87e2954d28205a4a68b7086aab3098637a186fd8bbddcfa74340c92af32f4329/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135333433383536383139313330333935382e706e67"></p>
<p>现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点最多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图所示结果：</p>
<p><img src="https://camo.githubusercontent.com/d036bd5e6cbb88324cd280894976060668437f14aebae96d1c9bbe4e4cbafe7d/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135333433383537303532393235363839352e706e67"></p>
<p>在第一棵树分枝和图1一样，由于A,B年龄较为相近，C,D年龄较为相近，他们被分为左右两拨，每拨用平均年龄作为预测值。</p>
<ul>
<li>此时计算残差（残差的意思就是：A的实际值 - A的预测值 =
A的残差），所以A的残差就是实际值14 - 预测值15 = 残差值-1。</li>
<li>注意，A的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是15，如果还有树则需要都累加起来作为A的预测值。</li>
</ul>
<p>然后拿它们的残差-1、1、-1、1代替A B C
D的原值，到第二棵树去学习，第二棵树只有两个值1和-1，直接分成两个节点，即A和C分在左边，B和D分在右边，经过计算（比如A，实际值-1
- 预测值-1 = 残差0，比如C，实际值-1 - 预测值-1 =
0），此时所有人的残差都是0。<strong>残差值都为0</strong>，<strong>相当于第二棵树的预测值和它们的实际值相等</strong>，则<strong>只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了，即每个人都得到了真实的预测值。</strong></p>
<p>换句话说，现在A,B,C,D的预测值都和真实年龄一致了。Perfect！</p>
<ul>
<li>A: 14岁高一学生，购物较少，经常问学长问题，预测年龄A = 15 – 1 =
14</li>
<li>B: 16岁高三学生，购物较少，经常被学弟问问题，预测年龄B = 15 + 1 =
16</li>
<li>C: 24岁应届毕业生，购物较多，经常问师兄问题，预测年龄C = 25 – 1 =
24</li>
<li>D: 26岁工作两年员工，购物较多，经常被师弟问问题，预测年龄D = 25 + 1
= 26</li>
</ul>
<p>所以，GBDT需要将多棵树的得分累加得到最终的预测得分，且每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差。</p>
<h3 id="梯度提升和梯度下降的区别和联系是什么">2.
梯度提升和梯度下降的区别和联系是什么？</h3>
<p>下表是梯度提升算法和梯度下降算法的对比情况。可以发现，两者<strong>都是</strong>在每
一轮迭代中，利用损失函数相对于模型的<strong>负梯度方向的信息</strong>来对当前模型进行更
新，</p>
<p>只不过在<strong>梯度下降</strong>中，模型是以参数化形式表示，从而模型的更新等价于参
数的更新。</p>
<p>而在<strong>梯度提升</strong>中，<strong>模型并不需要进行参数化表示，而是直接定义在函
数空间中，从而大大扩展了可以使用的模型种类。</strong></p>
<p><img src="https://camo.githubusercontent.com/8013736490003624e4964476ec341d28c81cdba8594163f127f2bb28feb53235/687474703a2f2f7778332e73696e61696d672e636e2f6d773639302f3030363330446566677931673474647768717a73646a3330727030616664686f2e6a7067"></p>
<h3 id="gbdt的优点和局限性有哪些">3.
<strong>GBDT</strong>的优点和局限性有哪些？</h3>
<h4 id="优点">3.1 优点</h4>
<ol type="1">
<li>预测阶段的计算速度快，树与树之间可并行化计算。</li>
<li>在分布稠密的数据集上，泛化能力和表达能力都很好，这使得GBDT在Kaggle的众多竞赛中，经常名列榜首。</li>
<li>采用决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系。</li>
</ol>
<h4 id="局限性">3.2 局限性</h4>
<ol type="1">
<li>GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。</li>
<li>GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。</li>
<li>训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。</li>
</ol>
<h3 id="rf随机森林与gbdt之间的区别与联系">4.
RF(随机森林)与GBDT之间的区别与联系</h3>
<p><strong>相同点</strong> ：</p>
<ul>
<li>都是由多棵树组成，最终的结果都是由多棵树一起决定。</li>
<li>RF和GBDT在使用CART树时，可以是分类树或者回归树。</li>
</ul>
<p><strong>不同点</strong> ：</p>
<ul>
<li>组成随机森林的树可以并行生成，而GBDT是<strong>串行</strong>生成</li>
<li>随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和</li>
<li>随机森林对异常值不敏感，而GBDT对异常值比较敏感</li>
<li>随机森林是减少模型的方差，而GBDT是减少模型的偏差</li>
<li>随机森林不需要进行特征归一化。而GBDT则需要进行特征归一化</li>
</ul>
<h3 id="代码实现-2">5. 代码实现</h3>
<p>GitHub：<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.2%20GBDT/GBDT_demo.ipynb">https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.2%20GBDT/GBDT_demo.ipynb</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br></pre></td></tr></table></figure>
<h4 id="获取训练数据">获取训练数据</h4>
<p>In [54]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_feature = np.genfromtxt(<span class="string">&quot;train_feat.txt&quot;</span>,dtype=np.float32)</span><br><span class="line">num_feature = <span class="built_in">len</span>(train_feature[<span class="number">0</span>])</span><br><span class="line">train_feature = pd.DataFrame(train_feature)</span><br><span class="line"></span><br><span class="line">train_label = train_feature.iloc[:, num_feature - <span class="number">1</span>]</span><br><span class="line">train_feature = train_feature.iloc[:, <span class="number">0</span>:num_feature - <span class="number">2</span>]</span><br><span class="line">train_feature</span><br></pre></td></tr></table></figure>
<p>Out[54]:</p>
<table>
<colgroup>
<col style="width: 1%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 13%">
</colgroup>
<thead>
<tr>
<th></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.005988</td>
<td>0.569231</td>
<td>0.647059</td>
<td>0.951220</td>
<td>-0.225434</td>
<td>0.837989</td>
<td>0.357258</td>
<td>-0.003058</td>
</tr>
<tr>
<td>1</td>
<td>0.161677</td>
<td>0.743195</td>
<td>0.682353</td>
<td>0.960976</td>
<td>-0.086705</td>
<td>0.780527</td>
<td>0.282945</td>
<td>0.149847</td>
</tr>
<tr>
<td>2</td>
<td>0.113772</td>
<td>0.744379</td>
<td>0.541176</td>
<td>0.990244</td>
<td>-0.005780</td>
<td>0.721468</td>
<td>0.434110</td>
<td>-0.318043</td>
</tr>
<tr>
<td>3</td>
<td>0.053892</td>
<td>0.608284</td>
<td>0.764706</td>
<td>0.951220</td>
<td>-0.248555</td>
<td>0.821229</td>
<td>0.848604</td>
<td>-0.003058</td>
</tr>
<tr>
<td>4</td>
<td>0.173653</td>
<td>0.866272</td>
<td>0.682353</td>
<td>0.951220</td>
<td>0.017341</td>
<td>0.704709</td>
<td>-0.021002</td>
<td>-0.195719</td>
</tr>
</tbody>
</table>
<p>In [55]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_label</span><br></pre></td></tr></table></figure>
<p>Out[55]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0    320.0</span><br><span class="line">1    361.0</span><br><span class="line">2    364.0</span><br><span class="line">3    336.0</span><br><span class="line">4    358.0</span><br><span class="line">Name: 9, dtype: float32</span><br></pre></td></tr></table></figure>
<h4 id="获取测试数据">获取测试数据</h4>
<p>In [56]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">test_feature = np.genfromtxt(&quot;test_feat.txt&quot;,dtype=np.float32)</span><br><span class="line">num_feature = len(test_feature[0])</span><br><span class="line">test_feature = pd.DataFrame(test_feature)</span><br><span class="line"></span><br><span class="line">test_label = test_feature.iloc[:, num_feature - 1]</span><br><span class="line">test_feature = test_feature.iloc[:, 0:num_feature - 2]</span><br><span class="line">test_feature</span><br></pre></td></tr></table></figure>
<p>Out[56]:</p>
<table>
<colgroup>
<col style="width: 1%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 13%">
</colgroup>
<thead>
<tr>
<th></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.005988</td>
<td>0.569231</td>
<td>0.647059</td>
<td>0.951220</td>
<td>-0.225434</td>
<td>0.837989</td>
<td>0.357258</td>
<td>-0.003058</td>
</tr>
<tr>
<td>1</td>
<td>0.161677</td>
<td>0.743195</td>
<td>0.682353</td>
<td>0.960976</td>
<td>-0.086705</td>
<td>0.780527</td>
<td>0.282945</td>
<td>0.149847</td>
</tr>
<tr>
<td>2</td>
<td>0.113772</td>
<td>0.744379</td>
<td>0.541176</td>
<td>0.990244</td>
<td>-0.005780</td>
<td>0.721468</td>
<td>0.434110</td>
<td>-0.318043</td>
</tr>
<tr>
<td>3</td>
<td>0.053892</td>
<td>0.608284</td>
<td>0.764706</td>
<td>0.951220</td>
<td>-0.248555</td>
<td>0.821229</td>
<td>0.848604</td>
<td>-0.003058</td>
</tr>
<tr>
<td>4</td>
<td>0.173653</td>
<td>0.866272</td>
<td>0.682353</td>
<td>0.951220</td>
<td>0.017341</td>
<td>0.704709</td>
<td>-0.021002</td>
<td>-0.195719</td>
</tr>
</tbody>
</table>
<p>In [57]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_label</span><br></pre></td></tr></table></figure>
<p>Out[57]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0    320.0</span><br><span class="line">1    361.0</span><br><span class="line">2    364.0</span><br><span class="line">3    336.0</span><br><span class="line">4    358.0</span><br><span class="line">Name: 9, dtype: float32</span><br></pre></td></tr></table></figure>
<h4 id="gbdt模型建立">GBDT模型建立</h4>
<p>In [58]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">gbdt = GradientBoostingRegressor(</span><br><span class="line">  loss = &#x27;ls&#x27;</span><br><span class="line">, learning_rate = 0.1</span><br><span class="line">, n_estimators = 100</span><br><span class="line">, subsample = 1</span><br><span class="line">, min_samples_split = 2</span><br><span class="line">, min_samples_leaf = 1</span><br><span class="line">, max_depth = 3</span><br><span class="line">, init = None</span><br><span class="line">, random_state = None</span><br><span class="line">, max_features = None</span><br><span class="line">, alpha = 0.9</span><br><span class="line">, verbose = 0</span><br><span class="line">, max_leaf_nodes = None</span><br><span class="line">, warm_start = False</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">gbdt.fit(train_feature, train_label)</span><br><span class="line">pred = gbdt.predict(test_feature)</span><br><span class="line">total_err = 0</span><br><span class="line"></span><br><span class="line">for i in range(pred.shape[0]):</span><br><span class="line">    print(&#x27;pred:&#x27;, pred[i], &#x27; label:&#x27;, test_label[i])</span><br><span class="line">print(&#x27;均方误差:&#x27;, np.sqrt(((pred - test_label) ** 2).mean()))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pred: 320.0008173984891  label: 320.0</span><br><span class="line">pred: 360.99965033119537  label: 361.0</span><br><span class="line">pred: 363.99928183902097  label: 364.0</span><br><span class="line">pred: 336.0002344322584  label: 336.0</span><br><span class="line">pred: 358.0000159974151  label: 358.0</span><br><span class="line">均方误差: 0.0005218003748239915</span><br></pre></td></tr></table></figure>
<h2 id="xgboost">XGBoost</h2>
<h3 id="什么是xgboost">1. 什么是XGBoost</h3>
<p>XGBoost是陈天奇等人开发的一个开源机器学习项目，高效地实现了GBDT算法并进行了算法和工程上的许多改进，被广泛应用在Kaggle竞赛及其他许多机器学习竞赛中并取得了不错的成绩。</p>
<p>说到XGBoost，不得不提GBDT(Gradient Boosting Decision
Tree)。因为XGBoost本质上还是一个GBDT，但是力争把速度和效率发挥到极致，所以叫X
(Extreme) GBoosted。包括前面说过，两者都是boosting方法。</p>
<p>关于GBDT，这里不再提，可以查看我前一篇的介绍，<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.2%20GBDT/3.2%20GBDT.md">点此跳转</a>。</p>
<h4 id="xgboost树的定义">1.1 XGBoost树的定义</h4>
<p>先来举个 <strong>例子</strong>
，我们要预测一家人对电子游戏的喜好程度，考虑到年轻和年老相比，年轻更可能喜欢电子游戏，以及男性和女性相比，男性更喜欢电子游戏，故先根据年龄大小区分小孩和大人，然后再通过性别区分开是男是女，逐一给各人在电子游戏喜好程度上打分，如下图所示。</p>
<p><img src="https://camo.githubusercontent.com/5f23a430faa189019542bc487f4efc8682de3a61e35a5f4dcc9d057359dcc316/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135333433383537373233323531363830302e706e67"></p>
<p>就这样，训练出了2棵树tree1和tree2，类似之前gbdt的原理，两棵树的结论累加起来便是最终的结论，所以小孩的预测分数就是两棵树中小孩所落到的结点的分数相加：2
+ 0.9 = 2.9。爷爷的预测分数同理：-1 + （-0.9）=
-1.9。具体如下图所示：</p>
<p><img src="https://camo.githubusercontent.com/7792e7b3d54f94926b16ada4e12427f4f3b15428d2b94584323512f7efb682c6/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135333433383537383733393139383433332e706e67"></p>
<p>恩，你可能要拍案而起了，惊呼，这不是跟上文介绍的GBDT乃异曲同工么？</p>
<p>事实上，如果不考虑工程实现、解决问题上的一些差异，XGBoost与GBDT比较大的不同就是目标函数的定义。XGBoost的目标函数如下图所示：</p>
<p><img src="https://camo.githubusercontent.com/2a5735441ade41b49e3b6664ab018b0b1afa8739db9fc0e8feb23d7605d9b040/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135333433383538303133393135393539332e706e67"></p>
<p>其中：</p>
<ul>
<li>红色箭头所指向的L 即为损失函数（比如平方损失函数：<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/3a3f2e8158b229d9e112baa70d76a54816e53061ba969099de0988c610e09a0c/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f6c28795f692c7925354569293d28795f692d79253545692925354532"><img src="https://camo.githubusercontent.com/3a3f2e8158b229d9e112baa70d76a54816e53061ba969099de0988c610e09a0c/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f6c28795f692c7925354569293d28795f692d79253545692925354532"></a>)</li>
<li>红色方框所框起来的是正则项（包括L1正则、L2正则）</li>
<li>红色圆圈所圈起来的为常数项</li>
<li>对于f(x)，XGBoost利用泰勒展开三项，做一个近似。<strong>f(x)表示的是其中一颗回归树。</strong></li>
</ul>
<p>看到这里可能有些读者会头晕了，这么多公式，
<strong>我在这里只做一个简要式的讲解，具体的算法细节和公式求解请查看这篇博文，讲得很仔细</strong>
：<a target="_blank" rel="noopener" href="https://blog.csdn.net/v_JULY_v/article/details/81410574">通俗理解kaggle比赛大杀器xgboost</a></p>
<p>XGBoost的<strong>核心算法思想</strong>不难，基本就是：</p>
<ol type="1">
<li>不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数
<strong>f(x)</strong> ，去拟合上次预测的残差。</li>
<li>当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数</li>
<li>最后只需要将每棵树对应的分数加起来就是该样本的预测值。</li>
</ol>
<p>显然，我们的目标是要使得树群的预测值<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/d1fa2600bb2fdc31c245153451e1b86af718480370457ed0c1d9954d3e632ad4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f795f6925354525374227253744"><img src="https://camo.githubusercontent.com/d1fa2600bb2fdc31c245153451e1b86af718480370457ed0c1d9954d3e632ad4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f795f6925354525374227253744" alt="img"></a>尽量接近真实值<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/0e28c816835a8a00c28dab343e8b8cb90100c7b4832cea590264c9e22cdeb712/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f795f69"><img src="https://camo.githubusercontent.com/0e28c816835a8a00c28dab343e8b8cb90100c7b4832cea590264c9e22cdeb712/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f795f69" alt="img"></a>，而且有尽量大的泛化能力。类似之前GBDT的套路，XGBoost也是需要将多棵树的得分累加得到最终的预测得分（每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差）。</p>
<p><img src="https://camo.githubusercontent.com/6e419fb4eee31d5c36e86f39552628c57ea58f918f8d081cb4add7228f86f47c/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135333433383635373236313833333439332e706e67"></p>
<p>那接下来，我们如何选择每一轮加入什么 f 呢？答案是非常直接的，选取一个
f 来使得我们的目标函数尽量最大地降低。这里 f
可以使用泰勒展开公式近似。</p>
<p><img src="https://camo.githubusercontent.com/b629fad546d9f568a29c6bd5b7d5c85bc26394cdd0e3b33fa74a063a31991b9c/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f7175657362617365363431353334333836353836373533303132302e706e67"></p>
<p>实质是把样本分配到叶子结点会对应一个obj，优化过程就是obj优化。也就是分裂节点到叶子不同的组合，不同的组合对应不同obj，所有的优化围绕这个思想展开。到目前为止我们讨论了目标函数中的第一个部分：训练误差。接下来我们讨论目标函数的第二个部分：正则项，即如何定义树的复杂度。</p>
<h4 id="正则项树的复杂度">1.2 正则项：树的复杂度</h4>
<p>XGBoost对树的复杂度包含了两个部分：</p>
<ul>
<li>一个是树里面叶子节点的个数T</li>
<li>一个是树上叶子节点的得分w的L2模平方（对w进行L2正则化，相当于针对每个叶结点的得分增加L2平滑，目的是为了避免过拟合）</li>
</ul>
<p><img src="https://camo.githubusercontent.com/ad4e284372673bd457bd3cd18fb3cbf5b169760eda736d442e0dd20d7a8328f7/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135333433383637343139393437313438332e706e67"></p>
<p>我们再来看一下XGBoost的目标函数（损失函数揭示训练误差 +
正则化定义复杂度）：</p>
<p><img src="https://camo.githubusercontent.com/242a53033214ad30be8bb3b112a5379ca4f47383832fa6cddeb84f4ae5b810ab/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f4c28253543706869293d25354373756d5f253742692537446c28795f69253545253742272537442d795f69292b25354373756d5f6b2535434f6d65676128665f7429"></p>
<p>正则化公式也就是目标函数的后半部分，对于上式而言，<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/d1fa2600bb2fdc31c245153451e1b86af718480370457ed0c1d9954d3e632ad4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f795f6925354525374227253744"><img src="https://camo.githubusercontent.com/d1fa2600bb2fdc31c245153451e1b86af718480370457ed0c1d9954d3e632ad4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f795f6925354525374227253744"></a>是整个累加模型的输出，正则化项∑kΩ(ft)是则表示树的复杂度的函数，值越小复杂度越低，泛化能力越强。</p>
<h4 id="树该怎么长">1.3 树该怎么长</h4>
<p><a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/3.3%20XGBoost#13-%E6%A0%91%E8%AF%A5%E6%80%8E%E4%B9%88%E9%95%BF"></a></p>
<p>很有意思的一个事是，我们从头到尾了解了xgboost如何优化、如何计算，但树到底长啥样，我们却一直没看到。很显然，一棵树的生成是由一个节点一分为二，然后不断分裂最终形成为整棵树。那么树怎么分裂的就成为了接下来我们要探讨的关键。对于一个叶子节点如何进行分裂，XGBoost作者在其原始论文中给出了一种分裂节点的方法：<strong>枚举所有不同树结构的贪心法</strong></p>
<p>不断地枚举不同树的结构，然后利用打分函数来寻找出一个最优结构的树，接着加入到模型中，不断重复这样的操作。这个寻找的过程使用的就是
<strong>贪心算法</strong> 。选择一个feature分裂，计算loss
function最小值，然后再选一个feature分裂，又得到一个loss
function最小值，你枚举完，找一个效果最好的，把树给分裂，就得到了小树苗。</p>
<p>总而言之，XGBoost使用了和CART回归树一样的想法，利用贪婪算法，遍历所有特征的所有特征划分点，不同的是使用的目标函数不一样。具体做法就是分裂后的目标函数值比单子叶子节点的目标函数的增益，同时为了限制树生长过深，还加了个阈值，只有当增益大于该阈值才进行分裂。从而继续分裂，形成一棵树，再形成一棵树，<strong>每次在上一次的预测基础上取最优进一步分裂/建树。</strong></p>
<h4 id="如何停止树的循环生成">1.4 如何停止树的循环生成</h4>
<p>凡是这种循环迭代的方式必定有停止条件，什么时候停止呢？简言之，设置树的最大深度、当样本权重和小于设定阈值时停止生长以防止过拟合。具体而言，则</p>
<ol type="1">
<li>当引入的分裂带来的增益小于设定阀值的时候，我们可以忽略掉这个分裂，所以并不是每一次分裂loss
function整体都会增加的，有点预剪枝的意思，阈值参数为（即正则项里叶子节点数T的系数）；</li>
<li>当树达到最大深度时则停止建立决策树，设置一个超参数max_depth，避免树太深导致学习局部样本，从而过拟合；</li>
<li>样本权重和小于设定阈值时则停止建树。什么意思呢，即涉及到一个超参数-最小的样本权重和min_child_weight，和GBM的
min_child_leaf
参数类似，但不完全一样。大意就是一个叶子节点样本太少了，也终止同样是防止过拟合；</li>
</ol>
<h3 id="xgboost与gbdt有什么不同">2. XGBoost与GBDT有什么不同</h3>
<p>除了算法上与传统的GBDT有一些不同外，XGBoost还在工程实现上做了大量的优化。总的来说，两者之间的区别和联系可以总结成以下几个方面。</p>
<ol type="1">
<li>GBDT是机器学习算法，XGBoost是该算法的工程实现。</li>
<li>在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模
型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。</li>
<li>GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代
价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。</li>
<li>传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类
器，比如线性分类器。</li>
<li>传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机
森林相似的策略，支持对数据进行采样。</li>
<li>传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺
失值的处理策略。</li>
</ol>
<h3 id="为什么xgboost要用泰勒展开优势在哪里">3.
为什么XGBoost要用泰勒展开，优势在哪里？</h3>
<p>XGBoost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准.
使用泰勒展开取得函数做自变量的二阶导数形式,
可以在不选定损失函数具体形式的情况下,
仅仅依靠输入数据的值就可以进行叶子分裂优化计算,
本质上也就把损失函数的选取和模型算法优化/参数选择分开了.
这种去耦合增加了XGBoost的适用性, 使得它按需选取损失函数, 可以用于分类,
也可以用于回归。</p>
<h3 id="代码实现-3">4. 代码实现</h3>
<p>GitHub：<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.3%20XGBoost/3.3%20XGBoost.ipynb">点击进入</a></p>
<h2 id="损失函数">损失函数:</h2>
<p>在机器学习中，损失函数（Loss
Function）是用于衡量模型预测结果与真实结果之间差异的函数。</p>
<p>损失函数的主要作用是评估模型在给定数据上的性能表现。它为模型的优化提供了一个明确的目标，通过最小化损失函数的值，来调整模型的参数，使得模型的预测结果越来越接近真实结果。</p>
<p>不同的机器学习任务通常会使用不同的损失函数。</p>
<h3 id="常见的损失函数">常见的损失函数:</h3>
<ol type="1">
<li><p>回归问题：</p>
<ul>
<li>均方误差（Mean Squared
Error，MSE）：计算预测值与真实值之差的平方的平均值，即 <span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n}(y_i -
\hat{y_i})^2\)</span>，其中 <span class="math inline">\(y_i\)</span>
是真实值，<span class="math inline">\(\hat{y_i}\)</span> 是预测值，<span class="math inline">\(n\)</span> 是样本数量。</li>
<li>平均绝对误差（Mean Absolute
Error，MAE）：计算预测值与真实值之差的绝对值的平均值，即 <span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n}|y_i -
\hat{y_i}|\)</span> 。</li>
</ul></li>
<li><p>分类问题：</p>
<ul>
<li>二分类交叉熵损失（Binary Cross Entropy
Loss）：常用于二分类问题，如逻辑回归。对于单个样本，其公式为 <span class="math inline">\(- [y \log(\hat{y}) + (1 - y) \log(1 -
\hat{y})]\)</span>，其中 <span class="math inline">\(y\)</span>
是真实标签（0 或 1），<span class="math inline">\(\hat{y}\)</span>
是预测的概率。</li>
<li>多分类交叉熵损失（Categorical Cross Entropy
Loss）：用于多分类问题，公式为 <span class="math inline">\(-\sum_{i=1}^{C} y_i \log(\hat{y_i})\)</span>，其中
<span class="math inline">\(C\)</span> 是类别数量，<span class="math inline">\(y_i\)</span> 是第 <span class="math inline">\(i\)</span> 类的真实标签（如果样本属于该类为
1，否则为 0），<span class="math inline">\(\hat{y_i}\)</span>
是模型预测样本属于第 <span class="math inline">\(i\)</span>
类的概率。</li>
</ul></li>
</ol>
<p>选择合适的损失函数对于模型的训练和性能至关重要。它会影响模型的学习速度、收敛性以及最终的泛化能力。</p>
<p>以下为您介绍几种常见的损失函数：</p>
<ol type="1">
<li><p><strong>均方误差（Mean Squared Error，MSE）</strong>：</p>
<ul>
<li>公式：<span class="math inline">\(MSE = \frac{1}{n}
\sum_{i=1}^{n}(y_i - \hat{y_i})^2\)</span></li>
<li>其中，<span class="math inline">\(y_i\)</span> 是真实值，<span class="math inline">\(\hat{y_i}\)</span> 是预测值，<span class="math inline">\(n\)</span> 是样本数量。</li>
<li>常用于回归问题，对较大的误差给予更高的惩罚。</li>
<li>例如，预测房价时，如果预测值与真实房价相差较大，MSE 会较大。</li>
</ul></li>
<li><p><strong>平均绝对误差（Mean Absolute Error，MAE）</strong>：</p>
<ul>
<li>公式：<span class="math inline">\(MAE = \frac{1}{n}
\sum_{i=1}^{n}|y_i - \hat{y_i}|\)</span></li>
<li>同样常用于回归问题，相比 MSE 对异常值更鲁棒。</li>
<li>比如，在预测股票价格时，MAE
可能更能承受个别极端价格波动的影响。</li>
</ul></li>
<li><p><strong>交叉熵损失（Cross Entropy Loss）</strong>：</p>
<ul>
<li>二分类交叉熵：<span class="math inline">\(L = - [y \log(\hat{y}) +
(1 - y) \log(1 - \hat{y})]\)</span> ，其中 <span class="math inline">\(y\)</span> 是真实标签（0 或 1），<span class="math inline">\(\hat{y}\)</span> 是预测的概率。</li>
<li>多分类交叉熵：<span class="math inline">\(L = -\sum_{i=1}^{C} y_i
\log(\hat{y_i})\)</span> ，其中 <span class="math inline">\(C\)</span>
是类别数量，<span class="math inline">\(y_i\)</span> 是第 <span class="math inline">\(i\)</span> 类的真实标签（如果样本属于该类为
1，否则为 0），<span class="math inline">\(\hat{y_i}\)</span>
是模型预测样本属于第 <span class="math inline">\(i\)</span>
类的概率。</li>
<li>广泛应用于分类问题，衡量预测概率分布与真实分布之间的差异。</li>
</ul>
<p>交叉熵损失函数（Cross-Entropy
Loss）主要用于衡量模型预测的概率分布与真实的概率分布之间的差异，它在机器学习，特别是深度学习的分类问题中被广泛使用。</p>
<p>其作用主要体现在以下方面：</p>
<ul>
<li><strong>评估模型性能</strong>：通过计算预测分布和真实分布之间的差距，来确定模型在分类任务中的表现优劣。交叉熵越小，表示两个概率分布越接近，说明模型的预测结果越接近真实标签，模型的性能也就越好。</li>
<li><strong>指导模型优化</strong>：在模型训练过程中，交叉熵损失函数的值被用于反向传播，以调整模型的参数，使得损失不断减小，从而使模型的预测逐渐逼近真实标签。</li>
<li><strong>处理多分类问题</strong>：适用于多类别分类任务，可以方便地处理具有多个类别的情况。对于每个样本，模型输出每个类别的预测概率，而交叉熵损失函数会综合考虑所有类别的预测概率和真实标签来计算损失。</li>
</ul>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720089363560.png" alt="1720089363560">
<figcaption aria-hidden="true">1720089363560</figcaption>
</figure>
<p>例如，在一个图像分类任务中，模型需要判断图片属于多个类别中的哪一个。模型的输出是每个类别的预测概率，而真实标签则是图片实际所属的类别。通过计算交叉熵损失，可以衡量模型的预测结果与真实类别之间的差异，并利用这个差异来调整模型的参数，以提高模型的分类准确性。</p>
<p>相比其他一些损失函数，交叉熵损失函数具有一些优点，例如易于理解和计算，对噪声数据具有一定的鲁棒性等。然而，它也存在一些缺点，比如对类别不平衡的数据可能较为敏感，在类别不平衡的数据集上，可能会过于关注多数类别而导致模型性能下降；并且它对输出概率分布的平滑性要求较高，如果输出概率分布过于离散，可能会导致损失值较大。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720089568706.png" alt="1720089568706">
<figcaption aria-hidden="true">1720089568706</figcaption>
</figure></li>
<li><p><strong>Hinge 损失</strong>：</p>
<ul>
<li>常用于支持向量机（SVM）中，特别是在二分类问题中。</li>
<li>对于二分类，公式为：<span class="math inline">\(L = \max(0, 1 - y
\cdot \hat{y})\)</span> ，其中 <span class="math inline">\(y\)</span>
是真实标签（1 或 -1），<span class="math inline">\(\hat{y}\)</span>
是预测值。</li>
</ul></li>
<li><p><strong>KL 散度（Kullback-Leibler Divergence）</strong>：</p>
<ul>
<li>用于衡量两个概率分布之间的差异。</li>
<li>公式：<span class="math inline">\(KL(P || Q) = \sum_{x} P(x) \log
\frac{P(x)}{Q(x)}\)</span></li>
</ul></li>
</ol>
<p>这些损失函数在不同的机器学习任务和场景中各有优缺点，选择合适的损失函数对于模型的性能和训练效果至关重要。</p>
<h3 id="损失函数的导数计算">损失函数的导数计算</h3>
<p>以下是几种常见损失函数及其导数的计算方法：</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720880004976.png" alt="1720880004976">
<figcaption aria-hidden="true">1720880004976</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720880028751.png" alt="1720880028751">
<figcaption aria-hidden="true">1720880028751</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720880055767.png" alt="1720880055767">
<figcaption aria-hidden="true">1720880055767</figcaption>
</figure>
<h2 id="激活函数">激活函数:</h2>
<p>在神经网络中，激活函数是一种对神经元的输入进行非线性变换的函数。</p>
<p>激活函数的主要作用包括：</p>
<ol type="1">
<li>引入非线性：如果没有激活函数，神经网络仅仅是对输入进行线性组合，其表达能力非常有限，无法处理复杂的非线性问题。通过引入非线性的激活函数，可以使神经网络能够拟合各种复杂的函数和模式。</li>
<li>控制神经元的输出范围：不同的激活函数会将输入映射到不同的输出范围，例如
<code>Sigmoid</code> 函数将输出限制在 <code>(0, 1)</code>
之间，<code>Tanh</code> 函数将输出限制在 <code>(-1, 1)</code>
之间。</li>
<li>增加网络的稀疏性：某些激活函数，如 <code>ReLU</code> （Rectified
Linear Unit），当输入为负数时输出为
0，这有助于在网络中引入稀疏性，减少计算量，并可能有助于防止过拟合。</li>
</ol>
<p>常见的激活函数有：</p>
<ol type="1">
<li><p><code>Sigmoid</code>
函数：<code>f(x) = 1 / (1 + e^(-x))</code>，输出范围在
<code>(0, 1)</code> 之间，常用于二分类问题的输出层。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image.png" alt="alt text"></p>
<p>可以看出，sigmoid函数连续，光滑，严格单调，以(0,0.5)中心对称，是一个非常良好的阈值函数。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719936221442.png" alt="1719936221442">
<figcaption aria-hidden="true">1719936221442</figcaption>
</figure></li>
<li><p><code>Tanh</code>
函数：<code>f(x) = (e^x - e^(-x)) / (e^x + e^(-x))</code>，输出范围在
<code>(-1, 1)</code> 之间。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-1.png" alt="alt text"></p></li>
<li><p><code>ReLU</code>
函数：<code>f(x) = max(0, x)</code>，计算简单，在很多深度神经网络中广泛使用。
线性整流函数，又称修正线性单元ReLU，是一种人工神经网络中常用的激活函数，通常指代以斜坡函数及其变种为代表的非线性函数。</p>
<p>线性整流函数（ReLU函数）的特点：</p>
<p>当输入为正时，不存在梯度饱和问题。 计算速度快得多。ReLU
函数中只存在线性关系，因此它的计算速度比Sigmoid函数和tanh函数更快。 Dead
ReLU问题。当输入为负时，ReLU完全失效，在正向传播过程中，这不是问题。有些区域很敏感，有些则不敏感。但是在反向传播过程中，如果输入负数，则梯度将完全为零，Sigmoid函数和tanh函数也具有相同的问题
ReLU函数的输出为0或正数，这意味着ReLU函数不是以0为中心的函数。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-2.png" alt="alt text"></p></li>
<li><p><code>Leaky ReLU</code>
函数：<code>f(x) = max(0.01x, x)</code>，是 <code>ReLU</code>
的改进版，解决了 <code>ReLU</code> 中神经元可能“死亡”的问题。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719937549074.png" alt="1719937549074">
<figcaption aria-hidden="true">1719937549074</figcaption>
</figure></li>
</ol>
<p>Leaky ReLU函数的特点：</p>
<p>Leaky ReLU函数通过把x xx的非常小的线性分量给予负输入0.01 x
0.01x0.01x来调整负值的零梯度问题。 Leaky有助于扩大ReLU函数的范围，通常α
0.01左右。 Leaky ReLU的函数范围是负无穷到正无穷。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-3.png" alt="alt text"></p>
<p>下面是一个使用 <code>Sigmoid</code> 激活函数的简单示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line">x = np.array([-<span class="number">2</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(sigmoid(x))</span><br></pre></td></tr></table></figure>
<p>激活函数的选择对神经网络的性能有很大影响，需要根据具体问题和网络结构进行合适的选择。</p>
<h3 id="常见的激活函数">常见的激活函数：</h3>
<ol type="1">
<li><p><strong>Sigmoid 函数</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = 1 / (1 + e^(-x))</code></li>
<li>特点：将输入值压缩到 0 到 1
之间，具有平滑的曲线。但在输入值较大或较小时，梯度接近
0，可能导致梯度消失问题。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image.png" alt="alt text"></li>
</ul></li>
<li><p><strong>Tanh 函数</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = (e^x - e^(-x)) / (e^x + e^(-x))</code></li>
<li>特点：将输入值压缩到 -1 到 1 之间，相比 Sigmoid 函数，以 0 为中心。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-1.png" alt="alt text"></li>
</ul></li>
<li><p><strong>ReLU 函数（Rectified Linear Unit）</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = max(0, x)</code></li>
<li>特点：计算简单，在正半轴上梯度恒为
1，有效缓解了梯度消失问题。但存在神经元“死亡”的可能，即输入为负时永远不被激活。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-2.png" alt="alt text"></li>
</ul></li>
<li><p><strong>Leaky ReLU 函数</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = max(ax, x)</code> ，其中 <code>a</code>
是一个较小的正数（如 0.01）</li>
<li>特点：对 ReLU 进行改进，解决了神经元“死亡”的问题。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719937549074.png" alt="1719937549074"></li>
</ul></li>
<li><p><strong>ELU 函数（Exponential Linear Unit）</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = x if x &gt; 0 else a(e^x - 1)</code> ，其中
<code>a</code> 是一个常数</li>
<li>特点：具有 ReLU 的优点，同时在输入为负时输出不为
0，使得平均输出更接近 0。</li>
<li><figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719937783555.png" alt="1719937783555">
<figcaption aria-hidden="true">1719937783555</figcaption>
</figure></li>
</ul></li>
<li><p><strong>Softmax 函数</strong>：</p>
<ul>
<li>常用于多分类问题的输出层，将多个神经元的输出值映射为概率分布。</li>
</ul></li>
</ol>
<p>以下是一个使用 Python 绘制部分常见激活函数图像的示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tanh</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.tanh(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">leaky_relu</span>(<span class="params">x, a=<span class="number">0.01</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(a * x, x)</span><br><span class="line"></span><br><span class="line">x = np.linspace(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">plt.plot(x, sigmoid(x), label=<span class="string">&#x27;Sigmoid&#x27;</span>)</span><br><span class="line">plt.plot(x, tanh(x), label=<span class="string">&#x27;Tanh&#x27;</span>)</span><br><span class="line">plt.plot(x, relu(x), label=<span class="string">&#x27;ReLU&#x27;</span>)</span><br><span class="line">plt.plot(x, leaky_relu(x), label=<span class="string">&#x27;Leaky ReLU&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Input&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Output&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Common Activation Functions&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>不同的激活函数在不同的场景和网络结构中表现各异，需要根据具体问题进行选择和调整。</p>
<h2 id="学习率">学习率</h2>
<p>学习率（Learning Rate）在机器学习中是一个非常关键的超参数。</p>
<p>学习率决定了模型在训练过程中参数更新的步长大小。</p>
<p>较小的学习率意味着模型在参数更新时采取较小的步幅，可能会使训练过程更加稳定，但收敛速度较慢，需要更多的训练迭代次数才能达到较好的效果。</p>
<p>例如，如果学习率过小，如 0.0001
，模型可能会在训练中进展缓慢，需要大量的训练轮数才能逐渐优化参数。</p>
<p>较大的学习率会使模型在参数更新时采取较大的步幅，可能会加快收敛速度，但也可能导致模型跳过最优解，甚至无法收敛。</p>
<p>比如，学习率过大，如 10
，模型可能会在训练中出现剧烈的波动，无法稳定地优化参数。</p>
<p>选择合适的学习率通常需要通过试验和错误来确定。常见的方法包括使用固定的学习率、学习率衰减（随着训练的进行逐渐减小学习率）、自适应学习率算法（如
Adam 优化器中的自适应学习率调整）等。</p>
<p>例如，在深度学习中，训练开始时可以使用较大的学习率，如 0.1
，然后随着训练的进行，逐渐将学习率减小到 0.001
，以实现更精细的参数调整和更好的收敛效果。</p>
<h3 id="以下是一些常见的学习率调整方法">以下是一些常见的学习率调整方法：</h3>
<ol type="1">
<li><p><strong>固定学习率</strong>：在整个训练过程中保持学习率不变。这种方法简单，但可能不是最优的，因为在训练的不同阶段可能需要不同的学习率。</p></li>
<li><p><strong>分段常数学习率</strong>：将训练过程分为几个阶段，每个阶段使用不同的固定学习率。例如，在前几个
epoch 使用较大的学习率，然后在后续阶段使用较小的学习率。</p></li>
<li><p><strong>学习率衰减</strong>：</p>
<ul>
<li><strong>按步长衰减</strong>：每隔一定的训练步数或
epoch，将学习率乘以一个小于 1 的衰减因子。</li>
<li><strong>指数衰减</strong>：学习率按照指数形式衰减，例如
<code>learning_rate = initial_learning_rate * decay_rate ^ (epoch / decay_steps)</code>
。</li>
<li><strong>多项式衰减</strong>：学习率按照多项式的形式逐渐减小。</li>
</ul></li>
<li><p><strong>自适应学习率算法</strong>：</p>
<ul>
<li><strong>Adagrad</strong>：根据每个参数之前的梯度历史来调整学习率，对于不常更新的参数给予较大的学习率，对于频繁更新的参数给予较小的学习率。</li>
<li><strong>Adadelta</strong>：是对 Adagrad
的改进，避免了学习率单调递减的问题。</li>
<li><strong>RMSProp</strong>：类似于
Adadelta，对梯度的二阶矩进行指数加权平均来调整学习率。</li>
<li><strong>Adam</strong>：结合了动量和 RMSProp
的优点，能够自适应地调整学习率。</li>
</ul></li>
<li><p><strong>余弦退火</strong>：学习率按照余弦函数的形式进行周期性的变化，在每个周期内从较高的值逐渐降低到较低的值，然后再上升。</p></li>
<li><p><strong>基于验证集性能调整</strong>：根据模型在验证集上的性能来动态调整学习率。如果验证集性能在一段时间内没有改善，就降低学习率。</p></li>
</ol>
<p>这些方法各有优缺点，具体选择哪种方法取决于数据集、模型架构和训练需求等因素。通常需要通过实验来找到最适合特定问题的学习率调整策略。</p>
<h2 id="optimizer的概念">optimizer的概念</h2>
<p>在机器学习中，<code>optimizer</code>（优化器）是用于调整模型参数以最小化损失函数或最大化目标函数的算法或策略。</p>
<p>优化器的主要作用是根据模型的当前状态（包括参数值和计算得到的梯度）来决定如何更新模型的参数，以使得模型在训练数据上的性能逐渐提高。</p>
<p>常见的优化器有随机梯度下降（Stochastic Gradient
Descent，SGD）、Adagrad、Adadelta、RMSprop、Adam 等。</p>
<p>以随机梯度下降（SGD）为例，它的基本思想是沿着梯度的反方向，以一定的学习率来更新参数。假设参数为
<code>w</code> ，梯度为 <code>g</code> ，学习率为 <code>lr</code>
，则更新公式为 <code>w = w - lr * g</code> 。</p>
<p>Adagrad
则根据每个参数之前的梯度历史来自适应地调整学习率。对于频繁更新的参数，学习率会逐渐减小，而对于很少更新的参数，学习率会相对较大。</p>
<p>RMSprop 类似于
Adagrad，但它不是累积所有的梯度平方，而是使用指数加权平均来计算梯度平方的估计值。</p>
<p>Adam 结合了动量和 RMSprop
的优点，同时考虑了梯度的一阶矩和二阶矩来动态调整学习率。</p>
<p>选择合适的优化器对于模型的训练效率和最终性能至关重要。例如，在数据量较大且模型较复杂时，Adam
通常能取得较好的效果；而在一些简单的模型和小数据集上，SGD
可能表现不错，并且通过适当的调整学习率等超参数，也能获得较好的结果。</p>
<p>再比如，在处理具有稀疏特征的问题时，Adagrad
可能更合适。总之，优化器的选择需要根据具体的问题和数据特点来决定，并可能需要通过实验来找到最优的选择。</p>
<h2 id="pytorch代码分析">pytorch代码分析:</h2>
<p>如何理解下面的代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">		<span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">		self.layer = nn.Linear(n_feature, n_hidden)</span><br><span class="line">		self.all_act = nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">		x = self.layer(x)</span><br><span class="line">		x = torch.tanh(x)</span><br><span class="line">		x = self.all_act(x)</span><br><span class="line">		<span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这段代码是一个简单的神经网络模型的定义，使用 PyTorch
框架来构建。让我解释一下这段代码的含义：</p>
<ol type="1">
<li><strong>class Net(nn.Module):</strong>：定义了一个名为
<strong>Net</strong> 的类，继承自 <strong>nn.Module</strong>
类，表示这个类是一个神经网络模型。</li>
</ol>
<p><strong>2. </strong>def <strong>init</strong>(self, n_feature,
n_hidden, n_output):<strong>：定义了 </strong>Net** 类的构造函数
<strong><strong>init</strong></strong>，该函数接受三个参数
<strong>n_feature</strong>、<strong>n_hidden</strong> 和
<strong>n_output</strong>，分别表示输入特征的数量、隐藏层神经元的数量和输出的数量。**</p>
<p><strong>3. </strong>super(Net, self).__init__()<strong>：调用父类
</strong>nn.Module** 的构造函数，初始化神经网络模型。**</p>
<p><strong>4. </strong>self.layer = nn.Linear(n_feature,
n_hidden)<strong>：定义了一个线性层
</strong>layer<strong>，输入特征数量为
</strong>n_feature<strong>，输出特征数量为
</strong>n_hidden<strong>。</strong></p>
<p><strong>5. </strong>self.all_act = nn.Linear(n_hidden,
n_output)<strong>：定义了另一个线性层
</strong>all_act<strong>，输入特征数量为
</strong>n_hidden<strong>，输出特征数量为
</strong>n_output<strong>。</strong></p>
<ol start="6" type="1">
<li><strong>def forward(self, x):</strong>：定义了前向传播函数
<strong>forward</strong>，接受输入
<strong>x</strong>，表示对输入数据进行前向传播计算。</li>
<li><strong>x = self.layer(x)</strong>：将输入 <strong>x</strong> 经过
<strong>layer</strong> 线性层的计算，得到隐藏层的输出。</li>
<li><strong>x =
torch.tanh(x)</strong>：对隐藏层的输出应用双曲正切函数（tanh）作为激活函数，增加模型的非线性能力。</li>
<li><strong>x =
self.all_act(x)</strong>：将经过激活函数后的隐藏层输出再经过
<strong>all_act</strong> 线性层的计算，得到最终的输出。</li>
<li><strong>return x</strong>：返回神经网络模型的输出。</li>
</ol>
<p>这段代码定义了一个简单的神经网络模型，包括一个输入层到隐藏层的线性变换和隐藏层到输出层的线性变换，中间使用了双曲正切函数作为激活函数。在神经网络的训练过程中，可以通过调用
<strong>forward</strong> 函数来进行前向传播计算。</p>
<h3 id="tf.one_hot">tf.one_hot</h3>
<p><code>tf.one_hot</code> 是 TensorFlow
中的一个函数，用于将输入的索引值转换为独热编码（One-Hot
Encoding）的张量。</p>
<p>独热编码是一种将类别变量转换为二进制向量的编码方式，其中只有一个元素为
1 ，其余元素为 0 。</p>
<p>以下是 <code>tf.one_hot</code> 函数的一般用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">indices = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">depth = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">one_hot_encoded = tf.one_hot(indices, depth)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(one_hot_encoded.numpy())</span><br></pre></td></tr></table></figure>
<p>在上述示例中，<code>indices</code>
是要编码的索引值列表，<code>depth</code>
表示编码的维度（即类别数量）。</p>
<p>例如，对于索引值 <code>0</code> ，在维度为 4
的编码中，得到的独热编码为 <code>[1, 0, 0, 0]</code> ；对于索引值
<code>2</code> ，得到的编码为 <code>[0, 0, 1, 0]</code> 。</p>
<p><code>tf.one_hot</code>
常用于将分类标签转换为适合神经网络输入的形式，方便模型进行处理和计算。</p>
<p>上述代码中，<code>indices = [0, 2, 1, 3]</code> 且
<code>depth = 4</code> ，使用 <code>tf.one_hot</code>
函数进行独热编码后的输出结果应该是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[1 0 0 0]</span><br><span class="line"> [0 0 1 0]</span><br><span class="line"> [0 1 0 0]</span><br><span class="line"> [0 0 0 1]]</span><br></pre></td></tr></table></figure>
<p>即索引为 0 的位置编码为 <code>[1, 0, 0, 0]</code> ，索引为 2
的位置编码为 <code>[0, 0, 1, 0]</code> ，索引为 1 的位置编码为
<code>[0, 1, 0, 0]</code> ，索引为 3 的位置编码为
<code>[0, 0, 0, 1]</code> 。</p>
<h3 id="self.critic_net.parameters">self.critic_net.parameters()</h3>
<p><code>self.critic_net.parameters()</code> 通常是在 Python
的深度学习框架（如 PyTorch）中使用的代码。</p>
<p><code>critic_net</code>
可能是您定义的一个神经网络模型（例如，用于评估或批评某些数据的模型）。</p>
<p><code>parameters()</code>
方法返回模型中的可学习参数，这些参数通常是权重（weights）和偏置（biases）。</p>
<p>例如，如果 <code>critic_net</code>
是一个简单的全连接神经网络，那么通过
<code>self.critic_net.parameters()</code>
您将获取到该网络中所有层的权重和偏置的迭代器。</p>
<p>您可以使用这个返回值来进行一些操作，比如：</p>
<ol type="1">
<li><p>在优化器中使用它来进行参数的更新，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(self.critic_net.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>对参数进行一些统计或分析，比如计算参数的数量、查看参数的值等。</p></li>
</ol>
<p>总之，<code>self.critic_net.parameters()</code>
为您提供了对模型可学习参数的访问，以便进行各种与模型训练和优化相关的操作。</p>
<h3 id="self.optimizer.zero_grad">self.optimizer.zero_grad()</h3>
<p><code>self.optimizer.zero_grad()</code>
通常用于将模型参数的梯度清零。</p>
<p>当进行反向传播计算梯度后，如果不将梯度清零，那么在后续的迭代中，新计算的梯度会与之前的梯度累加。这会导致梯度计算的错误，影响模型的训练效果</p>
<h3 id="loss.backwardretain_graphtrue">loss.backward(retain_graph=True)</h3>
<p>在深度学习中，<code>loss.backward(retain_graph=True)</code>
用于计算损失函数 <code>loss</code> 关于模型参数的梯度，同时通过设置
<code>retain_graph=True</code> 来保留计算图。</p>
<p>通常，在进行一次反向传播计算梯度后，计算图会被释放以节省内存。但当您需要多次对同一计算图执行反向传播时，就需要设置
<code>retain_graph=True</code> 。</p>
<p>例如，如果您在一个循环中多次计算梯度并更新参数，像下面这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">    <span class="comment"># 前向传播，计算损失</span></span><br><span class="line">    loss = some_operation()</span><br><span class="line">    <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">    loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 根据梯度更新参数</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>在上述示例中，由于在循环内多次执行反向传播，所以需要保留计算图，以便下一次迭代能够再次基于相同的计算图计算梯度。</p>
<p>如果不设置 <code>retain_graph=True</code> ，在第二次执行
<code>loss.backward()</code> 时会报错，因为计算图已经被释放。</p>
<p>需要注意的是，一直保留计算图会占用较多内存，所以在不需要多次使用计算图时，应避免设置
<code>retain_graph=True</code> ，以节省内存资源。</p>
<p>在 PyTorch 中，<code>retain_graph=True</code>
的作用是在执行反向传播（<code>backward</code>）后保留计算图。</p>
<p>PyTorch
采用动态图机制，默认情况下，每次反向传播之后都会释放计算图以节省内存。然而，当需要对同一个计算图进行多次反向传播时，就需要设置
<code>retain_graph=True</code>。</p>
<p>例如，假设有一个输入 <code>x</code>，经过一些计算得到
<code>y</code>，然后有两个输出 <code>output_1</code> 和
<code>output_2</code>，都需要对其执行反向传播。如果在第一次对
<code>output_1</code> 执行 <code>backward</code> 时没有设置
<code>retain_graph=True</code>，那么在执行完后计算图就会被释放。当再对
<code>output_2</code> 执行 <code>backward</code>
时，由于计算图已经不存在，就会引发错误。</p>
<p>通过设置 <code>retain_graph=True</code>，可以保留中间参数，使得多次
<code>backward</code> 之间不会相互影响。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss1.backward(retain_graph=<span class="literal">True</span>)  <span class="comment"># 第一次 backward，保留计算图</span></span><br><span class="line">loss2.backward()  <span class="comment"># 第二次 backward，此时计算图仍然存在</span></span><br></pre></td></tr></table></figure>
<p>需要注意的是，一直保留计算图会占用较多内存，所以在不需要多次使用计算图时，应避免设置
<code>retain_graph=True</code>，以节省内存资源。</p>
<p>另外，<code>create_graph</code>
参数用于创建导数计算图，允许进行高阶导数的计算；而
<code>grad_tensors</code>
可用于设置多梯度的权重，当有多个损失函数需要计算梯度时，可以设置各个损失之间权重的比例。</p>
<h4 id="如何判断是否需要多次使用计算图">如何判断是否需要多次使用计算图？</h4>
<p>判断是否需要多次使用计算图，主要考虑以下几个方面：</p>
<ol type="1">
<li>模型的结构和需求：如果你的模型在一次前向传播后，需要对不同的输出或中间结果进行多次反向传播来计算梯度，以实现不同的目的，例如优化不同的部分或计算多个相关的梯度，那么就可能需要多次使用计算图。</li>
<li>是否涉及多个相关的损失函数：当存在多个损失函数，且需要分别或同时根据这些损失函数计算梯度并更新参数时，通常需要多次使用计算图。</li>
<li>循环或迭代的操作：在某些情况下，可能会在一个循环或迭代中多次执行类似的计算和反向传播操作。例如，在强化学习中，可能需要在每个时间步都进行前向传播和反向传播。</li>
<li>中间结果的重用：如果在计算过程中需要多次访问或使用某些中间结果的梯度，那么就需要保留计算图以进行多次反向传播。</li>
</ol>
<p>例如，如果你有一个神经网络模型，其中包含多个子模块，每个子模块都有自己的损失，并且你希望分别根据这些子模块的损失来更新它们的参数，那么就需要多次使用计算图，对每个子模块的损失进行单独的反向传播。</p>
<p>又或者在训练过程中，你可能想要尝试不同的优化策略或超参数，需要在同一次前向传播后，多次计算梯度并更新参数，来比较不同策略或参数的效果，这也需要多次使用计算图。</p>
<p>另外，一些复杂的模型结构或自定义的计算流程可能会导致需要多次使用计算图的情况。但需注意，一直保留计算图会占用较多内存，所以在不需要多次使用计算图时，应避免设置
<code>retain_graph=True</code> ，以节省内存资源。</p>
<p>如果你不确定是否需要多次使用计算图，可以先尝试在不保留计算图的情况下（即不设置
<code>retain_graph=True</code> ）运行代码，观察是否会出现报错“trying to
backward through the graph a second time, but the buffers have already
been freed”。如果出现该报错，且确实需要进行多次反向传播，那么就需要设置
<code>retain_graph=True</code>
。同时，为了避免内存过度占用，在完成所有需要的反向传播后，应及时释放不再使用的计算图和相关数据。</p>
<h3 id="tf.reduce_mean">tf.reduce_mean</h3>
<p><code>tf.reduce_mean</code> 是 TensorFlow
中的一个函数，用于计算张量在指定维度上的平均值。</p>
<p>以下是一个示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个张量</span></span><br><span class="line">tensor = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算所有元素的平均值</span></span><br><span class="line">average = tf.reduce_mean(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每一行的平均值</span></span><br><span class="line">row_averages = tf.reduce_mean(tensor, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每一列的平均值</span></span><br><span class="line">column_averages = tf.reduce_mean(tensor, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;所有元素的平均值：&quot;</span>, sess.run(average))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;每一行的平均值：&quot;</span>, sess.run(row_averages))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;每一列的平均值：&quot;</span>, sess.run(column_averages))</span><br></pre></td></tr></table></figure>
<p>在上述示例中：</p>
<ul>
<li>当不指定 <code>axis</code>
参数时，<code>tf.reduce_mean(tensor)</code> 计算张量 <code>tensor</code>
中所有元素的平均值。</li>
<li><code>tf.reduce_mean(tensor, axis=1)</code>
计算每一行的平均值，得到一个长度为行数的张量。</li>
<li><code>tf.reduce_mean(tensor, axis=0)</code>
计算每一列的平均值，得到一个长度为列数的张量。</li>
</ul>
<p>例如，对于上述示例中的张量 <code>[[1, 2, 3], [4, 5, 6]]</code> ：</p>
<ul>
<li>所有元素的平均值为 <code>(1 + 2 + 3 + 4 + 5 + 6) / 6 = 3.5</code>
。</li>
<li>每一行的平均值分别为 <code>(1 + 2 + 3) / 3 = 2</code> 和
<code>(4 + 5 + 6) / 3 = 5</code> 。</li>
<li>每一列的平均值分别为 <code>(1 + 4) / 2 = 2.5</code> 、
<code>(2 + 5) / 2 = 3.5</code> 和 <code>(3 + 6) / 2 = 4.5</code> 。</li>
</ul>
<h3 id="tf.placeholder">tf.placeholder</h3>
<p>用于表示强化学习或神经网络中的状态输入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with tf.name_scope(&#x27;S&#x27;):#状态</span><br><span class="line">    S = tf.placeholder(tf.float32, shape=[None, state_dim], name=&#x27;s&#x27;)</span><br></pre></td></tr></table></figure>
<ol type="1">
<li>tf.name_scope('S')</li>
</ol>
<p>这行代码使用 TensorFlow 的 tf.name_scope 创建一个命名空间
S。命名空间在 TensorFlow
中用于组织图中的节点，使其更具可读性和结构化。在 TensorBoard
中查看图时，可以更清晰地看到节点的组织结构。</p>
<ol start="2" type="1">
<li>tf.placeholder(tf.float32, shape=[None, state_dim], name='s')</li>
</ol>
<p>这行代码定义了一个占位符 S。占位符在 TensorFlow
中用于在执行图时提供输入数据。这里的占位符有以下几个参数：</p>
<p>tf.float32：占位符的数据类型是 32 位浮点数。</p>
<p><strong>shape=[None, state_dim]：占位符的形状。shape
参数定义了输入数据的维度</strong>：</p>
<p>None
表示这个维度可以是任意长度，通常用于<strong>批次（batch）的大小</strong>。使用
None 是因为在实际运行时，批次的大小可能会有所不同。</p>
<p>state_dim
表示状态的维度。这是一个整数，定义了每个状态的特征数量。在强化学习中，状态通常是一个向量，其长度由具体的环境或问题决定。</p>
<p>name='s'：给占位符命名为
's'。这个名字在构建和调试图时有助于识别。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720887133485.png" alt="1720887133485">
<figcaption aria-hidden="true">1720887133485</figcaption>
</figure>
<h3 id="tf.variable_scope">tf.variable_scope</h3>
<p><code>tf.variable_scope</code> 是 TensorFlow
中用于管理变量作用域的一个重要工具。它帮助组织和复用变量，特别是在构建复杂的神经网络时</p>
<h4 id="什么是-tf.variable_scope">1. 什么是
<code>tf.variable_scope</code>？</h4>
<p><code>tf.variable_scope</code>
提供了一种机制来创建和管理变量范围（scope），这对变量进行命名和复用非常有帮助。通过使用变量范围，可以确保变量命名的一致性和避免命名冲突。</p>
<h4 id="为什么使用-tf.variable_scope">2. 为什么使用
<code>tf.variable_scope</code>？</h4>
<ul>
<li><strong>组织变量</strong>：可以将相关的变量组织在一起，使代码更具可读性和结构性。</li>
<li><strong>复用变量</strong>：在共享参数的模型（如共享权重的神经网络层）中，复用变量非常重要。</li>
<li><strong>命名管理</strong>：自动处理变量命名，避免命名冲突。</li>
</ul>
<h4 id="如何使用-tf.variable_scope">3. 如何使用
<code>tf.variable_scope</code>？</h4>
<p>使用 <code>tf.variable_scope</code>
创建一个新的变量作用域，可以在该作用域内定义和复用变量。</p>
<h5 id="示例-1创建变量作用域">示例 1：创建变量作用域</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope1&#x27;</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope2&#x27;</span>):</span><br><span class="line">    var2 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">2.0</span>))</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>var1</code> 和 <code>var2</code>
是在不同的变量作用域中创建的，尽管它们的名字相同，但在图中它们是不同的变量，分别命名为
<code>scope1/var</code> 和 <code>scope2/var</code>。</p>
<h5 id="示例-2复用变量">示例 2：复用变量</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope&#x27;</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope&#x27;</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">    var2 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>var2</code> 复用了 <code>var1</code>
的变量。通过设置
<code>reuse=True</code>，确保在相同的作用域内复用已经存在的变量。</p>
<h5 id="tf.get_variable-和-tf.variable-的区别">4.
<code>tf.get_variable</code> 和 <code>tf.Variable</code> 的区别</h5>
<ul>
<li><strong><code>tf.get_variable</code></strong>：用于创建或获取变量，通常与
<code>tf.variable_scope</code> 一起使用。它支持变量复用机制。</li>
<li><strong><code>tf.Variable</code></strong>：直接创建新变量，不支持复用机制。</li>
</ul>
<h5 id="示例-3使用-tf.get_variable-和-tf.variable">示例 3：使用
<code>tf.get_variable</code> 和 <code>tf.Variable</code></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope1&#x27;</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line">    var2 = tf.Variable([<span class="number">1.0</span>], name=<span class="string">&#x27;var2&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>var1</code> 是通过 <code>tf.get_variable</code>
创建的，支持复用机制，而 <code>var2</code> 是通过
<code>tf.Variable</code> 创建的，不支持复用。</p>
<h4 id="总结">5. 总结</h4>
<p><code>tf.variable_scope</code> 是 TensorFlow
中用于管理变量作用域的工具，帮助组织和复用变量，提供了一种结构化和高效的变量管理方式。通过合理使用变量作用域，可以避免命名冲突，实现变量复用，特别是在构建共享参数的复杂神经网络时非常有用。</p>
<h3 id="tf.session">tf.Session</h3>
<p><code>tf.Session</code> 是 TensorFlow 1.x
中一个重要的类，用于执行定义好的计算图（computation graph）。在
TensorFlow 2.x 中，<code>tf.Session</code> 被逐渐弃用，更多地采用了
Eager Execution 模式，但了解 <code>tf.Session</code>
对于维护和理解旧代码还是非常重要的。以下是对 <code>tf.Session</code>
的详细解释及其使用方法。</p>
<h4 id="什么是-tf.session">1. 什么是 <code>tf.Session</code>？</h4>
<p><code>tf.Session</code> 提供了一个运行 TensorFlow 操作的环境。它管理
TensorFlow 运行时的所有资源，包括变量的内存分配、执行设备的选择等。</p>
<h4 id="为什么需要-tf.session">2. 为什么需要
<code>tf.Session</code>？</h4>
<p>在 TensorFlow 1.x
中，计算图的定义和执行是分开的。你首先定义一个计算图，然后在
<code>tf.Session</code> 中执行它。<code>tf.Session</code>
提供了以下功能：</p>
<ul>
<li><strong>控制和管理资源</strong>：例如内存、线程等。</li>
<li><strong>分配计算设备</strong>：决定在 CPU 还是 GPU 上执行操作。</li>
<li><strong>执行计算图</strong>：运行图中的操作，获取结果。</li>
</ul>
<h4 id="如何使用-tf.session">3. 如何使用 <code>tf.Session</code>？</h4>
<h5 id="示例-1基本使用">示例 1：基本使用</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义计算图</span></span><br><span class="line">a = tf.constant(<span class="number">2</span>)</span><br><span class="line">b = tf.constant(<span class="number">3</span>)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话并运行计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result = sess.run(c)</span><br><span class="line">    <span class="built_in">print</span>(result)  <span class="comment"># 输出 5</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，首先定义了一个简单的计算图，然后创建一个
<code>tf.Session</code> 来运行这个图，并获取结果。</p>
<h5 id="示例-2使用-tf.placeholder-和-feed_dict">示例 2：使用
<code>tf.placeholder</code> 和 <code>feed_dict</code></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义占位符和操作</span></span><br><span class="line">a = tf.placeholder(tf.int32)</span><br><span class="line">b = tf.placeholder(tf.int32)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话并运行计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result = sess.run(c, feed_dict=&#123;a: <span class="number">2</span>, b: <span class="number">3</span>&#125;)</span><br><span class="line">    <span class="built_in">print</span>(result)  <span class="comment"># 输出 5</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，使用 <code>tf.placeholder</code>
定义了占位符，可以在运行时通过 <code>feed_dict</code> 提供输入数据。</p>
<h5 id="示例-3管理变量">示例 3：管理变量</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义变量和初始化操作</span></span><br><span class="line">var = tf.Variable([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话并初始化变量</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    result = sess.run(var)</span><br><span class="line">    <span class="built_in">print</span>(result)  <span class="comment"># 输出 [1. 2.]</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，定义了一个变量，并在会话中运行初始化操作来分配和初始化变量。</p>
<h4 id="tensorflow-2.x-中的变化">4. TensorFlow 2.x 中的变化</h4>
<p>在 TensorFlow 2.x 中，引入了 Eager
Execution，默认情况下无需显式创建会话，可以直接执行操作。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorFlow 2.x 默认启用 Eager Execution</span></span><br><span class="line">a = tf.constant(<span class="number">2</span>)</span><br><span class="line">b = tf.constant(<span class="number">3</span>)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c)  <span class="comment"># 直接输出 &lt;tf.Tensor: shape=(), dtype=int32, numpy=5&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="总结-1">5. 总结</h4>
<p><code>tf.Session</code> 是 TensorFlow 1.x
中用于执行计算图的核心工具，帮助管理和控制资源，执行计算操作。在
TensorFlow 2.x 中，虽然 <code>tf.Session</code>
被弃用，但理解它的工作机制对于维护旧代码和理解 TensorFlow
的运行原理仍然非常重要。</p>
<h3 id="tf.random_normal_initializer">tf.random_normal_initializer</h3>
<p>这是 TensorFlow
提供的一个初始化器，用于从正态分布（高斯分布）中随机抽取样本来初始化变量。</p>
<h4 id="背景知识">背景知识</h4>
<p>在神经网络中，权重和偏置的初始化对模型的收敛速度和效果有很大影响。TensorFlow
提供了多种初始化器来帮助我们初始化这些变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init_w = tf.random_normal_initializer(<span class="number">0.</span>, <span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>
<p><strong>参数解释</strong> ：</p>
<ul>
<li>第一个参数
<code>0.</code>：表示正态分布的均值（mean）。在这里，均值被设置为
<code>0</code>。</li>
<li>第二个参数 <code>0.3</code>：表示正态分布的标准差（standard
deviation）。在这里，标准差被设置为 <code>0.3</code>。
因此，<code>tf.random_normal_initializer(0., 0.3)</code> 表示使用均值为
<code>0</code>、标准差为 <code>0.3</code> 的正态分布来初始化变量。</li>
</ul>
<h4 id="实例说明">实例说明</h4>
<p>以下是一个简单的示例，展示如何使用这个初始化器来初始化一个 TensorFlow
变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个变量并使用 tf.random_normal_initializer 初始化</span></span><br><span class="line">init_w = tf.random_normal_initializer(<span class="number">0.</span>, <span class="number">0.3</span>)</span><br><span class="line">var = tf.Variable(initial_value=init_w(shape=[<span class="number">2</span>, <span class="number">3</span>], dtype=tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化全局变量</span></span><br><span class="line">tf.compat.v1.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建并运行一个会话以查看变量的值</span></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.compat.v1.global_variables_initializer())</span><br><span class="line">    <span class="built_in">print</span>(sess.run(var))</span><br></pre></td></tr></table></figure>
<h4 id="解释这个示例">解释这个示例</h4>
<ol type="1">
<li><strong>定义初始化器</strong>
：<code>init_w = tf.random_normal_initializer(0., 0.3)</code>
定义了一个初始化器，它会生成均值为 <code>0</code>、标准差为
<code>0.3</code> 的随机数。</li>
<li><strong>初始化变量</strong>
：<code>var = tf.Variable(initial_value=init_w(shape=[2, 3], dtype=tf.float32))</code>
使用这个初始化器来初始化一个形状为 <code>[2, 3]</code> 的变量。</li>
<li><strong>初始化全局变量</strong>
：<code>tf.compat.v1.global_variables_initializer()</code>
用于初始化所有的 TensorFlow 变量。</li>
<li><strong>运行会话</strong> ：创建并运行一个 TensorFlow
会话以查看变量的值。</li>
</ol>
<h3 id="tf.compat.v1.global_variables_initializer">tf.compat.v1.global_variables_initializer</h3>
<p><code>tf.compat.v1.global_variables_initializer</code> 是 TensorFlow
1.x 中的一个函数，用于初始化所有的全局变量。在 TensorFlow 2.x
中，这个函数被包括在 <code>compat.v1</code> 模块中，以便兼容旧代码。</p>
<h4 id="作用">作用</h4>
<p><code>tf.compat.v1.global_variables_initializer()</code>
函数会创建一个操作（operation），这个操作会初始化 TensorFlow
图中的所有全局变量。执行这个操作能够将所有变量赋值为它们的初始值。</p>
<h4 id="变量初始化的重要性">变量初始化的重要性</h4>
<p>在使用 TensorFlow
构建和训练模型时，变量（如权重和偏置）需要先被初始化，然后才能在计算图中使用。未初始化的变量在使用时会导致错误。</p>
<h3 id="使用示例">使用示例</h3>
<p>下面是一个简单的示例，展示了如何使用
<code>tf.compat.v1.global_variables_initializer()</code>
来初始化变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建变量</span></span><br><span class="line">var1 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">&quot;var1&quot;</span>)</span><br><span class="line">var2 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">&quot;var2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建初始化操作</span></span><br><span class="line">init_op = tf.compat.v1.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建并运行会话以初始化变量</span></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init_op)  <span class="comment"># 初始化所有变量</span></span><br><span class="line">    <span class="built_in">print</span>(sess.run(var1))</span><br><span class="line">    <span class="built_in">print</span>(sess.run(var2))</span><br></pre></td></tr></table></figure>
<h3 id="解释这个示例-1">解释这个示例</h3>
<ol type="1">
<li><p><strong>创建变量</strong>：我们创建了两个变量 <code>var1</code>
和 <code>var2</code>，它们的初始值是从正态分布中随机抽取的。</p></li>
<li><p><strong>创建初始化操作</strong>：使用
<code>tf.compat.v1.global_variables_initializer()</code>
创建一个初始化操作 <code>init_op</code>。</p></li>
<li><p><strong>运行会话</strong>：</p>
<ul>
<li>使用 <code>with tf.compat.v1.Session() as sess:</code> 创建一个
TensorFlow 会话。</li>
<li>使用 <code>sess.run(init_op)</code>
来运行初始化操作，初始化所有变量。</li>
<li>之后，通过 <code>sess.run(var1)</code> 和
<code>sess.run(var2)</code> 来查看变量的值。</li>
</ul></li>
</ol>
<h3 id="tensorflow-2.x-的变化">TensorFlow 2.x 的变化</h3>
<p>在 TensorFlow 2.x 中，变量的初始化通常由更高级别的 API（如
Keras）自动处理。直接使用
<code>tf.compat.v1.global_variables_initializer()</code>
的情况较少见，除非是在兼容旧代码或使用低级别 API 时。</p>
<p>以下是 TensorFlow 2.x 的一个简单示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建变量</span></span><br><span class="line">var1 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">&quot;var1&quot;</span>)</span><br><span class="line">var2 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">&quot;var2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化所有变量</span></span><br><span class="line">tf.compat.v1.global_variables_initializer().run(session=tf.compat.v1.Session())</span><br></pre></td></tr></table></figure>
<h3 id="总结-2">总结</h3>
<p><code>tf.compat.v1.global_variables_initializer()</code>
是用于初始化所有全局变量的函数。在 TensorFlow 2.x 中，通常通过更高级别的
API 处理变量的初始化，但在需要兼容 TensorFlow 1.x
代码时，<code>tf.compat.v1.global_variables_initializer()</code>
仍然非常有用。通过初始化变量，确保在计算图中使用这些变量时不会遇到未初始化变量的错误。</p>
<h3 id="tf.constant_initializer">tf.constant_initializer</h3>
<p><code>tf.constant_initializer</code> 是 TensorFlow
中的一个初始化器，用于创建一个常量初始化器对象，该对象用于将变量初始化为指定的常量值。</p>
<h4 id="使用方法">使用方法</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">initializer = tf.constant_initializer(value)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>value</code> 是要初始化的常量值。</li>
</ul>
<h4 id="示例">示例</h4>
<p>以下是一个简单的示例，展示如何使用
<code>tf.constant_initializer</code> 来初始化一个 TensorFlow
变量为常量值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个常量初始化器，将变量初始化为常量值 42.0</span></span><br><span class="line">initializer = tf.constant_initializer(<span class="number">42.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个变量，并使用常量初始化器初始化</span></span><br><span class="line">var = tf.Variable(initializer(shape=[<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">&quot;constant_var&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化所有变量</span></span><br><span class="line">tf.compat.v1.global_variables_initializer().run(session=tf.compat.v1.Session())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印变量的值</span></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(sess.run(var))</span><br></pre></td></tr></table></figure>
<h4 id="解释这个示例-2">解释这个示例</h4>
<ol type="1">
<li><p><strong>创建初始化器</strong>：使用
<code>tf.constant_initializer(42.0)</code>
创建一个常量初始化器，它会将变量初始化为常量值
<code>42.0</code>。</p></li>
<li><p><strong>初始化变量</strong>：使用
<code>initializer(shape=[2, 3])</code> 创建一个形状为
<code>[2, 3]</code> 的变量，并使用常量初始化器初始化它。</p></li>
<li><p><strong>运行会话</strong>：</p>
<ul>
<li>使用 <code>tf.compat.v1.global_variables_initializer()</code>
初始化所有变量。</li>
<li>使用 <code>sess.run(var)</code>
在会话中运行并打印变量的值，输出将会是一个形状为 <code>[2, 3]</code>
的数组，其所有元素都是 <code>42.0</code>。</li>
</ul></li>
</ol>
<h4 id="应用场景">应用场景</h4>
<ul>
<li><strong>固定初始化值</strong>：当你希望将所有权重或偏置初始化为固定的常量值时，可以使用
<code>tf.constant_initializer</code>。</li>
<li><strong>保证初始化的一致性</strong>：有时在调试和开发过程中，需要确保变量被初始化为固定的常量，以便进行稳定性检查和测试。</li>
</ul>
<h4 id="tensorflow-2.x-的变化-1">TensorFlow 2.x 的变化</h4>
<p>在 TensorFlow 2.x 中，变量的初始化通常由更高级别的 API（如
Keras）自动处理。直接使用 <code>tf.constant_initializer</code>
的情况较少见，除非是在兼容旧代码或使用低级别 API 时。</p>
<h4 id="总结-3">总结</h4>
<p><code>tf.constant_initializer</code> 是 TensorFlow
中的一个初始化器，用于将变量初始化为指定的常量值。通过设置常量值，可以确保在构建和训练神经网络时，某些变量始终具有固定的初始值。</p>
<h3 id="tf.layers.dense">tf.layers.dense</h3>
<p><code>tf.layers.dense</code> 是 TensorFlow
提供的一个高级API，用于创建全连接层。它自动创建并管理权重变量（kernel）和偏置变量（bias），并且可以通过参数来控制激活函数、初始化器、层名称等。这样可以简化神经网络的构建过程，并提供了一致性的接口。</p>
<p><strong>示例解释</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 s 是输入张量，init_w 和 init_b 是初始化器</span></span><br><span class="line">s = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, input_dim])  <span class="comment"># input_dim 是输入的特征数</span></span><br><span class="line"></span><br><span class="line">init_w = tf.random_normal_initializer(<span class="number">0.</span>, <span class="number">0.1</span>)</span><br><span class="line">init_b = tf.constant_initializer(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个全连接层</span></span><br><span class="line">net = tf.layers.dense(s, <span class="number">30</span>, activation=tf.nn.relu,</span><br><span class="line">                      kernel_initializer=init_w, bias_initializer=init_b, name=<span class="string">&#x27;l1&#x27;</span>,</span><br><span class="line">                      trainable=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>这段代码的作用是在输入张量 <code>s</code> 上创建一个具有 30
个神经元、ReLU
激活函数的全连接层，使用指定的权重和偏置初始化器，并指定该层为可训练的。这样，TensorFlow
将自动处理该层的变量初始化和管理，使得神经网络的搭建更加高效和便捷。</p>
<h4 id="上面30-个神经元的含义">上面30 个神经元的含义</h4>
<p>神经网络中的“神经元”是指一个计算单元，它接收输入、应用权重和偏置，并通过激活函数产生输出。在全连接层（dense
layer）中，神经元的数量决定了该层的输出维度。</p>
<p>在代码
<code>net = tf.layers.dense(s, 30, activation=tf.nn.relu, kernel_initializer=init_w, bias_initializer=init_b, name='l1', trainable=trainable)</code>
中，指定了这一层有 30 个神经元。具体来说：</p>
<ol type="1">
<li><strong>输入张量</strong> ：假设输入张量 <code>s</code> 的形状为
<code>[batch_size, input_dim]</code>，其中 <code>batch_size</code>
是批次大小（每次训练或推理使用的样本数量），<code>input_dim</code>
是输入特征的数量。</li>
<li><strong>神经元的作用</strong> ：每个神经元都接收输入张量
<code>s</code>
的所有特征，将这些特征与它自身的权重（weights）相乘，然后加上一个偏置（bias），再通过激活函数（如
ReLU）计算输出。</li>
<li><strong>输出维度</strong> ：具有 30 个神经元意味着输出张量的形状将是
<code>[batch_size, 30]</code>。也就是说，每个输入样本将被转换为一个 30
维的输出向量。</li>
</ol>
<h5 id="详细步骤">详细步骤</h5>
<ol type="1">
<li><strong>权重矩阵</strong> ：如果输入张量 <code>s</code> 的形状为
<code>[batch_size, input_dim]</code>，则权重矩阵 <code>W</code> 的形状为
<code>[input_dim, 30]</code>。</li>
<li><strong>计算加权和</strong> ：每个神经元计算加权和
<code>z = W*x + b</code>，其中 <code>x</code> 是输入特征，<code>W</code>
是权重，<code>b</code> 是偏置。</li>
<li><strong>应用激活函数</strong> ：计算出加权和 <code>z</code>
后，应用激活函数
<code>a = activation(z)</code>。在这个例子中，激活函数是 ReLU，即
<code>a = max(0, z)</code>。</li>
<li><strong>输出张量</strong> ：最终，每个输入样本被转换为一个 30
维的输出向量，所有样本组成的输出张量形状为
<code>[batch_size, 30]</code>。</li>
</ol>
<h5 id="举例说明">举例说明</h5>
<p>假设我们有一个输入张量 <code>s</code>，形状为
<code>[batch_size=2, input_dim=5]</code>：</p>
<p>我们定义一个具有 30 个神经元的全连接层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = tf.layers.dense(s, <span class="number">30</span>, activation=tf.nn.relu)</span><br></pre></td></tr></table></figure>
<p>输入张量 <code>s</code> 的形状是 <code>[2, 5]</code>，经过具有 30
个神经元的全连接层后，输出张量 <code>net</code> 的形状将是
<code>[2, 30]</code>。每个输入样本（2 个样本）被转换为一个 30
维的输出向量。</p>
<p>总之，具有 30 个神经元意味着这一层的输出维度是
30，每个输入样本将被转化为一个 30
维的向量。这有助于神经网络在不同层之间传递和处理信息，提取和学习更复杂的特征。</p>
<h3 id="tf.multiply">tf.multiply ：</h3>
<ul>
<li><code>tf.multiply</code> 是 TensorFlow
的一个操作，用于逐元素相乘。它的输入是两个张量，输出是这两个张量对应位置元素相乘的结果。</li>
</ul>
<h3 id="tf.get_collection">tf.get_collection</h3>
<p><code>tf.get_collection</code> 是 TensorFlow
中用于获取一个集合中的所有元素的函数。集合在 TensorFlow
中是一种组织和管理变量、操作等对象的方式，可以通过集合来方便地访问和操作这些对象。</p>
<h4 id="函数原型">函数原型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.get_collection(key, scope=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<h4 id="参数">参数</h4>
<ol type="1">
<li><p><strong><code>key</code></strong>：</p>
<ul>
<li>表示要获取的集合的键（名称），是一个字符串。</li>
<li>TensorFlow 中有一些预定义的集合键，例如
<code>tf.GraphKeys.GLOBAL_VARIABLES</code>、<code>tf.GraphKeys.TRAINABLE_VARIABLES</code>
等。</li>
</ul></li>
<li><p><strong><code>scope</code></strong>（可选）：</p>
<ul>
<li>表示要获取的集合中元素的作用域（scope）。如果指定了
<code>scope</code>，则只返回该作用域内的元素。</li>
</ul></li>
</ol>
<h4 id="返回值">返回值</h4>
<p>返回与 <code>key</code>
对应的集合中的所有元素，通常是一个列表。如果集合不存在，返回空列表。</p>
<h4 id="举例说明-1">举例说明</h4>
<p>假设我们在构建神经网络时，将一些变量添加到集合中，然后在训练或评估时需要获取这些变量，可以使用
<code>tf.get_collection</code>。</p>
<h5 id="示例代码">示例代码</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建变量并将其添加到集合中</span></span><br><span class="line">var1 = tf.Variable(<span class="number">1.0</span>, name=<span class="string">&#x27;var1&#x27;</span>)</span><br><span class="line">var2 = tf.Variable(<span class="number">2.0</span>, name=<span class="string">&#x27;var2&#x27;</span>)</span><br><span class="line">tf.add_to_collection(<span class="string">&#x27;my_collection&#x27;</span>, var1)</span><br><span class="line">tf.add_to_collection(<span class="string">&#x27;my_collection&#x27;</span>, var2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取集合中的所有变量</span></span><br><span class="line">collection_vars = tf.get_collection(<span class="string">&#x27;my_collection&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(collection_vars)</span><br></pre></td></tr></table></figure>
<h4 id="预定义集合键">预定义集合键</h4>
<p>TensorFlow 提供了一些常用的集合键，方便用户管理变量和操作：</p>
<ul>
<li><code>tf.GraphKeys.GLOBAL_VARIABLES</code>：所有全局变量。</li>
<li><code>tf.GraphKeys.TRAINABLE_VARIABLES</code>：所有可训练的变量。</li>
<li><code>tf.GraphKeys.SUMMARIES</code>：所有摘要操作。</li>
<li><code>tf.GraphKeys.UPDATE_OPS</code>：所有需要在训练时执行的更新操作。</li>
</ul>
<h4 id="在实际应用中的使用">在实际应用中的使用</h4>
<h5 id="获取所有可训练变量">获取所有可训练变量</h5>
<p>在训练神经网络时，我们通常需要获取所有可训练的变量，例如在优化器中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.01</span>)</span><br><span class="line">train_op = optimizer.minimize(loss, var_list=trainable_vars)</span><br></pre></td></tr></table></figure>
<h5 id="获取特定作用域内的变量">获取特定作用域内的变量</h5>
<p>我们还可以使用 <code>scope</code> 参数来获取特定作用域内的变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;layer1&#x27;</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">&#x27;var1&#x27;</span>, shape=[<span class="number">10</span>])</span><br><span class="line">  </span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;layer2&#x27;</span>):</span><br><span class="line">    var2 = tf.get_variable(<span class="string">&#x27;var2&#x27;</span>, shape=[<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">layer1_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="string">&#x27;layer1&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(layer1_vars)  <span class="comment"># 只会打印 layer1 作用域内的变量</span></span><br></pre></td></tr></table></figure>
<h4 id="总结-4">总结</h4>
<p><code>tf.get_collection</code>
是一个强大的函数，允许用户根据集合键和作用域来获取 TensorFlow
图中的变量和操作。它在组织和管理复杂模型中的变量和操作时非常有用，尤其是在训练、保存和恢复模型时。通过合理使用集合和
<code>tf.get_collection</code>，可以使代码更加清晰和易于维护。</p>
<h3 id="tf.stop_gradient">tf.stop_gradient</h3>
<p><code>tf.stop_gradient</code> 是 TensorFlow
中用于阻止梯度反向传播的操作。它返回一个与输入张量具有相同内容的新张量，但在计算梯度时会被视为常量。换句话说，<code>tf.stop_gradient</code>
可以用来“截断”梯度的传播，这在某些情况下（例如实现某些特殊的梯度更新规则或避免某些变量的梯度更新）非常有用。</p>
<h4 id="函数原型-1">函数原型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.stop_gradient(</span><br><span class="line">    <span class="built_in">input</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="参数-1">参数</h4>
<ul>
<li><strong><code>input</code></strong>：输入张量。</li>
<li><strong><code>name</code></strong>（可选）：操作名。</li>
</ul>
<h4 id="返回值-1">返回值</h4>
<p>返回一个与输入张量具有相同内容的新张量，但在反向传播过程中，该张量的梯度将被视为零。</p>
<h4 id="使用示例-1">使用示例</h4>
<h5 id="示例1简单示例">示例1：简单示例</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个变量和一个操作</span></span><br><span class="line">x = tf.Variable(<span class="number">2.0</span>, name=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 tf.stop_gradient 阻止梯度传播</span></span><br><span class="line">y_no_grad = tf.stop_gradient(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数</span></span><br><span class="line">loss = y_no_grad + x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算梯度</span></span><br><span class="line">gradients = tf.gradients(loss, [x])</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>y_no_grad</code> 是通过
<code>tf.stop_gradient</code> 创建的，因此在计算 <code>loss</code>
的梯度时，<code>y_no_grad</code> 不会对 <code>x</code>
的梯度产生影响。最终结果是 <code>gradients</code> 只包含 <code>x</code>
的梯度，即 1，而不是 <code>y = x * 2</code> 导致的 2。</p>
<h5 id="示例2在神经网络中的应用">示例2：在神经网络中的应用</h5>
<p>在一些强化学习算法中，我们可能希望通过停止梯度来实现某些特殊的策略。例如，在
DDPG 算法中，actor 网络的输出动作传递给 critic 网络，但我们不希望通过
critic 网络更新 actor 网络的参数，可以使用 <code>tf.stop_gradient</code>
实现这一点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一个 actor 网络输出动作 a</span></span><br><span class="line">a = actor_network(state)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们希望通过 critic 网络计算梯度，但不希望更新 actor 网络</span></span><br><span class="line">q_value = critic_network(tf.stop_gradient(a))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失并更新 critic 网络</span></span><br><span class="line">critic_loss = -tf.reduce_mean(q_value)</span><br><span class="line">critic_optimizer = tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">critic_train_op = critic_optimizer.minimize(critic_loss)</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>tf.stop_gradient(a)</code> 确保了
<code>critic_network</code> 的梯度不会影响 <code>actor_network</code>
的参数更新。</p>
<h4 id="总结-5">总结</h4>
<p><code>tf.stop_gradient</code>
是一个非常有用的操作，特别是在需要控制梯度传播路径或实现自定义梯度更新规则时。通过合理使用
<code>tf.stop_gradient</code>，可以更灵活地设计和训练复杂的模型。</p>
<h3 id="软替换代码">软替换代码</h3>
<p>代码理解:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.soft_replace = [tf.assign(t, (<span class="number">1</span> - self.replacement[<span class="string">&#x27;tau&#x27;</span>]) * t + self.replacement[<span class="string">&#x27;tau&#x27;</span>] * e)</span><br><span class="line">                     <span class="keyword">for</span> t, e <span class="keyword">in</span> <span class="built_in">zip</span>(self.t_params, self.e_params)]</span><br></pre></td></tr></table></figure>
<p>这一行代码的作用是实现软替换（soft
replacement），用来逐步更新目标网络的参数，使其接近评估网络的参数。在强化学习中，软替换是一种常见的参数更新策略，常用于深度确定性策略梯度（DDPG）等算法。</p>
<h4 id="让我们逐步解析这段代码">让我们逐步解析这段代码：</h4>
<ol type="1">
<li><p><strong><code>zip(self.t_params, self.e_params)</code></strong>：</p>
<ul>
<li><code>self.t_params</code> 包含目标网络的所有参数。</li>
<li><code>self.e_params</code> 包含评估网络的所有参数。</li>
<li><code>zip</code>
函数将这两个参数列表配对，使得每对包含一个目标网络的参数和一个评估网络的参数。</li>
</ul></li>
<li><p><strong>列表推导式</strong>：</p>
<ul>
<li>列表推导式用于生成一个包含多个 TensorFlow
操作的列表。在这里，每个操作都是一个 <code>tf.assign</code>
操作，用于更新目标网络的参数。</li>
</ul></li>
<li><p><strong><code>tf.assign</code></strong>：</p>
<ul>
<li><code>tf.assign</code> 是 TensorFlow
中用于更新变量值的操作。它接受两个参数：要更新的变量和新的值。</li>
<li><code>t</code> 是目标网络的参数，<code>e</code>
是评估网络的参数。</li>
</ul></li>
<li><p><strong>软更新公式</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span> - self.replacement[<span class="string">&#x27;tau&#x27;</span>]) * t + self.replacement[<span class="string">&#x27;tau&#x27;</span>] * e</span><br></pre></td></tr></table></figure>
<ul>
<li><code>self.replacement['tau']</code>
是软更新的速率，通常是一个小值（如
0.01）。它控制了目标网络参数向评估网络参数靠拢的速度。</li>
<li><code>(1 - self.replacement['tau']) * t</code>
保留了目标网络参数的大部分值。</li>
<li><code>self.replacement['tau'] * e</code>
引入了一小部分评估网络参数的值。</li>
<li>通过这种加权平均的方式，目标网络参数逐步向评估网络参数靠拢，但不会立即完全相同。</li>
</ul></li>
<li><p><strong>生成软替换操作列表</strong>：</p>
<ul>
<li>对于每一对目标网络参数 <code>t</code> 和评估网络参数
<code>e</code>，生成一个 <code>tf.assign</code>
操作，将目标网络参数更新为加权平均后的值。</li>
<li><code>self.soft_replace</code> 最终是一个包含所有这些
<code>tf.assign</code> 操作的列表。</li>
</ul></li>
</ol>
<h4 id="软替换的具体作用">软替换的具体作用</h4>
<p>在强化学习算法中，目标网络和评估网络的更新策略对算法的稳定性至关重要。软替换通过逐步更新目标网络的参数，可以减少训练过程中的振荡和不稳定性。相对于硬替换（每隔一段时间直接将评估网络的参数复制到目标网络），软替换更为平滑和稳定，有助于提高算法的训练效果。</p>
<h4 id="示例-1">示例</h4>
<p>假设评估网络和目标网络的参数分别为 <code>e_params = [1, 2, 3]</code>
和 <code>t_params = [4, 5, 6]</code>，且
<code>tau = 0.1</code>，则软替换后的目标网络参数计算如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">new_t_params = [(<span class="number">1</span> - <span class="number">0.1</span>) * <span class="number">4</span> + <span class="number">0.1</span> * <span class="number">1</span>, (<span class="number">1</span> - <span class="number">0.1</span>) * <span class="number">5</span> + <span class="number">0.1</span> * <span class="number">2</span>, (<span class="number">1</span> - <span class="number">0.1</span>) * <span class="number">6</span> + <span class="number">0.1</span> * <span class="number">3</span>]</span><br><span class="line">              = [<span class="number">3.7</span>, <span class="number">4.7</span>, <span class="number">5.7</span>]</span><br></pre></td></tr></table></figure>
<p>每次更新后，目标网络的参数都会逐渐向评估网络的参数靠拢，但不会立即相同，从而实现平滑的参数更新。</p>
<h3 id="硬替换代码">硬替换代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.t_replace_counter = <span class="number">0</span></span><br><span class="line">            self.hard_replace = [tf.assign(t, e) <span class="keyword">for</span> t, e <span class="keyword">in</span> <span class="built_in">zip</span>(self.t_params, self.e_params)]</span><br></pre></td></tr></table></figure>
<h3 id="tf.gradients">tf.gradients</h3>
<p><code>tf.gradients</code> 是 TensorFlow
中用于计算梯度的函数。它的主要功能是计算某个或某些张量（<code>ys</code>）相对于另一些张量（<code>xs</code>）的导数。</p>
<p>其函数签名如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.gradients(ys, xs, grad_ys=<span class="literal">None</span>, name=<span class="string">&#x27;gradients&#x27;</span>, colocate_gradients_with_ops=<span class="literal">False</span>, gate_gradients=<span class="literal">False</span>, aggregation_method=<span class="literal">None</span>, stop_gradients=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数说明如下：</p>
<ul>
<li><code>ys</code>：表示要计算梯度的张量或张量列表。</li>
<li><code>xs</code>：表示要对哪些张量计算梯度的张量或张量列表。</li>
<li><code>grad_ys</code>（可选）：也是一个张量或张量列表，用于对
<code>ys</code> 中的每个元素进行加权。如果未提供，则默认为每个元素都是
1。</li>
<li><code>name</code>（可选）：操作的名称。</li>
<li><code>colocate_gradients_with_ops</code>（可选）：通常不需要设置。</li>
<li><code>gate_gradients</code>（可选）：通常不需要设置。</li>
<li><code>aggregation_method</code>（可选）：聚合方法，一般使用默认值即可。</li>
<li><code>stop_gradients</code>（可选）：是一个张量或张量列表，用于指定在反向传播中梯度停止的节点。这意味着这些节点的梯度不会再向后传播，就好像它们被明确地断开了一样。</li>
</ul>
<p>例如，如果 <code>ys</code> 是一个张量，<code>xs</code>
是一个张量列表，那么返回的是一个与 <code>xs</code>
长度相同的列表，其中每个元素是 <code>ys</code> 相对于 <code>xs</code>
中对应张量的导数。</p>
<p>如果 <code>ys</code> 和 <code>xs</code>
都是张量列表，那么它会计算每个 <code>ys</code> 中的张量相对于每个
<code>xs</code> 中的张量的导数，并将结果以适当的方式组合成一个列表。</p>
<p>下面是一个简单的示例，假设有两个变量 <code>w1</code> 和
<code>w2</code>，通过矩阵乘法等操作得到结果 <code>res</code>，然后使用
<code>tf.gradients</code> 计算 <code>res</code> 相对于 <code>w1</code>
和 <code>w2</code> 的梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(((<span class="number">1</span>, <span class="number">2</span>)))  </span><br><span class="line">w2 = tf.Variable(((<span class="number">3</span>, <span class="number">4</span>)))  </span><br><span class="line">res = tf.matmul(w1, ((<span class="number">2</span>), (<span class="number">1</span>))) + tf.matmul(w2, ((<span class="number">3</span>), (<span class="number">5</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 res 相对于 w1 和 w2 的梯度</span></span><br><span class="line">grads = tf.gradients((res,), (w1, w2))  </span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()  </span><br><span class="line">    re = sess.run(grads)  </span><br><span class="line">    <span class="built_in">print</span>(re) </span><br></pre></td></tr></table></figure>
<p>在上述示例中，<code>tf.gradients</code>
返回了一个包含两个梯度张量的列表，分别对应 <code>res</code> 相对于
<code>w1</code> 和 <code>w2</code> 的梯度。</p>
<p><code>stop_gradients</code> 参数的使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant((<span class="number">0.0</span>))  </span><br><span class="line">b = a * <span class="number">2</span>  </span><br><span class="line"><span class="comment"># 计算 a+b 相对于 a 和 b 的梯度，stop_gradients 指定为 (a, b)，表示 a 和 b 的梯度不会再向后传播</span></span><br><span class="line">pg = tf.gradients(a+b, (a, b), stop_gradients=(a, b))  </span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    res = sess.run(pg)  </span><br><span class="line">    <span class="built_in">print</span>(res) </span><br></pre></td></tr></table></figure>
<p>在这个例子中，由于使用了 <code>stop_gradients=(a, b)</code>，所以
<code>a</code> 和 <code>b</code>
的梯度在计算后不会再向后传播，得到的梯度结果为
<code>(1.0, 1.0)</code>，而不是通常情况下的
<code>(3.0, 1.0)</code>。</p>
<p>如何理解stop_gradients</p>
<p>具体来说，对于表达式
<code>pg = tf.gradients(a+b, (a, b), stop_gradients=(a, b))</code>，意味着在计算
<code>a+b</code> 相对于 <code>a</code> 和 <code>b</code>
的梯度时，<code>a</code> 和 <code>b</code>
本身的梯度不会再进一步向后传播。也就是说，<code>a</code>
的梯度不会影响到依赖于 <code>a</code> 的其他操作，<code>b</code>
的梯度也不会影响到依赖于 <code>b</code> 的其他操作。</p>
<p>这在一些情况下是有用的，例如当你不希望某些变量的梯度继续传播，或者想要模拟某些部分的梯度为固定值的情况。通过设置
<code>stop_gradients</code>，可以更灵活地控制梯度的计算和传播。</p>
<p>例如，如果 <code>a</code> 和 <code>b</code>
是神经网络中的某些中间节点，并且你不希望它们的梯度对网络的其他部分产生影响，就可以将它们放入
<code>stop_gradients</code> 中。这样，在反向传播时，就只会计算到
<code>a+b</code> 这一步，而不会将梯度继续传播到 <code>a</code> 和
<code>b</code> 所依赖的其他变量上。</p>
<p>举个简单的例子，假设有如下计算图：<code>c = a + b</code>，<code>d = c * x</code>，如果计算
<code>d</code> 相对于 <code>a</code> 的梯度，正常情况下梯度会从
<code>d</code> 传播到 <code>c</code>，再传播到
<code>a</code>。但如果使用
<code>stop_gradients=(a, b)</code>，那么梯度只会传播到
<code>c</code>，而不会再传播到 <code>a</code> 和 <code>b</code>，即
<code>a</code> 和 <code>b</code>
的梯度不会再向后传播。这样可以在一定程度上实现对梯度传播的控制和干预。</p>
<p>需要注意的是，<code>tf.stop_gradient</code>
是在构建图的过程中使用，用来指定停止链式法则的节点；而
<code>stop_gradients</code> 是在构建图之后使用，当程序运行碰到在
<code>stop_gradients</code>
中定义的节点时，均会停止链式法则，进而求得部分偏导。它们的作用都是阻止梯度的进一步传播，但使用的时机略有不同。</p>
<p><code>grad_ys</code> 参数用于对梯度进行加权，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, shape=(<span class="number">3</span>))  </span><br><span class="line">w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, shape=(<span class="number">3</span>))  </span><br><span class="line">w3 = tf.get_variable(<span class="string">&#x27;w3&#x27;</span>, shape=(<span class="number">3</span>))  </span><br><span class="line">w4 = tf.get_variable(<span class="string">&#x27;w4&#x27;</span>, shape=(<span class="number">3</span>))  </span><br><span class="line">z1 = <span class="number">3</span>*w1 + <span class="number">2</span>*w2 + w3  </span><br><span class="line">z2 = -<span class="number">1</span>*w3 + w4</span><br><span class="line"></span><br><span class="line">grad_ys = (tf.convert_to_tensor((<span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>)), tf.convert_to_tensor((<span class="number">3.0</span>, <span class="number">2.0</span>, <span class="number">4.0</span>)))</span><br><span class="line">grads = tf.gradients((z1, z2), (w1, w2, w3, w4), grad_ys=grad_ys)  </span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()  </span><br><span class="line">    <span class="built_in">print</span>(sess.run(grads)) </span><br></pre></td></tr></table></figure>
<p>这里的 <code>grad_ys</code> 对 <code>z1</code> 和 <code>z2</code>
的梯度进行了加权，最终得到的梯度结果会相应地受到加权的影响。</p>
<p>在 TensorFlow 2.x 中也可以使用
<code>tf.gradients</code>，但通常更推荐使用 <code>tf.GradientTape</code>
来进行自动求导，它在使用上更加灵活和方便。不过，在某些情况下，仍然可能需要使用
<code>tf.gradients</code>，例如在一些特定的模型架构或代码结构中。但要注意将其放在计算图中使用。如果不在计算图中使用，就需要使用
<code>tf.GradientTape</code> 来替代。</p>
<h4 id="tf.gradients的输入和输出之间的形状的关系"><code>tf.gradients</code>的输入和输出之间的形状的关系</h4>
<p>在 TensorFlow 中，<code>tf.gradients()</code>
用于计算梯度。它接受求导的目标值 <code>ys</code> 和自变量
<code>xs</code>，返回的梯度值与 <code>ys</code> 和 <code>xs</code>
张量之间的形状关系如下：</p>
<hr>
<p><strong><code>tf.gradients()</code> 返回的结果是一个长度为
<code>len(xs)</code>
的张量列表</strong>。<strong>--------------这是重点</strong></p>
<p><strong>一般情况下，<code>tf.gradients(self.q, self.a)</code>
的返回值形状应该是与 <code>self.a</code> 的形状相同</strong></p>
<hr>
<p>如果 <code>ys</code> 是单个张量，那么每个返回的张量表示相应的
<code>xs</code> 元素对 <code>ys</code> 的梯度。如果 <code>ys</code>
是张量列表，那么返回的张量列表中的每个张量是所有 <code>ys</code>
张量对相应 <code>xs</code> 元素的梯度之和。</p>
<p>具体来说，假设 <code>ys = (y1, y2,..., ym)</code>
是一个张量列表，<code>xs = (x1, x2,..., xn)</code>
也是一个张量列表。那么返回的梯度列表中第 <code>i</code>
个张量表示的是所有 <code>y</code> 张量对 <code>xi</code>
的梯度之和，即：</p>
<p><span class="math inline">\(\frac{d(y1 + y2 +... +
ym)}{dx_i}\)</span></p>
<p>如果 <code>ys</code>
是单个张量，那么返回的列表中只有一个张量，其形状与 <code>xs</code>
中的元素形状相对应，表示该 <code>ys</code> 张量对每个 <code>xs</code>
元素的梯度。</p>
<p>例如，当 <code>ys = (y1, y2)</code>，<code>xs = (x1, x2, x3)</code>
时，返回的梯度结果为 <code>(grad1, grad2, grad3)</code>，其中：</p>
<ul>
<li><code>grad1</code> 表示的是 <span class="math inline">\(\frac{y1}{x1} + \frac{y2}{x1}\)</span></li>
<li><code>grad2</code> 表示的是 <span class="math inline">\(\frac{y1}{x2} + \frac{y2}{x2}\)</span></li>
<li><code>grad3</code> 表示的是 <span class="math inline">\(\frac{y1}{x3} + \frac{y2}{x3}\)</span></li>
</ul>
<p>另外，<code>tf.gradients()</code> 还有一个参数
<code>grad_ys</code>，它也是一个张量列表，用于对 <code>ys</code>
中的每个张量进行加权。如果提供了
<code>grad_ys</code>，则计算梯度时会将每个 <code>y</code> 张量乘以对应的
<code>grad_ys</code> 张量后再进行求导。</p>
<p>例如，在上述同样的 <code>ys</code> 和 <code>xs</code> 的情况下，如果
<code>grad_ys = (g1, g2)</code>，那么计算的梯度将变为：</p>
<ul>
<li><code>grad1</code> 表示的是 <span class="math inline">\(\frac{g1 *
y1}{x1} + \frac{g2 * y2}{x1}\)</span></li>
<li><code>grad2</code> 表示的是 <span class="math inline">\(\frac{g1 *
y1}{x2} + \frac{g2 * y2}{x2}\)</span></li>
<li><code>grad3</code> 表示的是 <span class="math inline">\(\frac{g1 *
y1}{x3} + \frac{g2 * y2}{x3}\)</span></li>
</ul>
<h4 id="如何理解-grad_ys">如何理解 <code>grad_ys</code></h4>
<p><code>grad_ys</code>（可选）是一个张量或张量列表，用于对
<code>ys</code> 中的每个元素进行加权。</p>
<p>当使用 <code>tf.gradients(ys, xs, grad_ys)</code> 时，如果
<code>grad_ys</code> 不为空，就需要使用梯度求导的链式法则来计算相对于
<code>xs</code> 的导数。</p>
<p>假设 <code>grad_ys = (grad_ys1, grad_ys2,..., grad_ysn)</code>，其中
<code>n</code> 是 <code>ys</code>
中元素的个数，<code>xs = (x1, x2,..., xm)</code>。那么对于
<code>xs</code> 中的每个元素 <code>xi</code>，其梯度计算方式为：</p>
<p>新的梯度
<code>new_grad(i) = ∂/∂xi = ∂/∂z1 * ∂z1/∂xi + ∂/∂z2 * ∂z2/∂xi +... + ∂/∂zn * ∂zn/∂xi</code>，其中
<code>z1, z2,..., zn</code> 是 <code>ys</code> 中的元素。</p>
<p>也就是说，<code>grad_ys</code> 中的每个张量会分别与对应的
<code>ys</code> 元素相乘，然后在计算相对于 <code>xs</code>
的梯度时，这些加权后的结果会参与到链式法则的计算中。</p>
<p>例如，在代码示例中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, shape=(<span class="number">3</span>))</span><br><span class="line">w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, shape=(<span class="number">3</span>))</span><br><span class="line">w3 = tf.get_variable(<span class="string">&#x27;w3&#x27;</span>, shape=(<span class="number">3</span>))</span><br><span class="line">w4 = tf.get_variable(<span class="string">&#x27;w4&#x27;</span>, shape=(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">z1 = <span class="number">3</span>*w1 + <span class="number">2</span>*w2 + w3</span><br><span class="line">z2 = -<span class="number">1</span>*w3 + w4</span><br><span class="line"></span><br><span class="line">grads = tf.gradients((z1, z2), (w1, w2, w3, w4), grad_ys=((-<span class="number">2.0</span>,-<span class="number">3.0</span>,-<span class="number">4.0</span>), (-<span class="number">2.0</span>,-<span class="number">3.0</span>,-<span class="number">4.0</span>)))</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    <span class="built_in">print</span>(sess.run(grads))</span><br></pre></td></tr></table></figure>
<p>如果不考虑 <code>grad_ys</code>，输出应该是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(array((3., 3., 3.), dtype=float32),</span><br><span class="line"> array((2., 2., 2.), dtype=float32),</span><br><span class="line"> array((1., 1., 1.), dtype=float32),</span><br><span class="line"> array((1., 1., 1.), dtype=float32))</span><br></pre></td></tr></table></figure>
<p>现在在权重参数
<code>grad_ys = ((-2.0,-3.0,-4.0), (-2.0,-3.0,-4.0))</code>
的加权下，输出实际为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(array((-6., -9., -12.), dtype=float32),</span><br><span class="line"> array((-4., -6., -8.), dtype=float32),</span><br><span class="line"> array((0., 0., 0.), dtype=float32),</span><br><span class="line"> array((1., 1., 1.), dtype=float32))</span><br></pre></td></tr></table></figure>
<p>具体计算过程为，对于 <code>w1</code> 的梯度，原来的梯度是
<code>3</code>（来自 <code>z1</code> 对 <code>w1</code>
的偏导），现在加权后为 <code>-2.0 * 3 = -6.0</code>；对于
<code>w2</code> 的梯度，原来的梯度是 <code>2</code>（来自
<code>z1</code> 对 <code>w2</code> 的偏导），加权后为
<code>-3.0 * 2 = -6.0</code>；对于 <code>w3</code> 的梯度，来自
<code>z1</code> 的部分为 <code>-2.0 * 1 = -2.0</code>，来自
<code>z2</code> 的部分为 <code>-2.0 * (-1) = 2.0</code>，相加得到
<code>0.0</code>。</p>
<p>这样，通过设置 <code>grad_ys</code>，可以对 <code>ys</code>
中每个元素的梯度进行不同的加权，从而更灵活地控制梯度的计算。在实际应用中，这可以根据具体的需求和模型架构来调整梯度的传播和计算方式。</p>
<h5 id="为什么需要加权">为什么需要加权</h5>
<p>在某些情况下，需要对梯度进行加权是为了在计算梯度时对不同的部分进行不同程度的强调或重视。</p>
<p>例如，在深度学习中，不同的输出或误差项可能对最终的结果有不同的重要性。通过为
<code>grad_ys</code>中的每个张量设置合适的权重，可以根据这些重要性的差异来调整梯度的计算。</p>
<p>具体来说，加权可以用于以下几种情况：</p>
<ol type="1">
<li><strong>强调重要的输出或误差</strong>：如果某些输出或误差对于模型的性能或目标更关键，那么可以给它们的梯度分配较大的权重，以便在反向传播过程中更显著地影响参数的调整。</li>
<li><strong>平衡不同规模或量级的因素</strong>：当不同的输出或误差具有不同的量级时，加权可以帮助平衡它们对梯度的贡献，避免较大量级的部分主导了梯度的计算。</li>
<li><strong>实现特定的优化目标</strong>：根据具体的优化目标或任务需求，设置特定的权重来引导模型的学习方向。</li>
<li><strong>处理多任务学习</strong>：在多任务学习场景中，不同的任务可能具有不同的重要性，通过加权梯度可以更好地协调不同任务之间的学习。</li>
</ol>
<p>例如，假设有两个输出 <code>y1</code>和
<code>y2</code>，对应的梯度分别为 <code>dy1/dx</code>和
<code>dy2/dx</code>。如果直接计算梯度之和，那么它们对最终梯度的贡献是相等的。但如果
<code>y1</code>相对更重要，就可以为
<code>dy1/dx</code>设置较大的权重，使得它在决定参数调整时起到更大的作用。</p>
<p>在 TensorFlow 中，通过设置
<code>grad_ys</code>参数来实现加权梯度计算。<code>grad_ys</code>是一个与
<code>ys</code>长度相同的张量列表，其中每个张量用于对相应的
<code>ys</code>元素的梯度进行加权。这样，在计算梯度时，就会考虑这些加权值，从而实现对不同部分梯度的不同程度的重视。</p>
<p>例如，在上述代码示例中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grad_ys = (tf.variable(((<span class="number">1</span>))), tf.variable(((<span class="number">2</span>))))</span><br><span class="line">grads = tf.gradients((res2a, res2b), (w1, w2), grad_ys=grad_ys)</span><br></pre></td></tr></table></figure>
<p>这里 <code>grad_ys</code>被设置为
<code>(tf.variable(((1))), tf.variable(((2))))</code>，表示在计算梯度时，<code>res2a</code>的梯度将乘以
1，而 <code>res2b</code>的梯度将乘以 2，从而实现了对
<code>res2b</code>的梯度相对更重视的效果。这样，在后续的反向传播和参数更新中，<code>res2b</code>的梯度将对参数调整产生更大的影响。</p>
<p>总的来说，加权梯度提供了一种灵活的方式，根据具体问题和需求，对不同部分的梯度进行定制化的处理，以更好地实现模型的学习和优化目标。但确定合适的权重通常需要对问题有深入的理解，并可能需要通过试验和调优来找到最优的设置。</p>
<h5 id="如何为-grad_ys-中的每个张量设置合适的权重">如何为
<code>grad_ys</code> 中的每个张量设置合适的权重？</h5>
<p>为
<code>grad_ys</code>中的每个张量设置合适的权重，需要根据具体的问题和模型需求来决定。设置权重的目的是为了在计算梯度时对不同的部分进行不同程度的加权。</p>
<p>一种常见的方法是根据对每个元素的重要性或关注度来分配权重。例如，如果某个元素对于最终结果的影响较大，或者你希望在梯度计算中更强调该元素的变化，那么可以为其分配较大的权重；反之，如果某个元素相对不那么重要，可以分配较小的权重。</p>
<p>另外，也可以基于一些先验知识或经验来设置权重。例如，在某些情况下，可能知道某些部分的梯度对于整体优化的重要性程度。</p>
<p>然而，确定最合适的权重通常需要通过试验和调优来找到。这可能涉及尝试不同的权重组合，并观察模型在训练或优化过程中的性能表现，以找到能够达到最佳效果的权重设置。</p>
<p>例如，在上述代码示例中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grad_ys = (tf.variable(((<span class="number">1</span>))), tf.variable(((<span class="number">2</span>))))</span><br><span class="line">grads = tf.gradients((res2a, res2b), (w1, w2), grad_ys=grad_ys)</span><br></pre></td></tr></table></figure>
<p>这里 <code>grad_ys</code>被设置为
<code>(tf.variable(((1))), tf.variable(((2))))</code>，表示在计算梯度时，<code>res2a</code>的梯度将乘以
1，而 <code>res2b</code>的梯度将乘以
2，从而实现了对不同部分梯度的不同加权。</p>
<p>具体到你的问题中，如何设置权重取决于你的具体任务和数据特点。你可能需要对问题有深入的理解，并结合一些实验和观察来找到合适的权重设置。这可能需要一些尝试和错误，以及对模型输出和性能的仔细分析。同时，也可以考虑使用一些自动调参的技术或算法来帮助找到最优的权重配置。</p>
<h3 id="np.hstack">np.hstack</h3>
<p><code>np.hstack</code> 是 NumPy
库中的一个函数，用于水平（按列）堆叠数组。</p>
<p>其函数签名通常为 <code>np.hstack(tup)</code> ，其中 <code>tup</code>
是一个元组，包含要堆叠的数组。</p>
<p>例如，如果有两个数组 <code>a = np.array([1, 2, 3])</code> 和
<code>b = np.array([4, 5, 6])</code> ，使用
<code>np.hstack((a, b))</code> 将会得到一个新的数组
<code>[1, 2, 3, 4, 5, 6]</code> 。</p>
<p>再比如，如果 <code>a = np.array([[1, 2], [3, 4]])</code>
，<code>b = np.array([[5, 6], [7, 8]])</code> ，那么
<code>np.hstack((a, b))</code> 的结果是
<code>[[1, 2, 5, 6], [3, 4, 7, 8]]</code> 。</p>
<p><code>np.hstack</code>
主要用于将多个数组在水平方向上拼接成一个新的数组，前提是参与拼接的数组在除了要拼接的维度（这里是列维度）之外的其他维度上形状要匹配。</p>
<h3 id="td-error">td error</h3>
<p>TD error 即时间差分误差（Temporal Difference
error），它是强化学习中一个重要的概念，用于衡量当前估计值与目标值之间的差异。</p>
<p>其定义为：当前时刻的状态值或动作值的估计与基于即时回报和下一时刻状态值或动作值的估计所构成的目标值之间的差值。</p>
<p>例如，在某个时刻 t，TD error 可以表示为：<span class="math inline">\(\delta_t = V_t(s_t) - (r_{t+1} + \gamma
V_t(s_{t+1}))\)</span> 或者 <span class="math inline">\(\delta_t =
Q_t(s_t, a_t) - (r_{t+1} + \gamma Q_t(s_{t+1}, a_{t+1}))\)</span> ，其中
<span class="math inline">\(V_t(s_t)\)</span> 表示 t 时刻对状态 <span class="math inline">\(s_t\)</span> 的状态值估计，<span class="math inline">\(Q_t(s_t, a_t)\)</span> 表示 t 时刻对状态 <span class="math inline">\(s_t\)</span> 采取动作 <span class="math inline">\(a_t\)</span> 的动作值估计，<span class="math inline">\(r_{t+1}\)</span> 是即时回报，<span class="math inline">\(\gamma\)</span> 是折扣系数，<span class="math inline">\(V_t(s_{t+1})\)</span> 和 <span class="math inline">\(Q_t(s_{t+1}, a_{t+1})\)</span>
分别是下一时刻的状态值估计和动作值估计。</p>
<p>TD error 的主要作用是指导强化学习算法的学习和更新过程。通过计算 TD
error，算法可以了解当前的估计与实际情况之间的差距，并据此调整模型的参数，以使得估计值更接近真实值。较小的
TD error 表示当前的估计较为准确，而较大的 TD error
则意味着模型需要进行更大的调整。</p>
<p>在强化学习中，一些算法（如 Q-learning、SARSA 等）会使用 TD error
来更新价值函数或策略，以逐步优化智能体的行为，使其能够在环境中获得更好的累积回报。</p>
<p>TD error
具有一些优点，例如它可以在没有完整状态序列的情况下进行学习，能够更快速灵活地更新状态或动作的价值估计。然而，它得到的价值是一种有偏估计，但其方差通常比其他方法（如蒙特卡洛方法）得到的方差要低。</p>
<p>不同的强化学习场景和算法中，TD error
的具体形式和应用可能会有所不同，但总体上都是用于衡量估计值与目标值的差异，以推动学习和优化的进行。如果在强化学习训练中遇到
TD error
值很大的情况，可能的原因包括网络架构不够复杂、训练样本数量不足、学习率过大或没有使用足够多的经验回放等。可以尝试使用更复杂的网络架构、增加训练数据、调低学习率、增加经验回放或使用更多的超参数调整方法来解决。</p>
<h3 id="tf.distributions.normal">tf.distributions.Normal</h3>
<p><code>tf.distributions.Normal</code> 是 TensorFlow
中用于定义正态分布（高斯分布）的类。</p>
<p>它的初始化函数 <code>__init__</code> 的参数如下：</p>
<ul>
<li><code>loc</code>：表示正态分布的均值（μ），是一个浮点型张量。</li>
<li><code>scale</code>：表示正态分布的标准差（σ），也是一个浮点型张量，且必须包含只正数值。</li>
<li><code>validate_args</code>：一个 Python 布尔值，默认为
<code>False</code>。当为 <code>True</code> 时，会进行参数验证。</li>
<li><code>allow_nan_stats</code>：一个 Python 布尔值，默认为
<code>True</code>，用于描述在统计量未定义时的行为。</li>
</ul>
<p>通过创建 <code>tf.distributions.Normal</code>
对象，可以进行与正态分布相关的操作，例如采样、计算概率、计算熵、交叉熵、KL
散度等。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow_probability <span class="keyword">as</span> tfp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个均值为 0，标准差为 1 的正态分布</span></span><br><span class="line">dist = tfp.distributions.Normal(loc=<span class="number">0.</span>, scale=<span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从该正态分布中采样</span></span><br><span class="line">samples = dist.sample((<span class="number">3</span>,)) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算某个值的概率密度函数（pdf）值</span></span><br><span class="line">pdf_value = dist.prob(<span class="number">0.5</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算累积分布函数（cdf）值</span></span><br><span class="line">cdf_value = dist.cdf(<span class="number">0.5</span>) </span><br></pre></td></tr></table></figure>
<p>在正态分布中，概率密度函数（pdf）的公式为：</p>
<p><span class="math display">\[
pdf(x; \mu, \sigma) = \frac{exp(-0.5(x - \mu)^2 / \sigma^2)}{\sqrt{(2\pi
\sigma^2)}}
\]</span></p>
<p>其中，<code>loc = mu</code> 是均值，<code>scale = sigma</code>
是标准差，<code>z</code> 是归一化常数。</p>
<p>正态分布是位置-尺度族的一员，也就是说，可以通过以下方式构建：</p>
<p><span class="math display">\[
x \sim Normal(loc=0, scale=1)
\]</span></p>
<p><span class="math display">\[
y = loc + scale * x
\]</span></p>
<p><code>tf.distributions.Normal</code> 类中还提供了其他一些方法，如
<code>covariance</code>（协方差）、<code>cross_entropy</code>（交叉熵）、<code>entropy</code>（熵）、<code>log_cdf</code>（对数累积分布函数）、<code>log_prob</code>（对数概率）、<code>log_survival_function</code>（对数生存函数）、<code>mean</code>（均值）、<code>mode</code>（众数）、<code>stddev</code>（标准差）、<code>variance</code>（方差）等，这些方法可以方便地用于各种与正态分布相关的计算和操作。具体使用方法可以参考
TensorFlow 的官方文档。</p>
<p>这些方法使得在 TensorFlow
中处理正态分布变得更加方便和高效，尤其是在涉及到概率计算、随机采样以及与其他分布进行比较和组合的任务中。例如，在一些机器学习和深度学习模型中，可能需要使用正态分布来初始化参数、生成随机噪声或者构建概率模型等。通过
<code>tf.distributions.Normal</code> 类，可以以一种与 TensorFlow
计算图兼容的方式进行这些操作，从而更好地利用 TensorFlow
的自动微分和优化功能。同时，还可以方便地处理批量数据（即同时处理多个正态分布，每个分布具有不同的参数），这在处理大规模数据或多任务学习等场景中非常有用。</p>
<h3 id="tf.clip_by_value">tf.clip_by_value</h3>
<p><code>tf.clip_by_value</code>是 TensorFlow
中的一个函数，用于将张量中的数值限制在指定的范围内。</p>
<p>它的语法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.clip_by_value(t, clip_value_min, clip_value_max, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>t</code>：要进行裁剪的张量。</li>
<li><code>clip_value_min</code>：裁剪的最小值。如果张量中的值小于这个最小值，它们将被设置为这个最小值。</li>
<li><code>clip_value_max</code>：裁剪的最大值。如果张量中的值大于这个最大值，它们将被设置为这个最大值。</li>
<li><code>name</code>（可选）：操作的名称。</li>
</ul>
<p>例如，如果有一个张量 <code>t</code>，使用
<code>tf.clip_by_value</code>可以将其值限制在 <code>2</code>和
<code>5</code>之间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">t = tf.constant(((<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>), (<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>)))</span><br><span class="line">t2 = tf.clip_by_value(t, <span class="number">2.5</span>, <span class="number">4.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(sess.run(t2)) </span><br></pre></td></tr></table></figure>
<p>输出结果将是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">((2.5, 2.5, 3.0), (4.0, 4.5, 4.5))</span><br></pre></td></tr></table></figure>
<p>在上述例子中，原始张量 <code>t</code>中的值小于
<code>2.5</code>的被替换为 <code>2.5</code>，大于
<code>4.5</code>的被替换为 <code>4.5</code>。</p>
<p>需要注意的是，<code>clip_value_min</code>需要小于或等于
<code>clip_value_max</code>，以获得正确的结果。如果输入的张量
<code>t</code>的元素类型是整数，而 <code>clip_value_min</code>或
<code>clip_value_max</code>是浮点数，需要先将整数张量转换为浮点数，否则可能会引发类型错误。此外，<code>clip_value_min</code>和
<code>clip_value_max</code>可以是标量张量，也可以是能广播到张量
<code>t</code>形状的张量。</p>
<h3 id="torch.nn.module">torch.nn.Module</h3>
<p><code>torch.nn.Module</code> 是 PyTorch
中所有神经网络模块的基类。它提供了许多用于构建和训练神经网络的基本功能。以下是一些关键点：</p>
<ol type="1">
<li><p><strong>模型定义</strong>：<code>torch.nn.Module</code>
是所有神经网络模块的基类，因此你可以通过继承它来定义自己的神经网络模型。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)  <span class="comment"># 定义一个全连接层</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></li>
<li><p><strong>参数管理</strong>：<code>torch.nn.Module</code>
自动管理模型的所有参数。当你定义一个神经网络层（如
<code>nn.Linear</code>、<code>nn.Conv2d</code>
等）时，这些层会自动添加到模型的参数列表中。例如，在上面的
<code>MyModel</code> 中，<code>self.fc</code>
是一个线性层，它的参数（权重和偏置）会被自动添加到模型中。</p></li>
<li><p><strong>前向传播</strong>：你需要定义一个 <code>forward</code>
方法来指定数据如何通过模型进行前向传播。例如，在上面的
<code>MyModel</code> 中，<code>forward</code> 方法将输入 <code>x</code>
传递给线性层 <code>self.fc</code>。</p></li>
<li><p><strong>训练和评估</strong>：<code>torch.nn.Module</code>
提供了一些方法来管理模型的训练和评估过程，例如
<code>train()</code>、<code>eval()</code>、<code>zero_grad()</code>、<code>step()</code>
等。</p></li>
<li><p><strong>保存和加载</strong>：<code>torch.nn.Module</code> 提供了
<code>save</code> 和 <code>load_state_dict</code>
方法来保存和加载模型的参数。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = MyModel()</span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model.pth&#x27;</span>)</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model.pth&#x27;</span>))</span><br></pre></td></tr></table></figure></li>
<li><p><strong>子模块</strong>：<code>torch.nn.Module</code>
可以包含其他子模块，这些子模块也可以是 <code>torch.nn.Module</code>
的实例。例如，一个复杂的神经网络可能由多个层和子网络组成。</p></li>
</ol>
<p>总的来说，<code>torch.nn.Module</code> 是 PyTorch
中构建和训练神经网络的核心类，它提供了许多方便的功能来管理模型参数、前向传播、训练和评估等。</p>
<h3 id="abc.abc">abc.ABC</h3>
<p>#abc.ABC 是 Python 的抽象基类，用于定义抽象方法。</p>
<p>在 Python 中，<code>abc.ABC</code>类是用于实现抽象基类（Abstract Base
Class）的。</p>
<p>抽象基类主要用于定义一组必须由其子类实现的方法。通过从
<code>abc.ABC</code>类继承，您可以创建一个抽象基类，并在其中定义抽象方法。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> abc</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyAbstractClass</span>(abc.ABC):</span><br><span class="line"><span class="meta">    @abc.abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">abstract_method</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConcreteClass</span>(<span class="title class_ inherited__">MyAbstractClass</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">abstract_method</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Implemented abstract method&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>在上述示例中，<code>MyAbstractClass</code>是一个抽象基类，它定义了一个抽象方法
<code>abstract_method</code>。<code>ConcreteClass</code>是
<code>MyAbstractClass</code>的子类，并且实现了这个抽象方法。</p>
<p>使用抽象基类的好处在于，它可以强制子类遵循一定的接口规范，有助于提高代码的可读性、可维护性和可扩展性。例如，在一个大型项目中，如果多个类都需要实现某些特定的方法，通过定义一个抽象基类，可以确保这些方法在所有相关的子类中都得到正确实现。</p>
<h3 id="property"><code>@property</code></h3>
<p><code>@property</code> 是 Python
的装饰器语法，用于将一个方法转换为只读属性。这意味着你可以像访问属性一样访问这个方法，而不需要调用它。这对于创建只读属性或需要计算属性值的方法非常有用。</p>
<p>下面是一个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Circle</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, radius</span>):</span><br><span class="line">        self._radius = radius</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">radius</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._radius</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">diameter</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._radius * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">c = Circle(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(c.radius)  <span class="comment"># 输出: 5</span></span><br><span class="line"><span class="built_in">print</span>(c.diameter)  <span class="comment"># 输出: 10</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>radius</code> 和 <code>diameter</code>
都是只读属性。<code>radius</code> 属性直接返回 <code>_radius</code>
属性的值，而 <code>diameter</code> 属性计算并返回直径的值。</p>
<p>使用 <code>@property</code>
装饰器的好处是，它允许你保持属性访问的一致性，同时提供额外的功能。例如，你可以使用
<code>@property</code> 装饰器来定义一个计算属性，或者使用
<code>@property</code> 和 <code>@radius.setter</code>
装饰器来定义一个可写的属性。</p>
<h3 id="import-gym">import gym</h3>
<p><code>import gym</code> 通常用于导入 OpenAI Gym 库。</p>
<p>OpenAI Gym
是一个用于开发和比较强化学习算法的工具包。它提供了各种各样的环境，例如经典控制问题（如
CartPole、MountainCar 等）、Atari 游戏等。</p>
<p>通过导入 Gym
库，您可以创建这些环境，并与它们进行交互，以测试和开发强化学习算法。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)</span><br><span class="line">observation = env.reset()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    env.render()</span><br><span class="line">    action = env.action_space.sample()  <span class="comment"># 随机选择一个动作</span></span><br><span class="line">    observation, reward, done, info = env.step(action)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> done:</span><br><span class="line">        observation = env.reset()</span><br><span class="line"></span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure>
<p>在上述代码中，首先创建了 <code>CartPole-v1</code>
环境，然后进行了一系列的交互操作，包括获取初始观察值、采取动作、获取新的观察值和奖励等。</p>
<p>不同的环境具有不同的状态、动作空间和奖励机制，您可以根据具体的问题和需求选择合适的环境进行强化学习算法的开发和实验。</p>
<h3 id="np.clip">np.clip</h3>
<p>“np.clip”通常是在 Python
编程中使用的一个函数。它用于将数组中的值限制在给定的区间范围内，以达到对数值进行裁剪或限制的目的。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz">ALTNT</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/">http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/img/altnt.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5/" title="强化学习相关概念"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">强化学习相关概念</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/22/%E8%B5%84%E6%96%99/%E5%91%BD%E4%BB%A4/" title="命令"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">命令</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/altnt.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ALTNT</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">27</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ALTNT"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB"><span class="toc-number">1.</span> <span class="toc-text">机器学习分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.</span> <span class="toc-text">回归问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BB%BA%E7%AB%8B"><span class="toc-number">1.2.</span> <span class="toc-text">回归模型的建立</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%E7%A1%AE%E5%AE%9A%E7%94%A8%E4%BA%8E%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.2.1.</span> <span class="toc-text">第一步：确定用于回归任务的模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E6%9D%A5%E8%A1%A1%E9%87%8F%E8%BF%99%E4%BA%9B%E5%A4%87%E9%80%89%E5%87%BD%E6%95%B0%E7%9A%84%E5%A5%BD%E5%9D%8F%E7%A8%8B%E5%BA%A6%E7%A1%AE%E5%AE%9A%E5%8F%82%E6%95%B0"><span class="toc-number">1.2.2.</span> <span class="toc-text">第二步：使用训练数据来衡量这些备选函数的好坏程度（确定参数）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%E6%A0%B9%E6%8D%AE%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%80%89%E5%87%BA%E6%8B%9F%E5%90%88%E6%9C%80%E5%A5%BD%E7%9A%84%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%9C%80%E7%BB%88%E7%9A%84%E6%8B%9F%E5%90%88%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.3.</span> <span class="toc-text">第三步：根据训练数据选出拟合最好的函数，作为最终的拟合函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="toc-number">2.1.</span> <span class="toc-text">预测结果分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">2.2.</span> <span class="toc-text">正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%81%97%E7%95%99%E9%97%AE%E9%A2%98"><span class="toc-number">2.3.</span> <span class="toc-text">遗留问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-number">2.4.</span> <span class="toc-text">梯度下降算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%AD%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7tips"><span class="toc-number">2.4.1.</span> <span class="toc-text">梯度下降中常用技巧（Tips）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%80%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">2.4.1.1.</span> <span class="toc-text">一、调整学习率</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#adagrad%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">2.4.1.1.1.</span> <span class="toc-text">AdaGrad优化器</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BA%8C%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8Dsgd"><span class="toc-number">2.4.1.2.</span> <span class="toc-text">二、随机梯度下降（SGD）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%89%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BEfeature-scaling"><span class="toc-number">2.4.1.3.</span> <span class="toc-text">三、特征缩放（Feature Scaling）</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">3.</span> <span class="toc-text">梯度上升和梯度下降的区别是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="toc-number">4.</span> <span class="toc-text">偏差和方差</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E5%AE%9A%E4%B9%89"><span class="toc-number">4.1.</span> <span class="toc-text">数学定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-number">4.2.</span> <span class="toc-text">偏差-方差与模型复杂度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95"><span class="toc-number">4.3.</span> <span class="toc-text">调整方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#k-%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">4.4.</span> <span class="toc-text">K-折交叉验证</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax"><span class="toc-number">5.</span> <span class="toc-text">softmax</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E7%94%A8%E9%80%94"><span class="toc-number">5.1.</span> <span class="toc-text">主要用途:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">6.</span> <span class="toc-text">池化层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">6.1.</span> <span class="toc-text">池化层的作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#max-pooling"><span class="toc-number">6.2.</span> <span class="toc-text">Max pooling:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">7.</span> <span class="toc-text">逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">7.1.</span> <span class="toc-text">1. 什么是逻辑回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFsigmoid%E5%87%BD%E6%95%B0"><span class="toc-number">7.2.</span> <span class="toc-text">2. 什么是Sigmoid函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">7.3.</span> <span class="toc-text">3. 损失函数是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E4%BB%A5%E8%BF%9B%E8%A1%8C%E5%A4%9A%E5%88%86%E7%B1%BB%E5%90%97"><span class="toc-number">7.4.</span> <span class="toc-text">4.可以进行多分类吗？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%9C%89%E4%BB%80%E4%B9%88%E4%BC%98%E7%82%B9"><span class="toc-number">7.5.</span> <span class="toc-text">5.逻辑回归有什么优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%9C%89%E5%93%AA%E4%BA%9B%E5%BA%94%E7%94%A8"><span class="toc-number">7.6.</span> <span class="toc-text">6. 逻辑回归有哪些应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%B8%B8%E7%94%A8%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="toc-number">7.7.</span> <span class="toc-text">7.
逻辑回归常用的优化方法有哪些</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E9%98%B6%E6%96%B9%E6%B3%95"><span class="toc-number">7.7.1.</span> <span class="toc-text">7.1 一阶方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E9%98%B6%E6%96%B9%E6%B3%95%E7%89%9B%E9%A1%BF%E6%B3%95%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95"><span class="toc-number">7.7.2.</span> <span class="toc-text">7.2 二阶方法：牛顿法、拟牛顿法：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E6%96%AF%E7%89%B9%E5%9B%9E%E5%BD%92%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%AF%B9%E7%89%B9%E5%BE%81%E8%BF%9B%E8%A1%8C%E7%A6%BB%E6%95%A3%E5%8C%96"><span class="toc-number">7.8.</span> <span class="toc-text">8.
逻辑斯特回归为什么要对特征进行离散化。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E4%B8%AD%E5%A2%9E%E5%A4%A7l1%E6%AD%A3%E5%88%99%E5%8C%96%E4%BC%9A%E6%98%AF%E4%BB%80%E4%B9%88%E7%BB%93%E6%9E%9C"><span class="toc-number">7.9.</span> <span class="toc-text">9.
逻辑回归的目标函数中增大L1正则化会是什么结果。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.10.</span> <span class="toc-text">10. 代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">8.</span> <span class="toc-text">随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">8.1.</span> <span class="toc-text">1.什么是随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#bagging%E6%80%9D%E6%83%B3"><span class="toc-number">8.1.1.</span> <span class="toc-text">1.1 Bagging思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-1"><span class="toc-number">8.1.2.</span> <span class="toc-text">1.2 随机森林</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%88%86%E7%B1%BB%E6%95%88%E6%9E%9C%E7%9A%84%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0"><span class="toc-number">8.2.</span> <span class="toc-text">2. 随机森林分类效果的影响因素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E6%9C%89%E4%BB%80%E4%B9%88%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">8.3.</span> <span class="toc-text">随机森林有什么优缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E5%80%BC"><span class="toc-number">8.4.</span> <span class="toc-text">4. 随机森林如何处理缺失值？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFoob%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E4%B8%ADoob%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E7%9A%84%E5%AE%83%E6%9C%89%E4%BB%80%E4%B9%88%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">8.5.</span> <span class="toc-text">5.
什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%9A%84%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98"><span class="toc-number">8.6.</span> <span class="toc-text">6. 随机森林的过拟合问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">8.7.</span> <span class="toc-text">7. 代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#import%E5%B7%A5%E5%85%B7%E5%BA%93"><span class="toc-number">8.7.1.</span> <span class="toc-text">0.import工具库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">8.7.2.</span> <span class="toc-text">1.加载数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="toc-number">8.7.3.</span> <span class="toc-text">构建模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gbdtgradient-boosting-decision-tree%E5%85%A8%E5%90%8D%E5%8F%AB%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">9.</span> <span class="toc-text">GBDT(Gradient
Boosting Decision Tree)，全名叫梯度提升决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E4%B8%80%E4%B8%8Bgbdt%E7%AE%97%E6%B3%95%E7%9A%84%E8%BF%87%E7%A8%8B"><span class="toc-number">9.1.</span> <span class="toc-text">1. 解释一下GBDT算法的过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#boosting%E6%80%9D%E6%83%B3"><span class="toc-number">9.1.1.</span> <span class="toc-text">1.1 Boosting思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gbdt%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E5%9B%9E%E4%BA%8B"><span class="toc-number">9.1.2.</span> <span class="toc-text">1.2 GBDT原来是这么回事</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">9.2.</span> <span class="toc-text">2.
梯度提升和梯度下降的区别和联系是什么？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gbdt%E7%9A%84%E4%BC%98%E7%82%B9%E5%92%8C%E5%B1%80%E9%99%90%E6%80%A7%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="toc-number">9.3.</span> <span class="toc-text">3.
GBDT的优点和局限性有哪些？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-number">9.3.1.</span> <span class="toc-text">3.1 优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">9.3.2.</span> <span class="toc-text">3.2 局限性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rf%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E4%B8%8Egbdt%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB"><span class="toc-number">9.4.</span> <span class="toc-text">4.
RF(随机森林)与GBDT之间的区别与联系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">9.5.</span> <span class="toc-text">5. 代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-number">9.5.1.</span> <span class="toc-text">获取训练数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE"><span class="toc-number">9.5.2.</span> <span class="toc-text">获取测试数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gbdt%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B"><span class="toc-number">9.5.3.</span> <span class="toc-text">GBDT模型建立</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#xgboost"><span class="toc-number">10.</span> <span class="toc-text">XGBoost</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFxgboost"><span class="toc-number">10.1.</span> <span class="toc-text">1. 什么是XGBoost</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#xgboost%E6%A0%91%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-number">10.1.1.</span> <span class="toc-text">1.1 XGBoost树的定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E9%A1%B9%E6%A0%91%E7%9A%84%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-number">10.1.2.</span> <span class="toc-text">1.2 正则项：树的复杂度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%91%E8%AF%A5%E6%80%8E%E4%B9%88%E9%95%BF"><span class="toc-number">10.1.3.</span> <span class="toc-text">1.3 树该怎么长</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%81%9C%E6%AD%A2%E6%A0%91%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%94%9F%E6%88%90"><span class="toc-number">10.1.4.</span> <span class="toc-text">1.4 如何停止树的循环生成</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#xgboost%E4%B8%8Egbdt%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C"><span class="toc-number">10.2.</span> <span class="toc-text">2. XGBoost与GBDT有什么不同</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88xgboost%E8%A6%81%E7%94%A8%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80%E4%BC%98%E5%8A%BF%E5%9C%A8%E5%93%AA%E9%87%8C"><span class="toc-number">10.3.</span> <span class="toc-text">3.
为什么XGBoost要用泰勒展开，优势在哪里？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-3"><span class="toc-number">10.4.</span> <span class="toc-text">4. 代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">11.</span> <span class="toc-text">损失函数:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">11.1.</span> <span class="toc-text">常见的损失函数:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0%E8%AE%A1%E7%AE%97"><span class="toc-number">11.2.</span> <span class="toc-text">损失函数的导数计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">12.</span> <span class="toc-text">激活函数:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">12.1.</span> <span class="toc-text">常见的激活函数：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">13.</span> <span class="toc-text">学习率</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A5%E4%B8%8B%E6%98%AF%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95"><span class="toc-number">13.1.</span> <span class="toc-text">以下是一些常见的学习率调整方法：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#optimizer%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">14.</span> <span class="toc-text">optimizer的概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90"><span class="toc-number">15.</span> <span class="toc-text">pytorch代码分析:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.one_hot"><span class="toc-number">15.1.</span> <span class="toc-text">tf.one_hot</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self.critic_net.parameters"><span class="toc-number">15.2.</span> <span class="toc-text">self.critic_net.parameters()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self.optimizer.zero_grad"><span class="toc-number">15.3.</span> <span class="toc-text">self.optimizer.zero_grad()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loss.backwardretain_graphtrue"><span class="toc-number">15.4.</span> <span class="toc-text">loss.backward(retain_graph&#x3D;True)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6%E9%9C%80%E8%A6%81%E5%A4%9A%E6%AC%A1%E4%BD%BF%E7%94%A8%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">15.4.1.</span> <span class="toc-text">如何判断是否需要多次使用计算图？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.reduce_mean"><span class="toc-number">15.5.</span> <span class="toc-text">tf.reduce_mean</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.placeholder"><span class="toc-number">15.6.</span> <span class="toc-text">tf.placeholder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.variable_scope"><span class="toc-number">15.7.</span> <span class="toc-text">tf.variable_scope</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-tf.variable_scope"><span class="toc-number">15.7.1.</span> <span class="toc-text">1. 什么是
tf.variable_scope？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8-tf.variable_scope"><span class="toc-number">15.7.2.</span> <span class="toc-text">2. 为什么使用
tf.variable_scope？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-tf.variable_scope"><span class="toc-number">15.7.3.</span> <span class="toc-text">3. 如何使用
tf.variable_scope？</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-1%E5%88%9B%E5%BB%BA%E5%8F%98%E9%87%8F%E4%BD%9C%E7%94%A8%E5%9F%9F"><span class="toc-number">15.7.3.1.</span> <span class="toc-text">示例 1：创建变量作用域</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-2%E5%A4%8D%E7%94%A8%E5%8F%98%E9%87%8F"><span class="toc-number">15.7.3.2.</span> <span class="toc-text">示例 2：复用变量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#tf.get_variable-%E5%92%8C-tf.variable-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">15.7.3.3.</span> <span class="toc-text">4.
tf.get_variable 和 tf.Variable 的区别</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-3%E4%BD%BF%E7%94%A8-tf.get_variable-%E5%92%8C-tf.variable"><span class="toc-number">15.7.3.4.</span> <span class="toc-text">示例 3：使用
tf.get_variable 和 tf.Variable</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">15.7.4.</span> <span class="toc-text">5. 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.session"><span class="toc-number">15.8.</span> <span class="toc-text">tf.Session</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-tf.session"><span class="toc-number">15.8.1.</span> <span class="toc-text">1. 什么是 tf.Session？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-tf.session"><span class="toc-number">15.8.2.</span> <span class="toc-text">2. 为什么需要
tf.Session？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-tf.session"><span class="toc-number">15.8.3.</span> <span class="toc-text">3. 如何使用 tf.Session？</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-1%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-number">15.8.3.1.</span> <span class="toc-text">示例 1：基本使用</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-2%E4%BD%BF%E7%94%A8-tf.placeholder-%E5%92%8C-feed_dict"><span class="toc-number">15.8.3.2.</span> <span class="toc-text">示例 2：使用
tf.placeholder 和 feed_dict</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-3%E7%AE%A1%E7%90%86%E5%8F%98%E9%87%8F"><span class="toc-number">15.8.3.3.</span> <span class="toc-text">示例 3：管理变量</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tensorflow-2.x-%E4%B8%AD%E7%9A%84%E5%8F%98%E5%8C%96"><span class="toc-number">15.8.4.</span> <span class="toc-text">4. TensorFlow 2.x 中的变化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-number">15.8.5.</span> <span class="toc-text">5. 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.random_normal_initializer"><span class="toc-number">15.9.</span> <span class="toc-text">tf.random_normal_initializer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86"><span class="toc-number">15.9.1.</span> <span class="toc-text">背景知识</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%E8%AF%B4%E6%98%8E"><span class="toc-number">15.9.2.</span> <span class="toc-text">实例说明</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E8%BF%99%E4%B8%AA%E7%A4%BA%E4%BE%8B"><span class="toc-number">15.9.3.</span> <span class="toc-text">解释这个示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.compat.v1.global_variables_initializer"><span class="toc-number">15.10.</span> <span class="toc-text">tf.compat.v1.global_variables_initializer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%9C%E7%94%A8"><span class="toc-number">15.10.1.</span> <span class="toc-text">作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%98%E9%87%8F%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-number">15.10.2.</span> <span class="toc-text">变量初始化的重要性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="toc-number">15.11.</span> <span class="toc-text">使用示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E8%BF%99%E4%B8%AA%E7%A4%BA%E4%BE%8B-1"><span class="toc-number">15.12.</span> <span class="toc-text">解释这个示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensorflow-2.x-%E7%9A%84%E5%8F%98%E5%8C%96"><span class="toc-number">15.13.</span> <span class="toc-text">TensorFlow 2.x 的变化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-2"><span class="toc-number">15.14.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.constant_initializer"><span class="toc-number">15.15.</span> <span class="toc-text">tf.constant_initializer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">15.15.1.</span> <span class="toc-text">使用方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B"><span class="toc-number">15.15.2.</span> <span class="toc-text">示例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E8%BF%99%E4%B8%AA%E7%A4%BA%E4%BE%8B-2"><span class="toc-number">15.15.3.</span> <span class="toc-text">解释这个示例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">15.15.4.</span> <span class="toc-text">应用场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tensorflow-2.x-%E7%9A%84%E5%8F%98%E5%8C%96-1"><span class="toc-number">15.15.5.</span> <span class="toc-text">TensorFlow 2.x 的变化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-3"><span class="toc-number">15.15.6.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.layers.dense"><span class="toc-number">15.16.</span> <span class="toc-text">tf.layers.dense</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8A%E9%9D%A230-%E4%B8%AA%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E5%90%AB%E4%B9%89"><span class="toc-number">15.16.1.</span> <span class="toc-text">上面30 个神经元的含义</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4"><span class="toc-number">15.16.1.1.</span> <span class="toc-text">详细步骤</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E"><span class="toc-number">15.16.1.2.</span> <span class="toc-text">举例说明</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.multiply"><span class="toc-number">15.17.</span> <span class="toc-text">tf.multiply ：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.get_collection"><span class="toc-number">15.18.</span> <span class="toc-text">tf.get_collection</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E5%8E%9F%E5%9E%8B"><span class="toc-number">15.18.1.</span> <span class="toc-text">函数原型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0"><span class="toc-number">15.18.2.</span> <span class="toc-text">参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%94%E5%9B%9E%E5%80%BC"><span class="toc-number">15.18.3.</span> <span class="toc-text">返回值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E-1"><span class="toc-number">15.18.4.</span> <span class="toc-text">举例说明</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81"><span class="toc-number">15.18.4.1.</span> <span class="toc-text">示例代码</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E5%AE%9A%E4%B9%89%E9%9B%86%E5%90%88%E9%94%AE"><span class="toc-number">15.18.5.</span> <span class="toc-text">预定义集合键</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">15.18.6.</span> <span class="toc-text">在实际应用中的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%89%80%E6%9C%89%E5%8F%AF%E8%AE%AD%E7%BB%83%E5%8F%98%E9%87%8F"><span class="toc-number">15.18.6.1.</span> <span class="toc-text">获取所有可训练变量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E7%89%B9%E5%AE%9A%E4%BD%9C%E7%94%A8%E5%9F%9F%E5%86%85%E7%9A%84%E5%8F%98%E9%87%8F"><span class="toc-number">15.18.6.2.</span> <span class="toc-text">获取特定作用域内的变量</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-4"><span class="toc-number">15.18.7.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.stop_gradient"><span class="toc-number">15.19.</span> <span class="toc-text">tf.stop_gradient</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E5%8E%9F%E5%9E%8B-1"><span class="toc-number">15.19.1.</span> <span class="toc-text">函数原型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0-1"><span class="toc-number">15.19.2.</span> <span class="toc-text">参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%94%E5%9B%9E%E5%80%BC-1"><span class="toc-number">15.19.3.</span> <span class="toc-text">返回值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B-1"><span class="toc-number">15.19.4.</span> <span class="toc-text">使用示例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B1%E7%AE%80%E5%8D%95%E7%A4%BA%E4%BE%8B"><span class="toc-number">15.19.4.1.</span> <span class="toc-text">示例1：简单示例</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B2%E5%9C%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">15.19.4.2.</span> <span class="toc-text">示例2：在神经网络中的应用</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-5"><span class="toc-number">15.19.5.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AF%E6%9B%BF%E6%8D%A2%E4%BB%A3%E7%A0%81"><span class="toc-number">15.20.</span> <span class="toc-text">软替换代码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A9%E6%88%91%E4%BB%AC%E9%80%90%E6%AD%A5%E8%A7%A3%E6%9E%90%E8%BF%99%E6%AE%B5%E4%BB%A3%E7%A0%81"><span class="toc-number">15.20.1.</span> <span class="toc-text">让我们逐步解析这段代码：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BD%AF%E6%9B%BF%E6%8D%A2%E7%9A%84%E5%85%B7%E4%BD%93%E4%BD%9C%E7%94%A8"><span class="toc-number">15.20.2.</span> <span class="toc-text">软替换的具体作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-1"><span class="toc-number">15.20.3.</span> <span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A1%AC%E6%9B%BF%E6%8D%A2%E4%BB%A3%E7%A0%81"><span class="toc-number">15.21.</span> <span class="toc-text">硬替换代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.gradients"><span class="toc-number">15.22.</span> <span class="toc-text">tf.gradients</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tf.gradients%E7%9A%84%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA%E4%B9%8B%E9%97%B4%E7%9A%84%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">15.22.1.</span> <span class="toc-text">tf.gradients的输入和输出之间的形状的关系</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3-grad_ys"><span class="toc-number">15.22.2.</span> <span class="toc-text">如何理解 grad_ys</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%8A%A0%E6%9D%83"><span class="toc-number">15.22.2.1.</span> <span class="toc-text">为什么需要加权</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%B8%BA-grad_ys-%E4%B8%AD%E7%9A%84%E6%AF%8F%E4%B8%AA%E5%BC%A0%E9%87%8F%E8%AE%BE%E7%BD%AE%E5%90%88%E9%80%82%E7%9A%84%E6%9D%83%E9%87%8D"><span class="toc-number">15.22.2.2.</span> <span class="toc-text">如何为
grad_ys 中的每个张量设置合适的权重？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#np.hstack"><span class="toc-number">15.23.</span> <span class="toc-text">np.hstack</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#td-error"><span class="toc-number">15.24.</span> <span class="toc-text">td error</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.distributions.normal"><span class="toc-number">15.25.</span> <span class="toc-text">tf.distributions.Normal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.clip_by_value"><span class="toc-number">15.26.</span> <span class="toc-text">tf.clip_by_value</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch.nn.module"><span class="toc-number">15.27.</span> <span class="toc-text">torch.nn.Module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#abc.abc"><span class="toc-number">15.28.</span> <span class="toc-text">abc.ABC</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#property"><span class="toc-number">15.29.</span> <span class="toc-text">@property</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#import-gym"><span class="toc-number">15.30.</span> <span class="toc-text">import gym</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#np.clip"><span class="toc-number">15.31.</span> <span class="toc-text">np.clip</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/21/crop_classification/Climate-adaptiveemergencycropmonitoringin1inaccessibleregionswithsatelliteimagery/" title="Climate-adaptive emergency crop monitoring in inaccessible regions with satellite imagery">Climate-adaptive emergency crop monitoring in inaccessible regions with satellite imagery</a><time datetime="2024-08-21T13:10:08.777Z" title="Created 2024-08-21 21:10:08">2024-08-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/14/%E8%B5%84%E6%96%99/git/" title="git">git</a><time datetime="2024-08-14T06:43:06.646Z" title="Created 2024-08-14 14:43:06">2024-08-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/14/%E8%B5%84%E6%96%99/Docker%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" title="Docker快速入门">Docker快速入门</a><time datetime="2024-08-14T03:51:21.076Z" title="Created 2024-08-14 11:51:21">2024-08-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/14/%E8%B5%84%E6%96%99/markdown/" title="Markdown 特殊符号及数学公式整理">Markdown 特殊符号及数学公式整理</a><time datetime="2024-08-14T03:47:51.740Z" title="Created 2024-08-14 11:47:51">2024-08-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/14/%E8%B5%84%E6%96%99/k8s/" title="k8s">k8s</a><time datetime="2024-08-14T03:43:57.520Z" title="Created 2024-08-14 11:43:57">2024-08-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By ALTNT</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>