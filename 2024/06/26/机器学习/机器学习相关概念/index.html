<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习相关概念 | ALTNT's Hexo Blog</title><meta name="author" content="ALTNT"><meta name="copyright" content="ALTNT"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="机器学习分类根据训练数据是否有标签，可以分为： 监督学习：所有训练数据均具有标签（典型的问题有回归：模型输出的是一个具体数值；分类：模型的输出是某一类别）在监督学习中，常用的模型种类可以分为：线性模型 和 非线性模型。其中，非线性模型应用更加广泛，表达能力也更强，包括深度学习，支持向量机（SVM），决策树，K-NN算法等。 半监督学习：训练数据中，部分具有标签，另一部分没有标签（但是没有标签的数据">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习相关概念">
<meta property="og:url" content="http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/index.html">
<meta property="og:site_name" content="ALTNT&#39;s Hexo Blog">
<meta property="og:description" content="机器学习分类根据训练数据是否有标签，可以分为： 监督学习：所有训练数据均具有标签（典型的问题有回归：模型输出的是一个具体数值；分类：模型的输出是某一类别）在监督学习中，常用的模型种类可以分为：线性模型 和 非线性模型。其中，非线性模型应用更加广泛，表达能力也更强，包括深度学习，支持向量机（SVM），决策树，K-NN算法等。 半监督学习：训练数据中，部分具有标签，另一部分没有标签（但是没有标签的数据">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://blog.705553939.xyz/img/altnt.jpeg">
<meta property="article:published_time" content="2024-06-26T09:09:56.000Z">
<meta property="article:modified_time" content="2024-07-13T16:13:33.388Z">
<meta property="article:author" content="ALTNT">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.705553939.xyz/img/altnt.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习相关概念',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-07-14 00:13:33'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/altnt.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">17</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="ALTNT's Hexo Blog"><span class="site-name">ALTNT's Hexo Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习相关概念</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-06-26T09:09:56.000Z" title="Created 2024-06-26 17:09:56">2024-06-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-07-13T16:13:33.388Z" title="Updated 2024-07-14 00:13:33">2024-07-14</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习相关概念"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="机器学习分类"><a href="#机器学习分类" class="headerlink" title="机器学习分类"></a>机器学习分类</h2><p>根据训练数据是否有标签，可以分为：</p>
<p>监督学习：所有训练数据均具有标签（典型的问题有回归：模型输出的是一个具体数值；分类：模型的输出是某一类别）<br>在监督学习中，常用的模型种类可以分为：线性模型 和 非线性模型。其中，非线性模型应用更加广泛，表达能力也更强，包括深度学习，支持向量机（SVM），决策树，K-NN算法等。</p>
<p>半监督学习：训练数据中，部分具有标签，另一部分没有标签（但是没有标签的数据，对于模型的学习也是有用处的）。<br>迁移学习：使用与当前任务无关的数据（可能有标签，可能没有标签）来促进当前模型的学习。<br>无监督学习：训练数据都没有标签。<br>    无监督学习存在的原因是，现实世界中，为训练数据进行标注成本较高，当训练数据都没有标签时，如果我们想要为数据进行分类，只能根据数据的特征进行划分，比如聚类算法。</p>
<p>强化学习：训练数据没有标签，智能体从环境交互中进行学习，来更新自身的策略，根据最终环境的反馈（获得的奖励）来调整自身行为。</p>
<h3 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h3><pre><code>机器学习笔记的第二篇博客，来介绍机器学习中最基础的回归任务，上一篇博客中有提到回归任务和分类任务的差别在于，回归任务中模型的输出是一个具体的数值， 而分类任务中模型的输出是某一类别。其实，许多问题我们都可以视为回归问题：![alt text](机器学习相关概念/image-4.png)
</code></pre>
<p>例如：根据股票市场的历史数据预测明天的股票走势；自动驾驶中根据传感器获取的信息输出方向盘的转动角度；推荐系统中，输入用户和商品的特征，模型输出一个[0,1]之间的数值，表示购买的可能性。</p>
<h3 id="回归模型的建立"><a href="#回归模型的建立" class="headerlink" title="回归模型的建立"></a>回归模型的建立</h3><p>机器学习模型建立的三个步骤：</p>
<ol>
<li>我们准备许多备选的函数  ***f *** ，构成一个集合，也就是机器学习中的模型（Model）。</li>
<li>使用训练数据来衡量这些备选函数的好坏程度。</li>
<li>根据训练数据选出拟合最好的函数，作为最终的拟合函数。</li>
</ol>
<h4 id="第一步：确定用于回归任务的模型"><a href="#第一步：确定用于回归任务的模型" class="headerlink" title="第一步：确定用于回归任务的模型"></a>第一步：确定用于回归任务的模型</h4><p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-5.png" alt="alt text"></p>
<p>这里选择一元线性函数 <em><strong>y &#x3D; wx + b</strong></em>即线性模型，一元表示我们使用的特征是一个（宝可梦的原CP值）；这样我们就构造了一个函数集合，由于参数<em><strong>w</strong></em>和<em><strong>b</strong></em>的取值是无穷的，所以函数集合中函数的个数是无穷个，接下来在函数集合中选择最好的函数的过程实际上就是为函数确定参数值的过程。</p>
<h4 id="第二步：使用训练数据来衡量这些备选函数的好坏程度（确定参数）"><a href="#第二步：使用训练数据来衡量这些备选函数的好坏程度（确定参数）" class="headerlink" title="第二步：使用训练数据来衡量这些备选函数的好坏程度（确定参数）"></a>第二步：使用训练数据来衡量这些备选函数的好坏程度（确定参数）</h4><p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-6.png" alt="alt text"></p>
<p>接下来我们需要根据训练数据来从备选函数中选择一个效果最好的函数（即确定函数参数部分），这里需要使用损失函数，损失函数的输入是  “用来进行回归任务的函数” 和 “真实标签” ，输出是  进行回归任务的函数的好坏程度（Loss的值越小，认为该函数的效果越好）。如上图所示，我们使用 平方差之和 作为损失函数。</p>
<h4 id="第三步：根据训练数据选出拟合最好的函数，作为最终的拟合函数"><a href="#第三步：根据训练数据选出拟合最好的函数，作为最终的拟合函数" class="headerlink" title="第三步：根据训练数据选出拟合最好的函数，作为最终的拟合函数"></a>第三步：根据训练数据选出拟合最好的函数，作为最终的拟合函数</h4><p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-7.png" alt="alt text"></p>
<p>第三步，我们在训练数据上根据损失函数来评估拟合函数的好坏，找到使得损失函数最小的一组参数，作为我们最终的拟合函数。其中，寻找参数时，使用的方法为 <em><strong>梯度下降法</strong></em> 。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>梯度下降算法是机器学习领域最广为人知、用途最广的优化算法，用来确定模型的参数（包括随机梯度下降SGD，Momentum，Adam等）。梯度下降算法的一个简单介绍如下：</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-8.png" alt="alt text"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720165442054.png" alt="1720165442054"></p>
<p>1、在机器学习中，只要损失函数是可微分（可求导）的，就可以使用梯度下降算法进行参数的求解，那么怎么判断损失函数是否可微？（后面解释）</p>
<p>上面图片展示的是，模型当中只有一个参数（所以直接对该参数求导就可以），如果模型中存在两个及以上的参数，那么就需要分别对每个参数计算偏导数，然后根据参数更新公式进行每个参数的更新，如下图所示:<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-9.png" alt="alt text"></p>
<p>梯度下降算法的原理已经清楚，其实就是沿着损失函数降低的方向更新模型的参数，但是如果损失函数很复杂，比如下面图片所示，<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-10.png" alt="alt text"></p>
<p>我们很可能在更新参数的过程中，走到导数为0的点（第四个红点位置），这时因为不知道更新的方向，就陷入了局部最优点（其实真正的全局最优点还在右边）。</p>
<p>2、不过对于上面回归问题中，损失函数为平方差之和，该损失函数为凸函数，没有局部最优点，只有全局最优点。那么如何判断一个函数是否为凸函数？（后面解释）</p>
<h3 id="预测结果分析"><a href="#预测结果分析" class="headerlink" title="预测结果分析"></a>预测结果分析</h3><p>我们上面的线性模型，经过梯度下降算法，寻得一组最优参数，其结果表现如下：<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-11.png" alt="alt text"><br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-12.png" alt="alt text"></p>
<p>训练数据上面的损失函数值为31.9，测试数据上面损失函数值为35。在实际问题中，我们更加关注的是模型在测试集上面的性能表现，也就是模型的 <em><strong>泛化能力</strong></em> ，线性模型在测试集上面的误差较大，所以如果我们重新设计预测模型，使用更加复杂的模型，会不会得到更好的效果？<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-13.png" alt="alt text"><br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-14.png" alt="alt text"></p>
<p>随着使用更加复杂的二次模型，三次模型，无论是在训练集还是测试集上面，效果都有提升。可是当继续增加模型的复杂度，使用四次模型的时候，虽然在训练集上面的<em><strong>loss</strong></em>更小，但是测试集上面的效果却变糟了。使用五次模型的时候，这一趋势更加明显：<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-15.png" alt="alt text"><br> 虽然复杂的模型对于训练数据的拟合程度会更好，但是很容易出现过拟合的现象（过于严格的去拟合训练数据，当面对新数据的时候没有办法做出准确的预测，即无法泛化到其他数据）。<br> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-16.png" alt="alt text"><br>  在机器学习模型训练过程中，我们要尽量避免过拟合的现象，一方面要选择合适的模型，模型不是越复杂越好，可以通过交叉验证来选择合适的模型；另一方面，<strong>可以通过一些技术手段来帮助我们避免过拟合，比如正则化，early stopping等等。</strong><br>  <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-17.png" alt="alt text"><br> 如上图所示，<strong>蓝色线代表训练集上的损失函数，红色线代表验证集的损失函数</strong>，当训练进行到中间垂直的线段时，模型应该是最优的；如果继续训练，就会造成过拟合现象。</p>
<h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-18.png" alt="alt text"></p>
<p>简单来说，正则化是一种为了减小<strong>测试误差</strong>的行为(有时候会增加训练误差)。我们在构造机器学习模型时，最终目的是让模型在面对新数据的时候，可以有很好的表现。当使用比较复杂的模型比如神经网络，去拟合数据时，很容易出现过拟合现象(训练集表现很好，测试集表现较差)，这会导致模型的泛化能力下降，这时候，就需要使用正则化，降低模型的复杂度。</p>
<p><strong>具体而言，正则化就是在损失函数后面增加一项惩罚项（对某些参数进行限制），使得我们的模型更加平滑。以上图为例，我们在损失函数后面增加一项关于参数w的正则项，限制参数w不要过大，这样模型会有更好的泛化能力。因为，这样对于测试数据中存在的噪声，会不那么敏感，即噪声对于预测的结果影响会降低。</strong><br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720515070068.png" alt="1720515070068"></p>
<p>加入正则化技术之后，训练集和测试集上面的误差如下：<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-19.png" alt="alt text"></p>
<p>和我们的预期分析是一致的，加入正则化之后，参数<img src="https://private.codecogs.com/gif.latex?%5Clambda" alt="\lambda"></p>
<p>越大，表示我们越关注模型的平滑程度（也就是模型的泛化能力），相对于训练误差考虑较少，所以训练集上面的loss增大，测试集上面的loss降低。但是，参数<img src="https://private.codecogs.com/gif.latex?%5Clambda" alt="\lambda"></p>
<p>不是越大越好，我们希望得到一个比较平滑的函数，但是不能过于平滑（会丧失其预测能力）。</p>
<p><strong>机器学习中的正则化概念</strong></p>
<p>在机器学习中，正则化（Regularization）是一种用于防止过拟合（Overfitting）的技术。</p>
<p>过拟合是指模型在训练数据上表现得非常好，但在新的、未见过的数据上表现不佳。这通常是因为模型过于复杂，学习到了训练数据中的噪声和特定的细节，而不是一般性的模式。</p>
<p>正则化通过在损失函数中添加一个惩罚项来限制模型的复杂度。常见的正则化方法有 L1 正则化和 L2 正则化。</p>
<p><strong>L1 正则化</strong>：也称为 Lasso 正则化，它在损失函数中添加的惩罚项是模型参数的绝对值之和。L1 正则化具有特征选择的效果，因为它可能会将一些不重要的特征对应的参数压缩至零。</p>
<p>例如，在线性回归中，假设模型的预测函数为 <code>y = w1 * x1 + w2 * x2 +... + wn * xn + b</code> ，L1 正则化项就是 <code>λ * |w1| + λ * |w2| +... + λ * |wn|</code> ，其中 <code>λ</code> 是正则化参数，控制正则化的强度。</p>
<p><strong>L2 正则化</strong>：也称为 Ridge 正则化，它在损失函数中添加的惩罚项是模型参数的平方和。L2 正则化会使模型的参数值趋向于较小的值，但不太会将参数压缩至零。</p>
<p>同样在线性回归中，L2 正则化项就是 <code>λ * (w1^2 + w2^2 +... + wn^2)</code> 。</p>
<p><strong>正则化的作用</strong>：</p>
<ol>
<li>控制模型复杂度：通过限制模型的参数大小，防止模型过于复杂。</li>
<li>提高模型泛化能力：使模型能够更好地应对新的数据，减少过拟合的风险。</li>
</ol>
<p><strong>举例说明</strong>：<br>假设我们正在训练一个神经网络来识别图像中的猫和狗。如果没有正则化，模型可能会过度学习训练数据中的细微特征，比如图片中的背景颜色或微小的噪声，导致在新的图像上识别准确率下降。</p>
<p>当我们应用 L2 正则化时，模型的参数会受到一定的约束，不会变得过大。这可能会导致模型在训练数据上的准确率稍微降低，但在测试数据上的表现会更好，因为它学习到了更通用的特征，而不是过度依赖于特定的训练样本。</p>
<p>总之，正则化是机器学习中非常重要的技术，有助于提高模型的性能和稳定性。</p>
<h3 id="遗留问题"><a href="#遗留问题" class="headerlink" title="遗留问题"></a>遗留问题</h3><p>1、在机器学习中，只要损失函数是可微的，就可以使用梯度下降算法进行参数的求解，那么怎么判断损失函数是否可微？（后面解释）</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720166051610.png" alt="1720166051610"></p>
<p>2、不过对于上面回归问题中，损失函数为平方差之和，该损失函数为凸函数，没有局部最优点，只有全局最优点。那么如何判断一个函数是否为凸函数？（后面解释）</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720166067204.png" alt="1720166067204"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-20.png" alt="alt text"></p>
<p>判断一个矩阵是不是半正定矩阵，方法之一是判断该矩阵的所有主子式是不是非负；对于上面的海森矩阵，其所有主子式为：</p>
<p><img src="https://private.codecogs.com/gif.latex?2x%5E%7B2%7D" alt="2x^{2}">，2, 0，均为非负，所以该海森矩阵为半正定矩阵。所以该损失函数为凸函数，只有全局最优值。</p>
<p><strong>半正定矩阵</strong></p>
<p>半正定矩阵是矩阵理论中的一个重要概念。</p>
<p>一个实对称矩阵 <code>A</code> 被称为半正定矩阵，如果对于任意的非零实向量 <code>x</code> ，都有 <code>x^T Ax ≥ 0</code> 。</p>
<p> <strong>性质</strong> ：</p>
<ol>
<li>半正定矩阵的所有特征值都是非负的。</li>
<li>半正定矩阵的主子式都非负。</li>
<li>半正定矩阵与另一个半正定矩阵的和仍是半正定矩阵。</li>
</ol>
<p>在实际的机器学习和优化问题中，Hessian 矩阵具有重要的地位。</p>
<p><strong>计算方面</strong>：<br>计算 Hessian 矩阵可能是计算密集型的，特别是对于具有大量参数的模型。在深度学习中，直接计算完整的 Hessian 矩阵通常是不现实的。然而，可以使用近似方法或针对特定结构的模型进行简化计算。例如，对于一些具有简单结构的神经网络，可能通过一些技巧来估计 Hessian 矩阵的部分元素。</p>
<p><strong>应用方面</strong>：</p>
<ol>
<li>优化算法：如牛顿法及其变体，利用 Hessian 矩阵来确定搜索方向和步长。相比于梯度下降法只依赖一阶导数（梯度），牛顿法考虑了二阶导数信息，能够在一些情况下更快地收敛到最优解。<ul>
<li>举例来说，在求解一个二次函数的最小值时，牛顿法通过一次迭代就可以直接到达最小值点，因为它准确地利用了 Hessian 矩阵的信息。</li>
</ul>
</li>
<li>模型分析：帮助理解模型的性质和行为。通过分析 Hessian 矩阵的特征值和特征向量，可以了解模型在不同方向上的敏感度和曲率，从而洞察模型的稳定性和鲁棒性。<ul>
<li>例如，在图像分类任务中，如果 Hessian 矩阵的某些特征值很大，说明模型在对应的特征方向上变化剧烈，可能对输入的微小变化非常敏感。</li>
</ul>
</li>
<li>正则化：可以用于设计一些基于二阶信息的正则化方法，以防止过拟合。<ul>
<li>比如，通过对 Hessian 矩阵进行某种变换或约束，使得模型的复杂度得到控制。</li>
</ul>
</li>
</ol>
<p>总之，尽管在实际中直接处理 Hessian 矩阵存在困难，但通过巧妙的近似和应用，它仍然为解决机器学习和优化问题提供了有价值的见解和工具。</p>
<h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><p>梯度下降算法是机器学习领域最广为人知、用途最广的优化算法，用来确定模型的参数（包括随机梯度下降SGD，Momentum，Adam等）。首先回顾一下梯度下降的计算过程：</p>
<h4 id="梯度下降中常用技巧（Tips）"><a href="#梯度下降中常用技巧（Tips）" class="headerlink" title="梯度下降中常用技巧（Tips）"></a>梯度下降中常用技巧（Tips）</h4><h5 id="一、调整学习率"><a href="#一、调整学习率" class="headerlink" title="一、调整学习率"></a>一、调整学习率</h5><p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-28.png" alt="alt text"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720543595377.png" alt="1720543595377"></p>
<p>所以，我们可以根据Loss曲线的变化情况，对我们的学习率进行一个合理的调整。可以绘制上图右边部分所示的曲线图，横轴代表参数更新次数，纵轴代表Loss值，学习率的大小分为四种情况：</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720543613512.png" alt="1720543613512"></p>
<p>在学习率的设置过程中，常见的做法是，进行 <em><strong>学习率的衰减</strong></em> 。</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-29.png" alt="alt text"></p>
<p>在模型训练初期，距离全局最优点较远，可以设置一个相对大一些的学习率，随着训练的进行，距离全局最优点的距离越来越小，此时应该减小学习率；所以让学习率随着时间或者更新的次数进行衰减。</p>
<p>除了学习率的衰减之外，另外一个做法是：为不同的参数设置不同的学习率。也就是接下来要介绍的一种优化算法AdaGrad。</p>
<h6 id="AdaGrad优化器"><a href="#AdaGrad优化器" class="headerlink" title="AdaGrad优化器"></a>AdaGrad优化器</h6><p>AdaGrad（Adaptive Gradient Algorithm）是一种自适应学习率的方法，用于优化机器学习模型，特别是在处理稀疏数据和高维数据时表现出色。AdaGrad的主要特点是它为每个参数维护一个单独的学习率，并根据之前的梯度信息调整这些学习率。</p>
<p>AdaGrad根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题,即为每个参数设置不同的学习率。具体来说，对于每个参数 <strong>θ</strong>i，AdaGrad计算其所有历史梯度的平方和，并用这个和来缩放当前的梯度，从而得到更新步长。</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-30.png" alt="alt text"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720543759855.png" alt="1720543759855"><br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-31.png" alt="alt text"></p>
<p>这里需要注意的一点为：<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-32.png" alt="alt text"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-33.png" alt="alt text"><br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-34.png" alt="alt text"><br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-35.png" alt="alt text"><br>而在AdaGrad中，正是这一思想的体现，不过为了减少计算量，增加运算速度，AdaGrad中使用过去一阶偏导数的均方根作为分母项（二阶偏导数）的近似。<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-36.png" alt="alt text"><br>过去一阶偏导数的均方根可以在一定程度上反应二阶偏导的大小情况。如上图所示，二阶偏导小的函数，其采样的一阶偏导的值也相对较小。</p>
<p><strong>优势</strong></p>
<ol>
<li><strong>自适应学习率</strong> ：不同参数有不同的学习率，能够更好地处理稀疏数据，使得更新幅度较大的参数的学习率更小，而更新幅度较小的参数的学习率更大。</li>
<li><strong>简化调参</strong> ：由于学习率是自适应的，通常不需要频繁调整学习率超参数。</li>
</ol>
<p><strong>局限性</strong></p>
<ol>
<li><strong>学习率过小</strong> ：随着时间的推移，累积梯度平方和不断增大，导致学习率逐渐缩小，可能会导致算法在后期学习变得非常缓慢。</li>
<li><strong>不适用于所有问题</strong> ：虽然AdaGrad在处理稀疏数据上有优势，但对于某些问题，其性能可能不如其他优化算法，如RMSprop或Adam。</li>
</ol>
<p><strong>典型应用</strong></p>
<p>AdaGrad常用于处理自然语言处理中的词嵌入、推荐系统中的用户行为数据等高维、稀疏数据场景。</p>
<p>总结来说，AdaGrad通过自适应地调整每个参数的学习率，在处理稀疏数据和高维数据时提供了显著的优势，但其逐渐减小的学习率可能在某些情况下限制其性能。</p>
<h5 id="二、随机梯度下降（SGD）"><a href="#二、随机梯度下降（SGD）" class="headerlink" title="二、随机梯度下降（SGD）"></a>二、随机梯度下降（SGD）</h5><p>随机梯度下降（SGD, Stochastic Gradient Descent）是一种用于优化机器学习模型参数的算法，特别适用于大规模数据集的训练。SGD与传统的批量梯度下降（Batch Gradient Descent）不同，它在每次迭代中仅使用一个样本或一个小批量的样本（mini-batch）来计算梯度和更新参数。这种方法具有较快的更新速度和更好的内存效率，特别是在处理大数据集时。</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720599100421.png" alt="1720599100421"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-37.png" alt="alt text"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-38.png" alt="alt text"><br>之前梯度下降算法中，Loss函数是对所有训练样本的Loss之和，而在随机梯度下降中，每次只采样一个样本，根据这一个样本进行一次梯度下降，所以随机梯度下降算法更新参数的过程更快。</p>
<p>随机梯度下降（SGD, Stochastic Gradient Descent）是一种用于优化机器学习模型参数的算法，特别适用于大规模数据集的训练。SGD与传统的批量梯度下降（Batch Gradient Descent）不同，它在每次迭代中仅使用一个样本或一个小批量的样本（mini-batch）来计算梯度和更新参数。这种方法具有较快的更新速度和更好的内存效率，特别是在处理大数据集时。</p>
<p><strong>优势</strong></p>
<ol>
<li><strong>快速收敛</strong>：由于每次迭代只使用一个样本或小批量样本，更新频繁，收敛速度快。</li>
<li><strong>内存效率</strong>：每次只需要加载一个样本或小批量样本，内存占用低，适合处理大规模数据集。</li>
<li><strong>逃离局部最优</strong>：由于引入了随机性，SGD有助于跳出局部最优解，更容易找到全局最优解。</li>
</ol>
<p><strong>局限性</strong></p>
<ol>
<li><strong>收敛波动</strong>：由于每次更新基于单个样本或小批量样本，导致参数更新不稳定，损失函数可能会剧烈波动。</li>
<li><strong>调参困难</strong>：学习率的选择对SGD的性能影响很大，通常需要进行超参数调优。</li>
</ol>
<p><strong>改进方法</strong></p>
<p>为了解决SGD的一些局限性，提出了多种改进算法，如：</p>
<ol>
<li><strong>Mini-batch SGD</strong>：在每次迭代中使用一个小批量样本，而不是单个样本，平衡了收敛速度和稳定性。</li>
<li><strong>动量（Momentum）</strong>：在参数更新时引入动量项，利用之前梯度的指数加权平均来加速收敛。</li>
<li><strong>RMSprop</strong>：自适应调整学习率，缓解学习率逐渐减小的问题。</li>
<li><strong>Adam</strong>：结合了动量和RMSprop的优点，自适应地调整学习率。</li>
</ol>
<p><strong>应用场景</strong></p>
<p>SGD广泛应用于深度学习和机器学习的各种模型训练中，包括神经网络、线性回归、逻辑回归等。它特别适用于大规模数据集和在线学习场景。</p>
<p>总结来说，SGD通过随机选择样本来进行参数更新，提供了快速且内存高效的优化方法，但其波动性和学习率调优是需要注意的问题。改进的变种算法如Mini-batch SGD、动量、RMSprop和Adam在实践中被广泛采用，以提高SGD的性能和稳定性。</p>
<h5 id="三、特征缩放（Feature-Scaling）"><a href="#三、特征缩放（Feature-Scaling）" class="headerlink" title="三、特征缩放（Feature Scaling）"></a>三、特征缩放（Feature Scaling）</h5><p>特征缩放是用来标准化数据特征的范围，减少特征中特异值的影响。</p>
<p>例如：<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-39.png" alt="alt text"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720544009279.png" alt="1720544009279"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720544026157.png" alt="1720544026157"></p>
<h2 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h2><p>这篇博客介绍机器学习中误差（error）的来源，知道我们的模型中产生的误差来自于哪一部分，才能更好地进行模型的调整。一般来说，误差的来源有两部分：偏差（bias）和方差（variance）。偏差和方差——用来衡量模型泛化能力的工具，所以我的理解是在测试集上面根据偏差和方差来对模型进行一个评估。</p>
<p>回顾之前回归问题中的例子，简单模型对于数据的拟合能力比较差，在训练集和测试集上面效果均不好；但同时不是越复杂的模型越好，因为有可能产生过拟合的现象，所以需要选择合适的模型。偏差-方差分析可以帮我们诊断模型中存在的问题（过于复杂或者过于简单）。</p>
<p><strong>偏差就是：预测输出的期望值 - 真实值，（描述模型的拟合能力）</strong></p>
<p><strong>方差就是：（每个模型实例的预测输出 - 模型预测输出的期望值）^ 2  （描述模型的稳定性，即受数据扰动的影响程度）</strong></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-21.png" alt="alt text"></p>
<p>还是以宝可梦进化之后的CP值预测为例，如果我们有一些不同的训练数据（也就是李宏毅老师PPT中所说从若干个平行世界中收集的不同的宝可梦），</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-22.png" alt="alt text"></p>
<p>实质上是指有几个不同的训练集（TrainData_1，TrainData_2，TrainData_3），模型分别在不同的训练集上面训练，然后在同样的测试集（TestData）上面测试。对于不同的训练集，我们会得到一个模型的实例，比如有一次模型和五次模型，训练结果：(这里，“模型”表示具体的模型类别（比如一次模型，二次模型）；“模型实例”表示一个模型在不同训练集上面训练得到的最终模型，有几个训练集就会有几个模型实例。)<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-23.png" alt="alt text"><br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-24.png" alt="alt text"></p>
<p>图片中，红色线表示在100个不同的训练集上面得到的模型的实例，蓝色线表示模型的预测输出的期望，黑色线是真实值。</p>
<p>可以看出，一次模型的偏差较大，方差较小；五次模型的偏差较小，方差较大。</p>
<h3 id="数学定义"><a href="#数学定义" class="headerlink" title="数学定义"></a>数学定义</h3><p>上面是偏差和方差一个比较直观的理解，接下来给出数学形式的定义：</p>
<p> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720518028206.png" alt="1720518028206"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720518369317.png" alt="1720518369317"></p>
<h3 id="偏差-方差与模型复杂度"><a href="#偏差-方差与模型复杂度" class="headerlink" title="偏差-方差与模型复杂度"></a>偏差-方差与模型复杂度</h3><p>偏差和方差的几种情况：<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-25.png" alt="alt text"></p>
<p>偏差小，方差小：追求的目标，理想的模型。<br>偏差小，方差大：模型比较复杂，在训练集上面过拟合，导致在测试集上面泛化效果不好。<br>偏差大，方差小：模型比较简单，拟合能力较差，在训练集上面欠拟合，导致在测试集上面泛化效果不好。<br>偏差大，方差大：最糟的情况，模型需要重新进行设计，不适合于现有数据集。</p>
<p>很直观的一个解释，因为偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力，所以偏差大的模型拟合能力差，模型简单，容易欠拟合；方差度量数据扰动所造成的影响（在不同的训练集上面训练得到的模型在测试集上面效果表现相差很大），说明模型过于拟合训练集，模型复杂。<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-26.png" alt="alt text"></p>
<h3 id="调整方法"><a href="#调整方法" class="headerlink" title="调整方法"></a>调整方法</h3><p>综上，根据模型在测试集上的表现，可以得出结论：</p>
<blockquote>
<p>偏差大，方差小：模型欠拟合</p>
<p>偏差小，方差大：模型过拟合</p>
</blockquote>
<p>对于偏差大（欠拟合）的情况，常用的解决方法：</p>
<p>• 重新设计模型，使用更复杂的模型结构</p>
<p>• 输入中使用更多的特征</p>
<p>对于方差大（过拟合）的情况，常用的解决方法：</p>
<p>• 参数正则化（减小模型的复杂程度）</p>
<p>• 使用更多的训练数据</p>
<h3 id="K-折交叉验证"><a href="#K-折交叉验证" class="headerlink" title="K-折交叉验证"></a>K-折交叉验证</h3><p>模型的设计选择需要在偏差和方差之间进行平衡，在选择合适的模型（比如一次模型还是二次模型）时，常用的方法是进行K-折交叉验证。将训练集的数据等分成K份，每次使用（K-1）份数据进行训练，余下的1份数据进行验证，进行K次，保证每份数据均做过验证集，统计K次验证集上面的loss，取loss均值最小的模型作为使用的模型。</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-27.png" alt="alt text"></p>
<h2 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h2><h3 id="主要用途"><a href="#主要用途" class="headerlink" title="主要用途:"></a>主要用途:</h3><p><strong>softmax是深度学习任务中常用于计算最终输出类别的函数</strong>。</p>
<p>Softmax 函数主要用于多分类问题中，将多个神经元的输出值转换为概率分布。</p>
<p>Softmax 函数在很多机器学习任务中都有广泛的应用，比如图像分类、文本分类等，它有助于将模型的输出转化为可解释的类别概率。</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719916089305.png" alt="1719916089305"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719916111218.png" alt="1719916111218"></p>
<ul>
<li>softmax和我们普通意义上的max函数不同，每一个元素都有一个概率，而不是其中一个元素为1，其余为0。它的含义是对于输入向量，有多大的概率去选择元素1、元素2、元素3等，主要的目的是使得概率计算过程可导。</li>
</ul>
<p>以下是对 Softmax 函数的一些关键理解点：</p>
<ol>
<li>输出值在 0 到 1 之间：Softmax 函数确保每个输出值都在 0 和 1 之间。</li>
<li>输出值总和为 1：所有输出值的总和为 1，这使得它们可以被解释为概率。</li>
<li>强调相对大小：Softmax 函数会放大输入值之间的差异。较大的输入值在经过 Softmax 计算后会得到更大的概率值，较小的输入值则会得到较小的概率值。</li>
</ol>
<p>例如，假设有一个神经网络的输出为 <code>[1, 2, 0]</code>，经过 Softmax 函数计算后，得到的概率分布可能是 <code>[0.269, 0.731, 0.0]</code>。这意味着模型预测第二个类别是最有可能的，第一个类别有一定的可能性，而第三个类别几乎不可能。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数:"></a>损失函数:</h2><p>在机器学习中，损失函数（Loss Function）是用于衡量模型预测结果与真实结果之间差异的函数。</p>
<p>损失函数的主要作用是评估模型在给定数据上的性能表现。它为模型的优化提供了一个明确的目标，通过最小化损失函数的值，来调整模型的参数，使得模型的预测结果越来越接近真实结果。</p>
<p>不同的机器学习任务通常会使用不同的损失函数。</p>
<h3 id="常见的损失函数"><a href="#常见的损失函数" class="headerlink" title="常见的损失函数:"></a>常见的损失函数:</h3><ol>
<li><p>回归问题：</p>
<ul>
<li>均方误差（Mean Squared Error，MSE）：计算预测值与真实值之差的平方的平均值，即 $\frac{1}{n}\sum_{i&#x3D;1}^{n}(y_i - \hat{y_i})^2$，其中 $y_i$ 是真实值，$\hat{y_i}$ 是预测值，$n$ 是样本数量。</li>
<li>平均绝对误差（Mean Absolute Error，MAE）：计算预测值与真实值之差的绝对值的平均值，即 $\frac{1}{n}\sum_{i&#x3D;1}^{n}|y_i - \hat{y_i}|$ 。</li>
</ul>
</li>
<li><p>分类问题：</p>
<ul>
<li>二分类交叉熵损失（Binary Cross Entropy Loss）：常用于二分类问题，如逻辑回归。对于单个样本，其公式为 $- [y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]$，其中 $y$ 是真实标签（0 或 1），$\hat{y}$ 是预测的概率。</li>
<li>多分类交叉熵损失（Categorical Cross Entropy Loss）：用于多分类问题，公式为 $-\sum_{i&#x3D;1}^{C} y_i \log(\hat{y_i})$，其中 $C$ 是类别数量，$y_i$ 是第 $i$ 类的真实标签（如果样本属于该类为 1，否则为 0），$\hat{y_i}$ 是模型预测样本属于第 $i$ 类的概率。</li>
</ul>
</li>
</ol>
<p>选择合适的损失函数对于模型的训练和性能至关重要。它会影响模型的学习速度、收敛性以及最终的泛化能力。</p>
<p>以下为您介绍几种常见的损失函数：</p>
<ol>
<li><p><strong>均方误差（Mean Squared Error，MSE）</strong>：</p>
<ul>
<li>公式：$MSE &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n}(y_i - \hat{y_i})^2$</li>
<li>其中，$y_i$ 是真实值，$\hat{y_i}$ 是预测值，$n$ 是样本数量。</li>
<li>常用于回归问题，对较大的误差给予更高的惩罚。</li>
<li>例如，预测房价时，如果预测值与真实房价相差较大，MSE 会较大。</li>
</ul>
</li>
<li><p><strong>平均绝对误差（Mean Absolute Error，MAE）</strong>：</p>
<ul>
<li>公式：$MAE &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n}|y_i - \hat{y_i}|$</li>
<li>同样常用于回归问题，相比 MSE 对异常值更鲁棒。</li>
<li>比如，在预测股票价格时，MAE 可能更能承受个别极端价格波动的影响。</li>
</ul>
</li>
<li><p><strong>交叉熵损失（Cross Entropy Loss）</strong>：</p>
<ul>
<li>二分类交叉熵：$L &#x3D; - [y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]$ ，其中 $y$ 是真实标签（0 或 1），$\hat{y}$ 是预测的概率。</li>
<li>多分类交叉熵：$L &#x3D; -\sum_{i&#x3D;1}^{C} y_i \log(\hat{y_i})$ ，其中 $C$ 是类别数量，$y_i$ 是第 $i$ 类的真实标签（如果样本属于该类为 1，否则为 0），$\hat{y_i}$ 是模型预测样本属于第 $i$ 类的概率。</li>
<li>广泛应用于分类问题，衡量预测概率分布与真实分布之间的差异。</li>
</ul>
<p>交叉熵损失函数（Cross-Entropy Loss）主要用于衡量模型预测的概率分布与真实的概率分布之间的差异，它在机器学习，特别是深度学习的分类问题中被广泛使用。</p>
<p>其作用主要体现在以下方面：</p>
<ul>
<li><strong>评估模型性能</strong>：通过计算预测分布和真实分布之间的差距，来确定模型在分类任务中的表现优劣。交叉熵越小，表示两个概率分布越接近，说明模型的预测结果越接近真实标签，模型的性能也就越好。</li>
<li><strong>指导模型优化</strong>：在模型训练过程中，交叉熵损失函数的值被用于反向传播，以调整模型的参数，使得损失不断减小，从而使模型的预测逐渐逼近真实标签。</li>
<li><strong>处理多分类问题</strong>：适用于多类别分类任务，可以方便地处理具有多个类别的情况。对于每个样本，模型输出每个类别的预测概率，而交叉熵损失函数会综合考虑所有类别的预测概率和真实标签来计算损失。</li>
</ul>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720089363560.png" alt="1720089363560"></p>
<p>例如，在一个图像分类任务中，模型需要判断图片属于多个类别中的哪一个。模型的输出是每个类别的预测概率，而真实标签则是图片实际所属的类别。通过计算交叉熵损失，可以衡量模型的预测结果与真实类别之间的差异，并利用这个差异来调整模型的参数，以提高模型的分类准确性。</p>
<p>相比其他一些损失函数，交叉熵损失函数具有一些优点，例如易于理解和计算，对噪声数据具有一定的鲁棒性等。然而，它也存在一些缺点，比如对类别不平衡的数据可能较为敏感，在类别不平衡的数据集上，可能会过于关注多数类别而导致模型性能下降；并且它对输出概率分布的平滑性要求较高，如果输出概率分布过于离散，可能会导致损失值较大。</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720089568706.png" alt="1720089568706"></p>
</li>
<li><p><strong>Hinge 损失</strong>：</p>
<ul>
<li>常用于支持向量机（SVM）中，特别是在二分类问题中。</li>
<li>对于二分类，公式为：$L &#x3D; \max(0, 1 - y \cdot \hat{y})$ ，其中 $y$ 是真实标签（1 或 -1），$\hat{y}$ 是预测值。</li>
</ul>
</li>
<li><p><strong>KL 散度（Kullback-Leibler Divergence）</strong>：</p>
<ul>
<li>用于衡量两个概率分布之间的差异。</li>
<li>公式：$KL(P || Q) &#x3D; \sum_{x} P(x) \log \frac{P(x)}{Q(x)}$</li>
</ul>
</li>
</ol>
<p>这些损失函数在不同的机器学习任务和场景中各有优缺点，选择合适的损失函数对于模型的性能和训练效果至关重要。</p>
<h3 id="损失函数的导数计算"><a href="#损失函数的导数计算" class="headerlink" title="损失函数的导数计算"></a>损失函数的导数计算</h3><p>以下是几种常见损失函数及其导数的计算方法：</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720880004976.png" alt="1720880004976"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720880028751.png" alt="1720880028751"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720880055767.png" alt="1720880055767"></p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数:"></a>激活函数:</h2><p>在神经网络中，激活函数是一种对神经元的输入进行非线性变换的函数。</p>
<p>激活函数的主要作用包括：</p>
<ol>
<li>引入非线性：如果没有激活函数，神经网络仅仅是对输入进行线性组合，其表达能力非常有限，无法处理复杂的非线性问题。通过引入非线性的激活函数，可以使神经网络能够拟合各种复杂的函数和模式。</li>
<li>控制神经元的输出范围：不同的激活函数会将输入映射到不同的输出范围，例如 <code>Sigmoid</code> 函数将输出限制在 <code>(0, 1)</code> 之间，<code>Tanh</code> 函数将输出限制在 <code>(-1, 1)</code> 之间。</li>
<li>增加网络的稀疏性：某些激活函数，如 <code>ReLU</code> （Rectified Linear Unit），当输入为负数时输出为 0，这有助于在网络中引入稀疏性，减少计算量，并可能有助于防止过拟合。</li>
</ol>
<p>常见的激活函数有：</p>
<ol>
<li><p><code>Sigmoid</code> 函数：<code>f(x) = 1 / (1 + e^(-x))</code>，输出范围在 <code>(0, 1)</code> 之间，常用于二分类问题的输出层。<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image.png" alt="alt text"></p>
<p>可以看出，sigmoid函数连续，光滑，严格单调，以(0,0.5)中心对称，是一个非常良好的阈值函数。</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719936221442.png" alt="1719936221442"></p>
</li>
<li><p><code>Tanh</code> 函数：<code>f(x) = (e^x - e^(-x)) / (e^x + e^(-x))</code>，输出范围在 <code>(-1, 1)</code> 之间。<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-1.png" alt="alt text"></p>
</li>
<li><p><code>ReLU</code> 函数：<code>f(x) = max(0, x)</code>，计算简单，在很多深度神经网络中广泛使用。<br>线性整流函数，又称修正线性单元ReLU，是一种人工神经网络中常用的激活函数，通常指代以斜坡函数及其变种为代表的非线性函数。</p>
<p>线性整流函数（ReLU函数）的特点：</p>
<p>当输入为正时，不存在梯度饱和问题。<br>计算速度快得多。ReLU 函数中只存在线性关系，因此它的计算速度比Sigmoid函数和tanh函数更快。<br>Dead ReLU问题。当输入为负时，ReLU完全失效，在正向传播过程中，这不是问题。有些区域很敏感，有些则不敏感。但是在反向传播过程中，如果输入负数，则梯度将完全为零，Sigmoid函数和tanh函数也具有相同的问题<br>ReLU函数的输出为0或正数，这意味着ReLU函数不是以0为中心的函数。<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-2.png" alt="alt text"></p>
</li>
<li><p><code>Leaky ReLU</code> 函数：<code>f(x) = max(0.01x, x)</code>，是 <code>ReLU</code> 的改进版，解决了 <code>ReLU</code> 中神经元可能“死亡”的问题。</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719937549074.png" alt="1719937549074"></p>
</li>
</ol>
<p>Leaky ReLU函数的特点：</p>
<p>Leaky ReLU函数通过把x xx的非常小的线性分量给予负输入0.01 x 0.01x0.01x来调整负值的零梯度问题。<br>Leaky有助于扩大ReLU函数的范围，通常α \alphaα的值为0.01左右。<br>Leaky ReLU的函数范围是负无穷到正无穷。<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-3.png" alt="alt text"></p>
<p>下面是一个使用 <code>Sigmoid</code> 激活函数的简单示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line">x = np.array([-<span class="number">2</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(sigmoid(x))</span><br></pre></td></tr></table></figure>

<p>激活函数的选择对神经网络的性能有很大影响，需要根据具体问题和网络结构进行合适的选择。</p>
<h3 id="常见的激活函数："><a href="#常见的激活函数：" class="headerlink" title="常见的激活函数："></a>常见的激活函数：</h3><ol>
<li><p><strong>Sigmoid 函数</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = 1 / (1 + e^(-x))</code></li>
<li>特点：将输入值压缩到 0 到 1 之间，具有平滑的曲线。但在输入值较大或较小时，梯度接近 0，可能导致梯度消失问题。<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image.png" alt="alt text"></li>
</ul>
</li>
<li><p><strong>Tanh 函数</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = (e^x - e^(-x)) / (e^x + e^(-x))</code></li>
<li>特点：将输入值压缩到 -1 到 1 之间，相比 Sigmoid 函数，以 0 为中心。<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-1.png" alt="alt text"></li>
</ul>
</li>
<li><p><strong>ReLU 函数（Rectified Linear Unit）</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = max(0, x)</code></li>
<li>特点：计算简单，在正半轴上梯度恒为 1，有效缓解了梯度消失问题。但存在神经元“死亡”的可能，即输入为负时永远不被激活。<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-2.png" alt="alt text"></li>
</ul>
</li>
<li><p><strong>Leaky ReLU 函数</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = max(ax, x)</code> ，其中 <code>a</code> 是一个较小的正数（如 0.01）</li>
<li>特点：对 ReLU 进行改进，解决了神经元“死亡”的问题。<br><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719937549074.png" alt="1719937549074"></li>
</ul>
</li>
<li><p><strong>ELU 函数（Exponential Linear Unit）</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = x if x &gt; 0 else a(e^x - 1)</code> ，其中 <code>a</code> 是一个常数</li>
<li>特点：具有 ReLU 的优点，同时在输入为负时输出不为 0，使得平均输出更接近 0。</li>
<li><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719937783555.png" alt="1719937783555"></li>
</ul>
</li>
<li><p><strong>Softmax 函数</strong>：</p>
<ul>
<li>常用于多分类问题的输出层，将多个神经元的输出值映射为概率分布。</li>
</ul>
</li>
</ol>
<p>以下是一个使用 Python 绘制部分常见激活函数图像的示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tanh</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.tanh(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">leaky_relu</span>(<span class="params">x, a=<span class="number">0.01</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(a * x, x)</span><br><span class="line"></span><br><span class="line">x = np.linspace(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">plt.plot(x, sigmoid(x), label=<span class="string">&#x27;Sigmoid&#x27;</span>)</span><br><span class="line">plt.plot(x, tanh(x), label=<span class="string">&#x27;Tanh&#x27;</span>)</span><br><span class="line">plt.plot(x, relu(x), label=<span class="string">&#x27;ReLU&#x27;</span>)</span><br><span class="line">plt.plot(x, leaky_relu(x), label=<span class="string">&#x27;Leaky ReLU&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Input&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Output&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Common Activation Functions&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>不同的激活函数在不同的场景和网络结构中表现各异，需要根据具体问题进行选择和调整。</p>
<h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><p>学习率（Learning Rate）在机器学习中是一个非常关键的超参数。</p>
<p>学习率决定了模型在训练过程中参数更新的步长大小。</p>
<p>较小的学习率意味着模型在参数更新时采取较小的步幅，可能会使训练过程更加稳定，但收敛速度较慢，需要更多的训练迭代次数才能达到较好的效果。</p>
<p>例如，如果学习率过小，如 0.0001 ，模型可能会在训练中进展缓慢，需要大量的训练轮数才能逐渐优化参数。</p>
<p>较大的学习率会使模型在参数更新时采取较大的步幅，可能会加快收敛速度，但也可能导致模型跳过最优解，甚至无法收敛。</p>
<p>比如，学习率过大，如 10 ，模型可能会在训练中出现剧烈的波动，无法稳定地优化参数。</p>
<p>选择合适的学习率通常需要通过试验和错误来确定。常见的方法包括使用固定的学习率、学习率衰减（随着训练的进行逐渐减小学习率）、自适应学习率算法（如 Adam 优化器中的自适应学习率调整）等。</p>
<p>例如，在深度学习中，训练开始时可以使用较大的学习率，如 0.1 ，然后随着训练的进行，逐渐将学习率减小到 0.001 ，以实现更精细的参数调整和更好的收敛效果。</p>
<h3 id="以下是一些常见的学习率调整方法："><a href="#以下是一些常见的学习率调整方法：" class="headerlink" title="以下是一些常见的学习率调整方法："></a>以下是一些常见的学习率调整方法：</h3><ol>
<li><p><strong>固定学习率</strong>：在整个训练过程中保持学习率不变。这种方法简单，但可能不是最优的，因为在训练的不同阶段可能需要不同的学习率。</p>
</li>
<li><p><strong>分段常数学习率</strong>：将训练过程分为几个阶段，每个阶段使用不同的固定学习率。例如，在前几个 epoch 使用较大的学习率，然后在后续阶段使用较小的学习率。</p>
</li>
<li><p><strong>学习率衰减</strong>：</p>
<ul>
<li><strong>按步长衰减</strong>：每隔一定的训练步数或 epoch，将学习率乘以一个小于 1 的衰减因子。</li>
<li><strong>指数衰减</strong>：学习率按照指数形式衰减，例如 <code>learning_rate = initial_learning_rate * decay_rate ^ (epoch / decay_steps)</code> 。</li>
<li><strong>多项式衰减</strong>：学习率按照多项式的形式逐渐减小。</li>
</ul>
</li>
<li><p><strong>自适应学习率算法</strong>：</p>
<ul>
<li><strong>Adagrad</strong>：根据每个参数之前的梯度历史来调整学习率，对于不常更新的参数给予较大的学习率，对于频繁更新的参数给予较小的学习率。</li>
<li><strong>Adadelta</strong>：是对 Adagrad 的改进，避免了学习率单调递减的问题。</li>
<li><strong>RMSProp</strong>：类似于 Adadelta，对梯度的二阶矩进行指数加权平均来调整学习率。</li>
<li><strong>Adam</strong>：结合了动量和 RMSProp 的优点，能够自适应地调整学习率。</li>
</ul>
</li>
<li><p><strong>余弦退火</strong>：学习率按照余弦函数的形式进行周期性的变化，在每个周期内从较高的值逐渐降低到较低的值，然后再上升。</p>
</li>
<li><p><strong>基于验证集性能调整</strong>：根据模型在验证集上的性能来动态调整学习率。如果验证集性能在一段时间内没有改善，就降低学习率。</p>
</li>
</ol>
<p>这些方法各有优缺点，具体选择哪种方法取决于数据集、模型架构和训练需求等因素。通常需要通过实验来找到最适合特定问题的学习率调整策略。</p>
<h2 id="optimizer的概念"><a href="#optimizer的概念" class="headerlink" title="optimizer的概念"></a>optimizer的概念</h2><p>在机器学习中，<code>optimizer</code>（优化器）是用于调整模型参数以最小化损失函数或最大化目标函数的算法或策略。</p>
<p>优化器的主要作用是根据模型的当前状态（包括参数值和计算得到的梯度）来决定如何更新模型的参数，以使得模型在训练数据上的性能逐渐提高。</p>
<p>常见的优化器有随机梯度下降（Stochastic Gradient Descent，SGD）、Adagrad、Adadelta、RMSprop、Adam 等。</p>
<p>以随机梯度下降（SGD）为例，它的基本思想是沿着梯度的反方向，以一定的学习率来更新参数。假设参数为 <code>w</code> ，梯度为 <code>g</code> ，学习率为 <code>lr</code> ，则更新公式为 <code>w = w - lr * g</code> 。</p>
<p>Adagrad 则根据每个参数之前的梯度历史来自适应地调整学习率。对于频繁更新的参数，学习率会逐渐减小，而对于很少更新的参数，学习率会相对较大。</p>
<p>RMSprop 类似于 Adagrad，但它不是累积所有的梯度平方，而是使用指数加权平均来计算梯度平方的估计值。</p>
<p>Adam 结合了动量和 RMSprop 的优点，同时考虑了梯度的一阶矩和二阶矩来动态调整学习率。</p>
<p>选择合适的优化器对于模型的训练效率和最终性能至关重要。例如，在数据量较大且模型较复杂时，Adam 通常能取得较好的效果；而在一些简单的模型和小数据集上，SGD 可能表现不错，并且通过适当的调整学习率等超参数，也能获得较好的结果。</p>
<p>再比如，在处理具有稀疏特征的问题时，Adagrad 可能更合适。总之，优化器的选择需要根据具体的问题和数据特点来决定，并可能需要通过实验来找到最优的选择。</p>
<h2 id="pytorch代码分析"><a href="#pytorch代码分析" class="headerlink" title="pytorch代码分析:"></a>pytorch代码分析:</h2><p>如何理解下面的代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">		<span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">		self.layer = nn.Linear(n_feature, n_hidden)</span><br><span class="line">		self.all_act = nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">		x = self.layer(x)</span><br><span class="line">		x = torch.tanh(x)</span><br><span class="line">		x = self.all_act(x)</span><br><span class="line">		<span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<p>这段代码是一个简单的神经网络模型的定义，使用 PyTorch 框架来构建。让我解释一下这段代码的含义：</p>
<ol>
<li>**class Net(nn.Module):**：定义了一个名为 <strong>Net</strong> 的类，继承自 <strong>nn.Module</strong> 类，表示这个类是一个神经网络模型。</li>
</ol>
<p>**2. **def <strong>init</strong>(self, n_feature, n_hidden, n_output):<strong>：定义了 <strong>Net</strong> 类的构造函数 <strong><strong>init</strong><strong>，该函数接受三个参数 <strong>n_feature</strong>、</strong>n_hidden</strong> 和 <strong>n_output</strong>，分别表示输入特征的数量、隐藏层神经元的数量和输出的数量。</strong></p>
<p>**3. **super(Net, self).<strong>init</strong>()<strong>：调用父类 <strong>nn.Module</strong> 的构造函数，初始化神经网络模型。</strong></p>
<p>**4. **self.layer &#x3D; nn.Linear(n_feature, n_hidden)<strong>：定义了一个线性层 <strong>layer</strong>，输入特征数量为 <strong>n_feature</strong>，输出特征数量为 <strong>n_hidden</strong>。</strong></p>
<p>**5. **self.all_act &#x3D; nn.Linear(n_hidden, n_output)<strong>：定义了另一个线性层 <strong>all_act</strong>，输入特征数量为 <strong>n_hidden</strong>，输出特征数量为 <strong>n_output</strong>。</strong></p>
<ol start="6">
<li>**def forward(self, x):**：定义了前向传播函数 <strong>forward</strong>，接受输入 <strong>x</strong>，表示对输入数据进行前向传播计算。</li>
<li>**x &#x3D; self.layer(x)**：将输入 <strong>x</strong> 经过 <strong>layer</strong> 线性层的计算，得到隐藏层的输出。</li>
<li>**x &#x3D; torch.tanh(x)**：对隐藏层的输出应用双曲正切函数（tanh）作为激活函数，增加模型的非线性能力。</li>
<li>**x &#x3D; self.all_act(x)**：将经过激活函数后的隐藏层输出再经过 <strong>all_act</strong> 线性层的计算，得到最终的输出。</li>
<li><strong>return x</strong>：返回神经网络模型的输出。</li>
</ol>
<p>这段代码定义了一个简单的神经网络模型，包括一个输入层到隐藏层的线性变换和隐藏层到输出层的线性变换，中间使用了双曲正切函数作为激活函数。在神经网络的训练过程中，可以通过调用 <strong>forward</strong> 函数来进行前向传播计算。</p>
<h3 id="tf-one-hot"><a href="#tf-one-hot" class="headerlink" title="tf.one_hot"></a>tf.one_hot</h3><p><code>tf.one_hot</code> 是 TensorFlow 中的一个函数，用于将输入的索引值转换为独热编码（One-Hot Encoding）的张量。</p>
<p>独热编码是一种将类别变量转换为二进制向量的编码方式，其中只有一个元素为 1 ，其余元素为 0 。</p>
<p>以下是 <code>tf.one_hot</code> 函数的一般用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">indices = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">depth = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">one_hot_encoded = tf.one_hot(indices, depth)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(one_hot_encoded.numpy())</span><br></pre></td></tr></table></figure>

<p>在上述示例中，<code>indices</code> 是要编码的索引值列表，<code>depth</code> 表示编码的维度（即类别数量）。</p>
<p>例如，对于索引值 <code>0</code> ，在维度为 4 的编码中，得到的独热编码为 <code>[1, 0, 0, 0]</code> ；对于索引值 <code>2</code> ，得到的编码为 <code>[0, 0, 1, 0]</code> 。</p>
<p><code>tf.one_hot</code> 常用于将分类标签转换为适合神经网络输入的形式，方便模型进行处理和计算。</p>
<p>上述代码中，<code>indices = [0, 2, 1, 3]</code> 且 <code>depth = 4</code> ，使用 <code>tf.one_hot</code> 函数进行独热编码后的输出结果应该是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[1 0 0 0]</span><br><span class="line"> [0 0 1 0]</span><br><span class="line"> [0 1 0 0]</span><br><span class="line"> [0 0 0 1]]</span><br></pre></td></tr></table></figure>

<p>即索引为 0 的位置编码为 <code>[1, 0, 0, 0]</code> ，索引为 2 的位置编码为 <code>[0, 0, 1, 0]</code> ，索引为 1 的位置编码为 <code>[0, 1, 0, 0]</code> ，索引为 3 的位置编码为 <code>[0, 0, 0, 1]</code> 。</p>
<h3 id="self-critic-net-parameters"><a href="#self-critic-net-parameters" class="headerlink" title="self.critic_net.parameters()"></a>self.critic_net.parameters()</h3><p><code>self.critic_net.parameters()</code> 通常是在 Python 的深度学习框架（如 PyTorch）中使用的代码。</p>
<p><code>critic_net</code> 可能是您定义的一个神经网络模型（例如，用于评估或批评某些数据的模型）。</p>
<p><code>parameters()</code> 方法返回模型中的可学习参数，这些参数通常是权重（weights）和偏置（biases）。</p>
<p>例如，如果 <code>critic_net</code> 是一个简单的全连接神经网络，那么通过 <code>self.critic_net.parameters()</code> 您将获取到该网络中所有层的权重和偏置的迭代器。</p>
<p>您可以使用这个返回值来进行一些操作，比如：</p>
<ol>
<li><p>在优化器中使用它来进行参数的更新，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(self.critic_net.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>对参数进行一些统计或分析，比如计算参数的数量、查看参数的值等。</p>
</li>
</ol>
<p>总之，<code>self.critic_net.parameters()</code> 为您提供了对模型可学习参数的访问，以便进行各种与模型训练和优化相关的操作。</p>
<h3 id="self-optimizer-zero-grad"><a href="#self-optimizer-zero-grad" class="headerlink" title="self.optimizer.zero_grad()"></a>self.optimizer.zero_grad()</h3><p><code>self.optimizer.zero_grad()</code> 通常用于将模型参数的梯度清零。</p>
<p>当进行反向传播计算梯度后，如果不将梯度清零，那么在后续的迭代中，新计算的梯度会与之前的梯度累加。这会导致梯度计算的错误，影响模型的训练效果</p>
<h3 id="loss-backward-retain-graph-True"><a href="#loss-backward-retain-graph-True" class="headerlink" title="loss.backward(retain_graph&#x3D;True)"></a>loss.backward(retain_graph&#x3D;True)</h3><p>在深度学习中，<code>loss.backward(retain_graph=True)</code> 用于计算损失函数 <code>loss</code> 关于模型参数的梯度，同时通过设置 <code>retain_graph=True</code> 来保留计算图。</p>
<p>通常，在进行一次反向传播计算梯度后，计算图会被释放以节省内存。但当您需要多次对同一计算图执行反向传播时，就需要设置 <code>retain_graph=True</code> 。</p>
<p>例如，如果您在一个循环中多次计算梯度并更新参数，像下面这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">    <span class="comment"># 前向传播，计算损失</span></span><br><span class="line">    loss = some_operation()</span><br><span class="line">    <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">    loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 根据梯度更新参数</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<p>在上述示例中，由于在循环内多次执行反向传播，所以需要保留计算图，以便下一次迭代能够再次基于相同的计算图计算梯度。</p>
<p>如果不设置 <code>retain_graph=True</code> ，在第二次执行 <code>loss.backward()</code> 时会报错，因为计算图已经被释放。</p>
<p>需要注意的是，一直保留计算图会占用较多内存，所以在不需要多次使用计算图时，应避免设置 <code>retain_graph=True</code> ，以节省内存资源。</p>
<p>在 PyTorch 中，<code>retain_graph=True</code> 的作用是在执行反向传播（<code>backward</code>）后保留计算图。</p>
<p>PyTorch 采用动态图机制，默认情况下，每次反向传播之后都会释放计算图以节省内存。然而，当需要对同一个计算图进行多次反向传播时，就需要设置 <code>retain_graph=True</code>。</p>
<p>例如，假设有一个输入 <code>x</code>，经过一些计算得到 <code>y</code>，然后有两个输出 <code>output_1</code> 和 <code>output_2</code>，都需要对其执行反向传播。如果在第一次对 <code>output_1</code> 执行 <code>backward</code> 时没有设置 <code>retain_graph=True</code>，那么在执行完后计算图就会被释放。当再对 <code>output_2</code> 执行 <code>backward</code> 时，由于计算图已经不存在，就会引发错误。</p>
<p>通过设置 <code>retain_graph=True</code>，可以保留中间参数，使得多次 <code>backward</code> 之间不会相互影响。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss1.backward(retain_graph=<span class="literal">True</span>)  <span class="comment"># 第一次 backward，保留计算图</span></span><br><span class="line">loss2.backward()  <span class="comment"># 第二次 backward，此时计算图仍然存在</span></span><br></pre></td></tr></table></figure>

<p>需要注意的是，一直保留计算图会占用较多内存，所以在不需要多次使用计算图时，应避免设置 <code>retain_graph=True</code>，以节省内存资源。</p>
<p>另外，<code>create_graph</code> 参数用于创建导数计算图，允许进行高阶导数的计算；而 <code>grad_tensors</code> 可用于设置多梯度的权重，当有多个损失函数需要计算梯度时，可以设置各个损失之间权重的比例。</p>
<h4 id="如何判断是否需要多次使用计算图？"><a href="#如何判断是否需要多次使用计算图？" class="headerlink" title="如何判断是否需要多次使用计算图？"></a>如何判断是否需要多次使用计算图？</h4><p>判断是否需要多次使用计算图，主要考虑以下几个方面：</p>
<ol>
<li>模型的结构和需求：如果你的模型在一次前向传播后，需要对不同的输出或中间结果进行多次反向传播来计算梯度，以实现不同的目的，例如优化不同的部分或计算多个相关的梯度，那么就可能需要多次使用计算图。</li>
<li>是否涉及多个相关的损失函数：当存在多个损失函数，且需要分别或同时根据这些损失函数计算梯度并更新参数时，通常需要多次使用计算图。</li>
<li>循环或迭代的操作：在某些情况下，可能会在一个循环或迭代中多次执行类似的计算和反向传播操作。例如，在强化学习中，可能需要在每个时间步都进行前向传播和反向传播。</li>
<li>中间结果的重用：如果在计算过程中需要多次访问或使用某些中间结果的梯度，那么就需要保留计算图以进行多次反向传播。</li>
</ol>
<p>例如，如果你有一个神经网络模型，其中包含多个子模块，每个子模块都有自己的损失，并且你希望分别根据这些子模块的损失来更新它们的参数，那么就需要多次使用计算图，对每个子模块的损失进行单独的反向传播。</p>
<p>又或者在训练过程中，你可能想要尝试不同的优化策略或超参数，需要在同一次前向传播后，多次计算梯度并更新参数，来比较不同策略或参数的效果，这也需要多次使用计算图。</p>
<p>另外，一些复杂的模型结构或自定义的计算流程可能会导致需要多次使用计算图的情况。但需注意，一直保留计算图会占用较多内存，所以在不需要多次使用计算图时，应避免设置 <code>retain_graph=True</code> ，以节省内存资源。</p>
<p>如果你不确定是否需要多次使用计算图，可以先尝试在不保留计算图的情况下（即不设置 <code>retain_graph=True</code> ）运行代码，观察是否会出现报错“trying to backward through the graph a second time, but the buffers have already been freed”。如果出现该报错，且确实需要进行多次反向传播，那么就需要设置 <code>retain_graph=True</code> 。同时，为了避免内存过度占用，在完成所有需要的反向传播后，应及时释放不再使用的计算图和相关数据。</p>
<h3 id="tf-reduce-mean"><a href="#tf-reduce-mean" class="headerlink" title="tf.reduce_mean"></a>tf.reduce_mean</h3><p><code>tf.reduce_mean</code> 是 TensorFlow 中的一个函数，用于计算张量在指定维度上的平均值。</p>
<p>以下是一个示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个张量</span></span><br><span class="line">tensor = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算所有元素的平均值</span></span><br><span class="line">average = tf.reduce_mean(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每一行的平均值</span></span><br><span class="line">row_averages = tf.reduce_mean(tensor, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每一列的平均值</span></span><br><span class="line">column_averages = tf.reduce_mean(tensor, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;所有元素的平均值：&quot;</span>, sess.run(average))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;每一行的平均值：&quot;</span>, sess.run(row_averages))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;每一列的平均值：&quot;</span>, sess.run(column_averages))</span><br></pre></td></tr></table></figure>

<p>在上述示例中：</p>
<ul>
<li>当不指定 <code>axis</code> 参数时，<code>tf.reduce_mean(tensor)</code> 计算张量 <code>tensor</code> 中所有元素的平均值。</li>
<li><code>tf.reduce_mean(tensor, axis=1)</code> 计算每一行的平均值，得到一个长度为行数的张量。</li>
<li><code>tf.reduce_mean(tensor, axis=0)</code> 计算每一列的平均值，得到一个长度为列数的张量。</li>
</ul>
<p>例如，对于上述示例中的张量 <code>[[1, 2, 3], [4, 5, 6]]</code> ：</p>
<ul>
<li>所有元素的平均值为 <code>(1 + 2 + 3 + 4 + 5 + 6) / 6 = 3.5</code> 。</li>
<li>每一行的平均值分别为 <code>(1 + 2 + 3) / 3 = 2</code> 和 <code>(4 + 5 + 6) / 3 = 5</code> 。</li>
<li>每一列的平均值分别为 <code>(1 + 4) / 2 = 2.5</code> 、 <code>(2 + 5) / 2 = 3.5</code> 和 <code>(3 + 6) / 2 = 4.5</code> 。</li>
</ul>
<h3 id="tf-placeholder"><a href="#tf-placeholder" class="headerlink" title="tf.placeholder"></a>tf.placeholder</h3><p>用于表示强化学习或神经网络中的状态输入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with tf.name_scope(&#x27;S&#x27;):#状态</span><br><span class="line">    S = tf.placeholder(tf.float32, shape=[None, state_dim], name=&#x27;s&#x27;)</span><br></pre></td></tr></table></figure>

<ol>
<li>tf.name_scope(‘S’)</li>
</ol>
<p>这行代码使用 TensorFlow 的 tf.name_scope 创建一个命名空间 S。命名空间在 TensorFlow 中用于组织图中的节点，使其更具可读性和结构化。在 TensorBoard 中查看图时，可以更清晰地看到节点的组织结构。</p>
<ol start="2">
<li>tf.placeholder(tf.float32, shape&#x3D;[None, state_dim], name&#x3D;’s’)</li>
</ol>
<p>这行代码定义了一个占位符 S。占位符在 TensorFlow 中用于在执行图时提供输入数据。这里的占位符有以下几个参数：</p>
<p>tf.float32：占位符的数据类型是 32 位浮点数。</p>
<p><strong>shape&#x3D;[None, state_dim]：占位符的形状。shape 参数定义了输入数据的维度</strong>：</p>
<p>None 表示这个维度可以是任意长度，通常用于<strong>批次（batch）的大小</strong>。使用 None 是因为在实际运行时，批次的大小可能会有所不同。</p>
<p>state_dim 表示状态的维度。这是一个整数，定义了每个状态的特征数量。在强化学习中，状态通常是一个向量，其长度由具体的环境或问题决定。</p>
<p>name&#x3D;’s’：给占位符命名为 ‘s’。这个名字在构建和调试图时有助于识别。</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720887133485.png" alt="1720887133485"></p>
<h3 id="tf-variable-scope"><a href="#tf-variable-scope" class="headerlink" title="tf.variable_scope"></a>tf.variable_scope</h3><p><code>tf.variable_scope</code> 是 TensorFlow 中用于管理变量作用域的一个重要工具。它帮助组织和复用变量，特别是在构建复杂的神经网络时</p>
<h4 id="1-什么是-tf-variable-scope？"><a href="#1-什么是-tf-variable-scope？" class="headerlink" title="1. 什么是 tf.variable_scope？"></a>1. 什么是 <code>tf.variable_scope</code>？</h4><p><code>tf.variable_scope</code> 提供了一种机制来创建和管理变量范围（scope），这对变量进行命名和复用非常有帮助。通过使用变量范围，可以确保变量命名的一致性和避免命名冲突。</p>
<h4 id="2-为什么使用-tf-variable-scope？"><a href="#2-为什么使用-tf-variable-scope？" class="headerlink" title="2. 为什么使用 tf.variable_scope？"></a>2. 为什么使用 <code>tf.variable_scope</code>？</h4><ul>
<li><strong>组织变量</strong>：可以将相关的变量组织在一起，使代码更具可读性和结构性。</li>
<li><strong>复用变量</strong>：在共享参数的模型（如共享权重的神经网络层）中，复用变量非常重要。</li>
<li><strong>命名管理</strong>：自动处理变量命名，避免命名冲突。</li>
</ul>
<h4 id="3-如何使用-tf-variable-scope？"><a href="#3-如何使用-tf-variable-scope？" class="headerlink" title="3. 如何使用 tf.variable_scope？"></a>3. 如何使用 <code>tf.variable_scope</code>？</h4><p>使用 <code>tf.variable_scope</code> 创建一个新的变量作用域，可以在该作用域内定义和复用变量。</p>
<h5 id="示例-1：创建变量作用域"><a href="#示例-1：创建变量作用域" class="headerlink" title="示例 1：创建变量作用域"></a>示例 1：创建变量作用域</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope1&#x27;</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope2&#x27;</span>):</span><br><span class="line">    var2 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">2.0</span>))</span><br></pre></td></tr></table></figure>

<p>在这个例子中，<code>var1</code> 和 <code>var2</code> 是在不同的变量作用域中创建的，尽管它们的名字相同，但在图中它们是不同的变量，分别命名为 <code>scope1/var</code> 和 <code>scope2/var</code>。</p>
<h5 id="示例-2：复用变量"><a href="#示例-2：复用变量" class="headerlink" title="示例 2：复用变量"></a>示例 2：复用变量</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope&#x27;</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope&#x27;</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">    var2 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>在这个例子中，<code>var2</code> 复用了 <code>var1</code> 的变量。通过设置 <code>reuse=True</code>，确保在相同的作用域内复用已经存在的变量。</p>
<h5 id="4-tf-get-variable-和-tf-Variable-的区别"><a href="#4-tf-get-variable-和-tf-Variable-的区别" class="headerlink" title="4. tf.get_variable 和 tf.Variable 的区别"></a>4. <code>tf.get_variable</code> 和 <code>tf.Variable</code> 的区别</h5><ul>
<li>**<code>tf.get_variable</code>**：用于创建或获取变量，通常与 <code>tf.variable_scope</code> 一起使用。它支持变量复用机制。</li>
<li>**<code>tf.Variable</code>**：直接创建新变量，不支持复用机制。</li>
</ul>
<h5 id="示例-3：使用-tf-get-variable-和-tf-Variable"><a href="#示例-3：使用-tf-get-variable-和-tf-Variable" class="headerlink" title="示例 3：使用 tf.get_variable 和 tf.Variable"></a>示例 3：使用 <code>tf.get_variable</code> 和 <code>tf.Variable</code></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope1&#x27;</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line">    var2 = tf.Variable([<span class="number">1.0</span>], name=<span class="string">&#x27;var2&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>在这个例子中，<code>var1</code> 是通过 <code>tf.get_variable</code> 创建的，支持复用机制，而 <code>var2</code> 是通过 <code>tf.Variable</code> 创建的，不支持复用。</p>
<h4 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h4><p><code>tf.variable_scope</code> 是 TensorFlow 中用于管理变量作用域的工具，帮助组织和复用变量，提供了一种结构化和高效的变量管理方式。通过合理使用变量作用域，可以避免命名冲突，实现变量复用，特别是在构建共享参数的复杂神经网络时非常有用。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz">ALTNT</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/">http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/img/altnt.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5/" title="强化学习相关概念"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">强化学习相关概念</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/22/%E8%B5%84%E6%96%99/%E5%91%BD%E4%BB%A4/" title=""><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info"></div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/altnt.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ALTNT</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">17</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">8</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ALTNT"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB"><span class="toc-number">1.</span> <span class="toc-text">机器学习分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.</span> <span class="toc-text">回归问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BB%BA%E7%AB%8B"><span class="toc-number">1.2.</span> <span class="toc-text">回归模型的建立</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E7%A1%AE%E5%AE%9A%E7%94%A8%E4%BA%8E%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.2.1.</span> <span class="toc-text">第一步：确定用于回归任务的模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E6%9D%A5%E8%A1%A1%E9%87%8F%E8%BF%99%E4%BA%9B%E5%A4%87%E9%80%89%E5%87%BD%E6%95%B0%E7%9A%84%E5%A5%BD%E5%9D%8F%E7%A8%8B%E5%BA%A6%EF%BC%88%E7%A1%AE%E5%AE%9A%E5%8F%82%E6%95%B0%EF%BC%89"><span class="toc-number">1.2.2.</span> <span class="toc-text">第二步：使用训练数据来衡量这些备选函数的好坏程度（确定参数）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A%E6%A0%B9%E6%8D%AE%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%80%89%E5%87%BA%E6%8B%9F%E5%90%88%E6%9C%80%E5%A5%BD%E7%9A%84%E5%87%BD%E6%95%B0%EF%BC%8C%E4%BD%9C%E4%B8%BA%E6%9C%80%E7%BB%88%E7%9A%84%E6%8B%9F%E5%90%88%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.3.</span> <span class="toc-text">第三步：根据训练数据选出拟合最好的函数，作为最终的拟合函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="toc-number">2.1.</span> <span class="toc-text">预测结果分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">2.2.</span> <span class="toc-text">正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%81%97%E7%95%99%E9%97%AE%E9%A2%98"><span class="toc-number">2.3.</span> <span class="toc-text">遗留问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-number">2.4.</span> <span class="toc-text">梯度下降算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%AD%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7%EF%BC%88Tips%EF%BC%89"><span class="toc-number">2.4.1.</span> <span class="toc-text">梯度下降中常用技巧（Tips）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">2.4.1.1.</span> <span class="toc-text">一、调整学习率</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#AdaGrad%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">2.4.1.1.1.</span> <span class="toc-text">AdaGrad优化器</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88SGD%EF%BC%89"><span class="toc-number">2.4.1.2.</span> <span class="toc-text">二、随机梯度下降（SGD）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%EF%BC%88Feature-Scaling%EF%BC%89"><span class="toc-number">2.4.1.3.</span> <span class="toc-text">三、特征缩放（Feature Scaling）</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="toc-number">3.</span> <span class="toc-text">偏差和方差</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E5%AE%9A%E4%B9%89"><span class="toc-number">3.1.</span> <span class="toc-text">数学定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-number">3.2.</span> <span class="toc-text">偏差-方差与模型复杂度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95"><span class="toc-number">3.3.</span> <span class="toc-text">调整方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">3.4.</span> <span class="toc-text">K-折交叉验证</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax"><span class="toc-number">4.</span> <span class="toc-text">softmax</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E7%94%A8%E9%80%94"><span class="toc-number">4.1.</span> <span class="toc-text">主要用途:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.</span> <span class="toc-text">损失函数:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">5.1.</span> <span class="toc-text">常见的损失函数:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0%E8%AE%A1%E7%AE%97"><span class="toc-number">5.2.</span> <span class="toc-text">损失函数的导数计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">6.</span> <span class="toc-text">激活函数:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%9A"><span class="toc-number">6.1.</span> <span class="toc-text">常见的激活函数：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">7.</span> <span class="toc-text">学习率</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A5%E4%B8%8B%E6%98%AF%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="toc-number">7.1.</span> <span class="toc-text">以下是一些常见的学习率调整方法：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#optimizer%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">8.</span> <span class="toc-text">optimizer的概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90"><span class="toc-number">9.</span> <span class="toc-text">pytorch代码分析:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-one-hot"><span class="toc-number">9.1.</span> <span class="toc-text">tf.one_hot</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self-critic-net-parameters"><span class="toc-number">9.2.</span> <span class="toc-text">self.critic_net.parameters()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self-optimizer-zero-grad"><span class="toc-number">9.3.</span> <span class="toc-text">self.optimizer.zero_grad()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loss-backward-retain-graph-True"><span class="toc-number">9.4.</span> <span class="toc-text">loss.backward(retain_graph&#x3D;True)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6%E9%9C%80%E8%A6%81%E5%A4%9A%E6%AC%A1%E4%BD%BF%E7%94%A8%E8%AE%A1%E7%AE%97%E5%9B%BE%EF%BC%9F"><span class="toc-number">9.4.1.</span> <span class="toc-text">如何判断是否需要多次使用计算图？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-reduce-mean"><span class="toc-number">9.5.</span> <span class="toc-text">tf.reduce_mean</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-placeholder"><span class="toc-number">9.6.</span> <span class="toc-text">tf.placeholder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf-variable-scope"><span class="toc-number">9.7.</span> <span class="toc-text">tf.variable_scope</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF-tf-variable-scope%EF%BC%9F"><span class="toc-number">9.7.1.</span> <span class="toc-text">1. 什么是 tf.variable_scope？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8-tf-variable-scope%EF%BC%9F"><span class="toc-number">9.7.2.</span> <span class="toc-text">2. 为什么使用 tf.variable_scope？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-tf-variable-scope%EF%BC%9F"><span class="toc-number">9.7.3.</span> <span class="toc-text">3. 如何使用 tf.variable_scope？</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-1%EF%BC%9A%E5%88%9B%E5%BB%BA%E5%8F%98%E9%87%8F%E4%BD%9C%E7%94%A8%E5%9F%9F"><span class="toc-number">9.7.3.1.</span> <span class="toc-text">示例 1：创建变量作用域</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-2%EF%BC%9A%E5%A4%8D%E7%94%A8%E5%8F%98%E9%87%8F"><span class="toc-number">9.7.3.2.</span> <span class="toc-text">示例 2：复用变量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-tf-get-variable-%E5%92%8C-tf-Variable-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">9.7.3.3.</span> <span class="toc-text">4. tf.get_variable 和 tf.Variable 的区别</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-3%EF%BC%9A%E4%BD%BF%E7%94%A8-tf-get-variable-%E5%92%8C-tf-Variable"><span class="toc-number">9.7.3.4.</span> <span class="toc-text">示例 3：使用 tf.get_variable 和 tf.Variable</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E6%80%BB%E7%BB%93"><span class="toc-number">9.7.4.</span> <span class="toc-text">5. 总结</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/28/vscode%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/vscode%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/" title="vscode插件开发">vscode插件开发</a><time datetime="2024-06-28T06:47:30.251Z" title="Created 2024-06-28 14:47:30">2024-06-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5/" title="强化学习相关概念">强化学习相关概念</a><time datetime="2024-06-28T04:03:18.846Z" title="Created 2024-06-28 12:03:18">2024-06-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/" title="机器学习相关概念">机器学习相关概念</a><time datetime="2024-06-26T09:09:56.000Z" title="Created 2024-06-26 17:09:56">2024-06-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/22/%E8%B5%84%E6%96%99/%E5%91%BD%E4%BB%A4/" title="Untitled">Untitled</a><time datetime="2024-06-22T03:44:40.000Z" title="Created 2024-06-22 11:44:40">2024-06-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/05/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E5%8E%9F%E7%90%86/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%AE%BA%E7%AC%94%E8%AE%B0/" title="数据库系统概论笔记">数据库系统概论笔记</a><time datetime="2024-06-05T15:46:23.990Z" title="Created 2024-06-05 23:46:23">2024-06-05</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By ALTNT</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>