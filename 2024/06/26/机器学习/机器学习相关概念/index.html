<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习相关概念 | ALTNT's Hexo Blog</title><meta name="author" content="ALTNT"><meta name="copyright" content="ALTNT"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="机器学习分类 根据训练数据是否有标签，可以分为： 监督学习：所有训练数据均具有标签（典型的问题有回归：模型输出的是一个具体数值；分类：模型的输出是某一类别） 在监督学习中，常用的模型种类可以分为：线性模型 和 非线性模型。其中，非线性模型应用更加广泛，表达能力也更强，包括深度学习，支持向量机（SVM），决策树，K-NN算法等。 半监督学习：训练数据中，部分具有标签，另一部分没有标签（但是没有标签的">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习相关概念">
<meta property="og:url" content="http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/index.html">
<meta property="og:site_name" content="ALTNT&#39;s Hexo Blog">
<meta property="og:description" content="机器学习分类 根据训练数据是否有标签，可以分为： 监督学习：所有训练数据均具有标签（典型的问题有回归：模型输出的是一个具体数值；分类：模型的输出是某一类别） 在监督学习中，常用的模型种类可以分为：线性模型 和 非线性模型。其中，非线性模型应用更加广泛，表达能力也更强，包括深度学习，支持向量机（SVM），决策树，K-NN算法等。 半监督学习：训练数据中，部分具有标签，另一部分没有标签（但是没有标签的">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://blog.705553939.xyz/img/altnt.jpeg">
<meta property="article:published_time" content="2024-06-26T09:09:56.000Z">
<meta property="article:modified_time" content="2024-08-28T09:55:28.714Z">
<meta property="article:author" content="ALTNT">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.705553939.xyz/img/altnt.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习相关概念',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-08-28 17:55:28'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/altnt.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">35</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="ALTNT's Hexo Blog"><span class="site-name">ALTNT's Hexo Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习相关概念</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-06-26T09:09:56.000Z" title="Created 2024-06-26 17:09:56">2024-06-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-08-28T09:55:28.714Z" title="Updated 2024-08-28 17:55:28">2024-08-28</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习相关概念"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="机器学习分类">机器学习分类</h2>
<p>根据训练数据是否有标签，可以分为：</p>
<p>监督学习：所有训练数据均具有标签（典型的问题有回归：模型输出的是一个具体数值；分类：模型的输出是某一类别）
在监督学习中，常用的模型种类可以分为：线性模型 和
非线性模型。其中，非线性模型应用更加广泛，表达能力也更强，包括深度学习，支持向量机（SVM），决策树，K-NN算法等。</p>
<p>半监督学习：训练数据中，部分具有标签，另一部分没有标签（但是没有标签的数据，对于模型的学习也是有用处的）。
迁移学习：使用与当前任务无关的数据（可能有标签，可能没有标签）来促进当前模型的学习。
无监督学习：训练数据都没有标签。
无监督学习存在的原因是，现实世界中，为训练数据进行标注成本较高，当训练数据都没有标签时，如果我们想要为数据进行分类，只能根据数据的特征进行划分，比如聚类算法。</p>
<p>强化学习：训练数据没有标签，智能体从环境交互中进行学习，来更新自身的策略，根据最终环境的反馈（获得的奖励）来调整自身行为。</p>
<h3 id="回归问题">回归问题</h3>
<pre><code>机器学习笔记的第二篇博客，来介绍机器学习中最基础的回归任务，上一篇博客中有提到回归任务和分类任务的差别在于，回归任务中模型的输出是一个具体的数值， 而分类任务中模型的输出是某一类别。其实，许多问题我们都可以视为回归问题：![alt text](机器学习相关概念/image-4.png)</code></pre>
<p>例如：根据股票市场的历史数据预测明天的股票走势；自动驾驶中根据传感器获取的信息输出方向盘的转动角度；推荐系统中，输入用户和商品的特征，模型输出一个[0,1]之间的数值，表示购买的可能性。</p>
<h3 id="回归模型的建立">回归模型的建立</h3>
<p>机器学习模型建立的三个步骤：</p>
<ol type="1">
<li>我们准备许多备选的函数 <strong><em>f </em></strong>
，构成一个集合，也就是机器学习中的模型（Model）。</li>
<li>使用训练数据来衡量这些备选函数的好坏程度。</li>
<li>根据训练数据选出拟合最好的函数，作为最终的拟合函数。</li>
</ol>
<h4 id="第一步确定用于回归任务的模型">第一步：确定用于回归任务的模型</h4>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-5.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>这里选择一元线性函数 <strong><em>y = wx +
b</em></strong>即线性模型，一元表示我们使用的特征是一个（宝可梦的原CP值）；这样我们就构造了一个函数集合，由于参数<strong><em>w</em></strong>和<strong><em>b</em></strong>的取值是无穷的，所以函数集合中函数的个数是无穷个，接下来在函数集合中选择最好的函数的过程实际上就是为函数确定参数值的过程。</p>
<h4 id="第二步使用训练数据来衡量这些备选函数的好坏程度确定参数">第二步：使用训练数据来衡量这些备选函数的好坏程度（确定参数）</h4>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-6.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>接下来我们需要根据训练数据来从备选函数中选择一个效果最好的函数（即确定函数参数部分），这里需要使用损失函数，损失函数的输入是
“用来进行回归任务的函数” 和 “真实标签” ，输出是
进行回归任务的函数的好坏程度（Loss的值越小，认为该函数的效果越好）。如上图所示，我们使用
平方差之和 作为损失函数。</p>
<h4 id="第三步根据训练数据选出拟合最好的函数作为最终的拟合函数">第三步：根据训练数据选出拟合最好的函数，作为最终的拟合函数</h4>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-7.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>第三步，我们在训练数据上根据损失函数来评估拟合函数的好坏，找到使得损失函数最小的一组参数，作为我们最终的拟合函数。其中，寻找参数时，使用的方法为
<strong><em>梯度下降法</em></strong> 。</p>
<h2 id="梯度下降">梯度下降</h2>
<p>梯度下降算法是机器学习领域最广为人知、用途最广的优化算法，用来确定模型的参数（包括随机梯度下降SGD，Momentum，Adam等）。梯度下降算法的一个简单介绍如下：</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-8.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720165442054.png" alt="1720165442054">
<figcaption aria-hidden="true">1720165442054</figcaption>
</figure>
<p>1、在机器学习中，只要损失函数是可微分（可求导）的，就可以使用梯度下降算法进行参数的求解，那么怎么判断损失函数是否可微？（后面解释）</p>
<p>上面图片展示的是，模型当中只有一个参数（所以直接对该参数求导就可以），如果模型中存在两个及以上的参数，那么就需要分别对每个参数计算偏导数，然后根据参数更新公式进行每个参数的更新，如下图所示:
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-9.png" alt="alt text"></p>
<p>梯度下降算法的原理已经清楚，其实就是沿着损失函数降低的方向更新模型的参数，但是如果损失函数很复杂，比如下面图片所示，
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-10.png" alt="alt text"></p>
<p>我们很可能在更新参数的过程中，走到导数为0的点（第四个红点位置），这时因为不知道更新的方向，就陷入了局部最优点（其实真正的全局最优点还在右边）。</p>
<p>2、不过对于上面回归问题中，损失函数为平方差之和，该损失函数为凸函数，没有局部最优点，只有全局最优点。那么如何判断一个函数是否为凸函数？（后面解释）</p>
<h3 id="预测结果分析">预测结果分析</h3>
<p>我们上面的线性模型，经过梯度下降算法，寻得一组最优参数，其结果表现如下：
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-11.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-12.png" alt="alt text"></p>
<p>训练数据上面的损失函数值为31.9，测试数据上面损失函数值为35。在实际问题中，我们更加关注的是模型在测试集上面的性能表现，也就是模型的
<strong><em>泛化能力</em></strong>
，线性模型在测试集上面的误差较大，所以如果我们重新设计预测模型，使用更加复杂的模型，会不会得到更好的效果？
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-13.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-14.png" alt="alt text"></p>
<p>随着使用更加复杂的二次模型，三次模型，无论是在训练集还是测试集上面，效果都有提升。可是当继续增加模型的复杂度，使用四次模型的时候，虽然在训练集上面的<strong><em>loss</em></strong>更小，但是测试集上面的效果却变糟了。使用五次模型的时候，这一趋势更加明显：
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-15.png" alt="alt text">
虽然复杂的模型对于训练数据的拟合程度会更好，但是很容易出现过拟合的现象（过于严格的去拟合训练数据，当面对新数据的时候没有办法做出准确的预测，即无法泛化到其他数据）。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-16.png" alt="alt text">
在机器学习模型训练过程中，我们要尽量避免过拟合的现象，一方面要选择合适的模型，模型不是越复杂越好，可以通过交叉验证来选择合适的模型；另一方面，<strong>可以通过一些技术手段来帮助我们避免过拟合，比如正则化，early
stopping等等。</strong> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-17.png" alt="alt text">
如上图所示，<strong>蓝色线代表训练集上的损失函数，红色线代表验证集的损失函数</strong>，当训练进行到中间垂直的线段时，模型应该是最优的；如果继续训练，就会造成过拟合现象。</p>
<h3 id="正则化">正则化</h3>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-18.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>简单来说，正则化是一种为了减小<strong>测试误差</strong>的行为(有时候会增加训练误差)。我们在构造机器学习模型时，最终目的是让模型在面对新数据的时候，可以有很好的表现。当使用比较复杂的模型比如神经网络，去拟合数据时，很容易出现过拟合现象(训练集表现很好，测试集表现较差)，这会导致模型的泛化能力下降，这时候，就需要使用正则化，降低模型的复杂度。</p>
<p><strong>具体而言，正则化就是在损失函数后面增加一项惩罚项（对某些参数进行限制），使得我们的模型更加平滑。以上图为例，我们在损失函数后面增加一项关于参数w的正则项，限制参数w不要过大，这样模型会有更好的泛化能力。因为，这样对于测试数据中存在的噪声，会不那么敏感，即噪声对于预测的结果影响会降低。</strong>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720515070068.png" alt="1720515070068"></p>
<p>加入正则化技术之后，训练集和测试集上面的误差如下： <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-19.png" alt="alt text"></p>
<p>和我们的预期分析是一致的，加入正则化之后，参数<embed src="https://private.codecogs.com/gif.latex?%5Clambda"></p>
<p>越大，表示我们越关注模型的平滑程度（也就是模型的泛化能力），相对于训练误差考虑较少，所以训练集上面的loss增大，测试集上面的loss降低。但是，参数<embed src="https://private.codecogs.com/gif.latex?%5Clambda"></p>
<p>不是越大越好，我们希望得到一个比较平滑的函数，但是不能过于平滑（会丧失其预测能力）。</p>
<p><strong>机器学习中的正则化概念</strong></p>
<p>在机器学习中，正则化（Regularization）是一种用于防止过拟合（Overfitting）的技术。</p>
<p>过拟合是指模型在训练数据上表现得非常好，但在新的、未见过的数据上表现不佳。这通常是因为模型过于复杂，学习到了训练数据中的噪声和特定的细节，而不是一般性的模式。</p>
<p>正则化通过在损失函数中添加一个惩罚项来限制模型的复杂度。常见的正则化方法有
L1 正则化和 L2 正则化。</p>
<p><strong>L1 正则化</strong>：也称为 Lasso
正则化，它在损失函数中添加的惩罚项是模型参数的绝对值之和。L1
正则化具有特征选择的效果，因为它可能会将一些不重要的特征对应的参数压缩至零。</p>
<p>例如，在线性回归中，假设模型的预测函数为
<code>y = w1 * x1 + w2 * x2 +... + wn * xn + b</code> ，L1 正则化项就是
<code>λ * |w1| + λ * |w2| +... + λ * |wn|</code> ，其中 <code>λ</code>
是正则化参数，控制正则化的强度。</p>
<p><strong>L2 正则化</strong>：也称为 Ridge
正则化，它在损失函数中添加的惩罚项是模型参数的平方和。L2
正则化会使模型的参数值趋向于较小的值，但不太会将参数压缩至零。</p>
<p>同样在线性回归中，L2 正则化项就是
<code>λ * (w1^2 + w2^2 +... + wn^2)</code> 。</p>
<p><strong>正则化的作用</strong>：</p>
<ol type="1">
<li>控制模型复杂度：通过限制模型的参数大小，防止模型过于复杂。</li>
<li>提高模型泛化能力：使模型能够更好地应对新的数据，减少过拟合的风险。</li>
</ol>
<p><strong>举例说明</strong>：
假设我们正在训练一个神经网络来识别图像中的猫和狗。如果没有正则化，模型可能会过度学习训练数据中的细微特征，比如图片中的背景颜色或微小的噪声，导致在新的图像上识别准确率下降。</p>
<p>当我们应用 L2
正则化时，模型的参数会受到一定的约束，不会变得过大。这可能会导致模型在训练数据上的准确率稍微降低，但在测试数据上的表现会更好，因为它学习到了更通用的特征，而不是过度依赖于特定的训练样本。</p>
<p>总之，正则化是机器学习中非常重要的技术，有助于提高模型的性能和稳定性。</p>
<h3 id="遗留问题">遗留问题</h3>
<p>1、在机器学习中，只要损失函数是可微的，就可以使用梯度下降算法进行参数的求解，那么怎么判断损失函数是否可微？（后面解释）</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720166051610.png" alt="1720166051610">
<figcaption aria-hidden="true">1720166051610</figcaption>
</figure>
<p>2、不过对于上面回归问题中，损失函数为平方差之和，该损失函数为凸函数，没有局部最优点，只有全局最优点。那么如何判断一个函数是否为凸函数？（后面解释）</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720166067204.png" alt="1720166067204">
<figcaption aria-hidden="true">1720166067204</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-20.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>判断一个矩阵是不是半正定矩阵，方法之一是判断该矩阵的所有主子式是不是非负；对于上面的海森矩阵，其所有主子式为：</p>
<p><embed src="https://private.codecogs.com/gif.latex?2x%5E%7B2%7D">，2,
0，均为非负，所以该海森矩阵为半正定矩阵。所以该损失函数为凸函数，只有全局最优值。</p>
<p><strong>半正定矩阵</strong></p>
<p>半正定矩阵是矩阵理论中的一个重要概念。</p>
<p>一个实对称矩阵 <code>A</code>
被称为半正定矩阵，如果对于任意的非零实向量 <code>x</code> ，都有
<code>x^T Ax ≥ 0</code> 。</p>
<p><strong>性质</strong> ：</p>
<ol type="1">
<li>半正定矩阵的所有特征值都是非负的。</li>
<li>半正定矩阵的主子式都非负。</li>
<li>半正定矩阵与另一个半正定矩阵的和仍是半正定矩阵。</li>
</ol>
<p>在实际的机器学习和优化问题中，Hessian 矩阵具有重要的地位。</p>
<p><strong>计算方面</strong>： 计算 Hessian
矩阵可能是计算密集型的，特别是对于具有大量参数的模型。在深度学习中，直接计算完整的
Hessian
矩阵通常是不现实的。然而，可以使用近似方法或针对特定结构的模型进行简化计算。例如，对于一些具有简单结构的神经网络，可能通过一些技巧来估计
Hessian 矩阵的部分元素。</p>
<p><strong>应用方面</strong>：</p>
<ol type="1">
<li>优化算法：如牛顿法及其变体，利用 Hessian
矩阵来确定搜索方向和步长。相比于梯度下降法只依赖一阶导数（梯度），牛顿法考虑了二阶导数信息，能够在一些情况下更快地收敛到最优解。
<ul>
<li>举例来说，在求解一个二次函数的最小值时，牛顿法通过一次迭代就可以直接到达最小值点，因为它准确地利用了
Hessian 矩阵的信息。</li>
</ul></li>
<li>模型分析：帮助理解模型的性质和行为。通过分析 Hessian
矩阵的特征值和特征向量，可以了解模型在不同方向上的敏感度和曲率，从而洞察模型的稳定性和鲁棒性。
<ul>
<li>例如，在图像分类任务中，如果 Hessian
矩阵的某些特征值很大，说明模型在对应的特征方向上变化剧烈，可能对输入的微小变化非常敏感。</li>
</ul></li>
<li>正则化：可以用于设计一些基于二阶信息的正则化方法，以防止过拟合。
<ul>
<li>比如，通过对 Hessian
矩阵进行某种变换或约束，使得模型的复杂度得到控制。</li>
</ul></li>
</ol>
<p>总之，尽管在实际中直接处理 Hessian
矩阵存在困难，但通过巧妙的近似和应用，它仍然为解决机器学习和优化问题提供了有价值的见解和工具。</p>
<h3 id="梯度下降算法">梯度下降算法</h3>
<p>梯度下降算法是机器学习领域最广为人知、用途最广的优化算法，用来确定模型的参数（包括随机梯度下降SGD，Momentum，Adam等）。首先回顾一下梯度下降的计算过程：</p>
<h4 id="梯度下降中常用技巧tips">梯度下降中常用技巧（Tips）</h4>
<h5 id="一调整学习率">一、调整学习率</h5>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-28.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720543595377.png" alt="1720543595377">
<figcaption aria-hidden="true">1720543595377</figcaption>
</figure>
<p>所以，我们可以根据Loss曲线的变化情况，对我们的学习率进行一个合理的调整。可以绘制上图右边部分所示的曲线图，横轴代表参数更新次数，纵轴代表Loss值，学习率的大小分为四种情况：</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720543613512.png" alt="1720543613512">
<figcaption aria-hidden="true">1720543613512</figcaption>
</figure>
<p>在学习率的设置过程中，常见的做法是，进行
<strong><em>学习率的衰减</em></strong> 。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-29.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>在模型训练初期，距离全局最优点较远，可以设置一个相对大一些的学习率，随着训练的进行，距离全局最优点的距离越来越小，此时应该减小学习率；所以让学习率随着时间或者更新的次数进行衰减。</p>
<p>除了学习率的衰减之外，另外一个做法是：为不同的参数设置不同的学习率。也就是接下来要介绍的一种优化算法AdaGrad。</p>
<h6 id="adagrad优化器">AdaGrad优化器</h6>
<p>AdaGrad（Adaptive Gradient
Algorithm）是一种自适应学习率的方法，用于优化机器学习模型，特别是在处理稀疏数据和高维数据时表现出色。AdaGrad的主要特点是它为每个参数维护一个单独的学习率，并根据之前的梯度信息调整这些学习率。</p>
<p>AdaGrad根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题,即为每个参数设置不同的学习率。具体来说，对于每个参数
<strong>θ</strong>i，AdaGrad计算其所有历史梯度的平方和，并用这个和来缩放当前的梯度，从而得到更新步长。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-30.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720543759855.png" alt="1720543759855">
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-31.png" alt="alt text"></p>
<p>这里需要注意的一点为： <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-32.png" alt="alt text"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-33.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-34.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-35.png" alt="alt text">
而在AdaGrad中，正是这一思想的体现，不过为了减少计算量，增加运算速度，AdaGrad中使用过去一阶偏导数的均方根作为分母项（二阶偏导数）的近似。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-36.png" alt="alt text">
过去一阶偏导数的均方根可以在一定程度上反应二阶偏导的大小情况。如上图所示，二阶偏导小的函数，其采样的一阶偏导的值也相对较小。</p>
<p><strong>优势</strong></p>
<ol type="1">
<li><strong>自适应学习率</strong>
：不同参数有不同的学习率，能够更好地处理稀疏数据，使得更新幅度较大的参数的学习率更小，而更新幅度较小的参数的学习率更大。</li>
<li><strong>简化调参</strong>
：由于学习率是自适应的，通常不需要频繁调整学习率超参数。</li>
</ol>
<p><strong>局限性</strong></p>
<ol type="1">
<li><strong>学习率过小</strong>
：随着时间的推移，累积梯度平方和不断增大，导致学习率逐渐缩小，可能会导致算法在后期学习变得非常缓慢。</li>
<li><strong>不适用于所有问题</strong>
：虽然AdaGrad在处理稀疏数据上有优势，但对于某些问题，其性能可能不如其他优化算法，如RMSprop或Adam。</li>
</ol>
<p><strong>典型应用</strong></p>
<p>AdaGrad常用于处理自然语言处理中的词嵌入、推荐系统中的用户行为数据等高维、稀疏数据场景。</p>
<p>总结来说，AdaGrad通过自适应地调整每个参数的学习率，在处理稀疏数据和高维数据时提供了显著的优势，但其逐渐减小的学习率可能在某些情况下限制其性能。</p>
<h5 id="二随机梯度下降sgd">二、随机梯度下降（SGD）</h5>
<p>随机梯度下降（SGD, Stochastic Gradient
Descent）是一种用于优化机器学习模型参数的算法，特别适用于大规模数据集的训练。SGD与传统的批量梯度下降（Batch
Gradient
Descent）不同，它在每次迭代中仅使用一个样本或一个小批量的样本（mini-batch）来计算梯度和更新参数。这种方法具有较快的更新速度和更好的内存效率，特别是在处理大数据集时。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720599100421.png" alt="1720599100421">
<figcaption aria-hidden="true">1720599100421</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-37.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-38.png" alt="alt text">
之前梯度下降算法中，Loss函数是对所有训练样本的Loss之和，而在随机梯度下降中，每次只采样一个样本，根据这一个样本进行一次梯度下降，所以随机梯度下降算法更新参数的过程更快。</p>
<p>随机梯度下降（SGD, Stochastic Gradient
Descent）是一种用于优化机器学习模型参数的算法，特别适用于大规模数据集的训练。SGD与传统的批量梯度下降（Batch
Gradient
Descent）不同，它在每次迭代中仅使用一个样本或一个小批量的样本（mini-batch）来计算梯度和更新参数。这种方法具有较快的更新速度和更好的内存效率，特别是在处理大数据集时。</p>
<p><strong>优势</strong></p>
<ol type="1">
<li><strong>快速收敛</strong>：由于每次迭代只使用一个样本或小批量样本，更新频繁，收敛速度快。</li>
<li><strong>内存效率</strong>：每次只需要加载一个样本或小批量样本，内存占用低，适合处理大规模数据集。</li>
<li><strong>逃离局部最优</strong>：由于引入了随机性，SGD有助于跳出局部最优解，更容易找到全局最优解。</li>
</ol>
<p><strong>局限性</strong></p>
<ol type="1">
<li><strong>收敛波动</strong>：由于每次更新基于单个样本或小批量样本，导致参数更新不稳定，损失函数可能会剧烈波动。</li>
<li><strong>调参困难</strong>：学习率的选择对SGD的性能影响很大，通常需要进行超参数调优。</li>
</ol>
<p><strong>改进方法</strong></p>
<p>为了解决SGD的一些局限性，提出了多种改进算法，如：</p>
<ol type="1">
<li><strong>Mini-batch
SGD</strong>：在每次迭代中使用一个小批量样本，而不是单个样本，平衡了收敛速度和稳定性。</li>
<li><strong>动量（Momentum）</strong>：在参数更新时引入动量项，利用之前梯度的指数加权平均来加速收敛。</li>
<li><strong>RMSprop</strong>：自适应调整学习率，缓解学习率逐渐减小的问题。</li>
<li><strong>Adam</strong>：结合了动量和RMSprop的优点，自适应地调整学习率。</li>
</ol>
<p><strong>应用场景</strong></p>
<p>SGD广泛应用于深度学习和机器学习的各种模型训练中，包括神经网络、线性回归、逻辑回归等。它特别适用于大规模数据集和在线学习场景。</p>
<p>总结来说，SGD通过随机选择样本来进行参数更新，提供了快速且内存高效的优化方法，但其波动性和学习率调优是需要注意的问题。改进的变种算法如Mini-batch
SGD、动量、RMSprop和Adam在实践中被广泛采用，以提高SGD的性能和稳定性。</p>
<h5 id="三特征缩放feature-scaling">三、特征缩放（Feature Scaling）</h5>
<p>特征缩放是用来标准化数据特征的范围，减少特征中特异值的影响。</p>
<p>例如： <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-39.png" alt="alt text"></p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720544009279.png" alt="1720544009279">
<figcaption aria-hidden="true">1720544009279</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720544026157.png" alt="1720544026157">
<figcaption aria-hidden="true">1720544026157</figcaption>
</figure>
<h2 id="梯度上升和梯度下降的区别是什么">梯度上升和梯度下降的区别是什么？</h2>
<p><strong>!!!!!!!
简单来说就是正常的机器学习算法是需要计算loss函数的梯度,往梯度下降的方向走,需要loss函数的最小值</strong></p>
<pre><code>**但是梯度上升时,这个函数不是loss函数,而是一个真正的func函数,需要找这个func函数的最大值**</code></pre>
<p>梯度上升和梯度下降是两种相反的优化方法，主要区别如下：</p>
<p><strong>目标方向</strong>：</p>
<p>梯度下降的目标是找到函数的最小值，因此沿着函数梯度的负方向更新参数。</p>
<p>梯度上升的目标是找到函数的最大值，所以沿着函数梯度的正方向更新参数。</p>
<p><strong>更新参数的方式</strong>：</p>
<p>假设函数 <code>f</code> 关于参数 <code>w</code> 的梯度为
<code>∇f(w)</code> ，学习率为 <code>α</code> 。</p>
<p>在梯度下降中，参数的更新公式为：<code>w = w - α * ∇f(w)</code> 。</p>
<p>在梯度上升中，参数的更新公式为：<code>w = w + α * ∇f(w)</code> 。</p>
<p><strong>应用场景</strong>：</p>
<p>梯度下降常用于损失函数的最小化，比如在机器学习中，通过最小化预测值与真实值之间的差异来优化模型的参数。</p>
<p>梯度上升则常用于需要最大化某个目标函数的情况，比如强化学习中最大化奖励，或者在某些特定的优化问题中找到使某个函数达到最大值的参数配置。</p>
<p><strong>示例</strong>：</p>
<p>假设有一个简单的二次函数 <code>f(w) = w^2</code> ，其梯度为
<code>∇f(w) = 2w</code> 。</p>
<p>如果使用梯度下降，学习率为 <code>0.1</code> ，初始参数
<code>w = 2</code> ，则第一次更新为
<code>w = 2 - 0.1 * 2 * 2 = 1.6</code> 。</p>
<p>如果使用梯度上升，同样学习率为 <code>0.1</code> ，初始参数
<code>w = 2</code> ，则第一次更新为
<code>w = 2 + 0.1 * 2 * 2 = 2.4</code> 。</p>
<p>总的来说，梯度上升和梯度下降的核心区别在于更新参数的方向，一个朝着梯度正方向，一个朝着梯度负方向，以分别实现最大化和最小化的目标。</p>
<h2 id="偏差和方差">偏差和方差</h2>
<p>这篇博客介绍机器学习中误差（error）的来源，知道我们的模型中产生的误差来自于哪一部分，才能更好地进行模型的调整。一般来说，误差的来源有两部分：偏差（bias）和方差（variance）。偏差和方差——用来衡量模型泛化能力的工具，所以我的理解是在测试集上面根据偏差和方差来对模型进行一个评估。</p>
<p>回顾之前回归问题中的例子，简单模型对于数据的拟合能力比较差，在训练集和测试集上面效果均不好；但同时不是越复杂的模型越好，因为有可能产生过拟合的现象，所以需要选择合适的模型。偏差-方差分析可以帮我们诊断模型中存在的问题（过于复杂或者过于简单）。</p>
<p><strong>偏差就是：预测输出的期望值 -
真实值，（描述模型的拟合能力）</strong></p>
<p><strong>方差就是：（每个模型实例的预测输出 - 模型预测输出的期望值）^
2 （描述模型的稳定性，即受数据扰动的影响程度）</strong></p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-21.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>还是以宝可梦进化之后的CP值预测为例，如果我们有一些不同的训练数据（也就是李宏毅老师PPT中所说从若干个平行世界中收集的不同的宝可梦），</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-22.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>实质上是指有几个不同的训练集（TrainData_1，TrainData_2，TrainData_3），模型分别在不同的训练集上面训练，然后在同样的测试集（TestData）上面测试。对于不同的训练集，我们会得到一个模型的实例，比如有一次模型和五次模型，训练结果：(这里，“模型”表示具体的模型类别（比如一次模型，二次模型）；“模型实例”表示一个模型在不同训练集上面训练得到的最终模型，有几个训练集就会有几个模型实例。)
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-23.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-24.png" alt="alt text"></p>
<p>图片中，红色线表示在100个不同的训练集上面得到的模型的实例，蓝色线表示模型的预测输出的期望，黑色线是真实值。</p>
<p>可以看出，一次模型的偏差较大，方差较小；五次模型的偏差较小，方差较大。</p>
<h3 id="数学定义">数学定义</h3>
<p>上面是偏差和方差一个比较直观的理解，接下来给出数学形式的定义：</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720518028206.png" alt="1720518028206">
<figcaption aria-hidden="true">1720518028206</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720518369317.png" alt="1720518369317">
<figcaption aria-hidden="true">1720518369317</figcaption>
</figure>
<h3 id="偏差-方差与模型复杂度">偏差-方差与模型复杂度</h3>
<p>偏差和方差的几种情况： <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-25.png" alt="alt text"></p>
<p>偏差小，方差小：追求的目标，理想的模型。
偏差小，方差大：模型比较复杂，在训练集上面过拟合，导致在测试集上面泛化效果不好。
偏差大，方差小：模型比较简单，拟合能力较差，在训练集上面欠拟合，导致在测试集上面泛化效果不好。
偏差大，方差大：最糟的情况，模型需要重新进行设计，不适合于现有数据集。</p>
<p>很直观的一个解释，因为偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力，所以偏差大的模型拟合能力差，模型简单，容易欠拟合；方差度量数据扰动所造成的影响（在不同的训练集上面训练得到的模型在测试集上面效果表现相差很大），说明模型过于拟合训练集，模型复杂。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-26.png" alt="alt text"></p>
<h3 id="调整方法">调整方法</h3>
<p>综上，根据模型在测试集上的表现，可以得出结论：</p>
<blockquote>
<p>偏差大，方差小：模型欠拟合</p>
<p>偏差小，方差大：模型过拟合</p>
</blockquote>
<p>对于偏差大（欠拟合）的情况，常用的解决方法：</p>
<p>• 重新设计模型，使用更复杂的模型结构</p>
<p>• 输入中使用更多的特征</p>
<p>对于方差大（过拟合）的情况，常用的解决方法：</p>
<p>• 参数正则化（减小模型的复杂程度）</p>
<p>• 使用更多的训练数据</p>
<h3 id="k-折交叉验证">K-折交叉验证</h3>
<p>模型的设计选择需要在偏差和方差之间进行平衡，在选择合适的模型（比如一次模型还是二次模型）时，常用的方法是进行K-折交叉验证。将训练集的数据等分成K份，每次使用（K-1）份数据进行训练，余下的1份数据进行验证，进行K次，保证每份数据均做过验证集，统计K次验证集上面的loss，取loss均值最小的模型作为使用的模型。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-27.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<h2 id="softmax">softmax</h2>
<h3 id="主要用途">主要用途:</h3>
<p><strong>softmax是深度学习任务中常用于计算最终输出类别的函数</strong>。</p>
<p>Softmax
函数主要用于多分类问题中，将多个神经元的输出值转换为概率分布。</p>
<p>Softmax
函数在很多机器学习任务中都有广泛的应用，比如图像分类、文本分类等，它有助于将模型的输出转化为可解释的类别概率。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719916089305.png" alt="1719916089305">
<figcaption aria-hidden="true">1719916089305</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719916111218.png" alt="1719916111218">
<figcaption aria-hidden="true">1719916111218</figcaption>
</figure>
<ul>
<li>softmax和我们普通意义上的max函数不同，每一个元素都有一个概率，而不是其中一个元素为1，其余为0。它的含义是对于输入向量，有多大的概率去选择元素1、元素2、元素3等，主要的目的是使得概率计算过程可导。</li>
</ul>
<p>以下是对 Softmax 函数的一些关键理解点：</p>
<ol type="1">
<li>输出值在 0 到 1 之间：Softmax 函数确保每个输出值都在 0 和 1
之间。</li>
<li>输出值总和为 1：所有输出值的总和为
1，这使得它们可以被解释为概率。</li>
<li>强调相对大小：Softmax 函数会放大输入值之间的差异。较大的输入值在经过
Softmax
计算后会得到更大的概率值，较小的输入值则会得到较小的概率值。</li>
</ol>
<p>例如，假设有一个神经网络的输出为 <code>[1, 2, 0]</code>，经过 Softmax
函数计算后，得到的概率分布可能是
<code>[0.269, 0.731, 0.0]</code>。这意味着模型预测第二个类别是最有可能的，第一个类别有一定的可能性，而第三个类别几乎不可能。</p>
<h2 id="全连接层fully-connected-layer">全连接层（Fully Connected
Layer）</h2>
<p>全连接层（Fully Connected Layer），也称为密集层（Dense
Layer），是一种在深度学习中广泛使用的神经网络层。</p>
<p>全连接层在整个网络卷积神经网络中起到<strong>“分类器”</strong>的作用。如果说卷积层、池化层和激活函数等操作是将原始数据映射到隐层特征空间的话，
<strong>全连接层则起到将学到的特征表示映射到样本的标记空间的作用</strong>
。</p>
<p><strong>在 CNN
中，全连接常出现在最后几层，用于对前面设计的特征做加权和</strong>。比如
mnist，前面的卷积和池化相当于做特征工程，后面的全连接相当于做特征加权。（卷积相当于全连接的有意弱化，按照局部视野的启发，把局部之外的弱影响直接抹为零影响；还做了一点强制，不同的局部所使用的参数居然一致。弱化使参数变少，节省计算量，又专攻局部不贪多求全；强制进一步减少参数。少即是多）
<strong>在 RNN 中，全连接用来把 embedding
空间拉到隐层空间，把隐层空间转回 label 空间等。</strong></p>
<p>一段来自知乎的通俗理解：</p>
<p>　　从卷积网络谈起，卷积网络在形式上有一点点像咱们正在召开的“人民代表大会”。卷积核的个数相当于候选人，图像中不同的特征会激活不同的“候选人”（卷积核）。池化层（仅指最大池化）起着类似于“合票”的作用，不同特征在对不同的“候选人”有着各自的喜好。</p>
<p>　　全连接相当于是“代表普选”。所有被各个区域选出的代表，对最终结果进行“投票”，全连接保证了receiptive
field
是整个图像，既图像中各个部分（所谓所有代表），都有对最终结果影响的权利。</p>
<p>　　 <strong>在实际使用中，全连接层可由卷积操作实现</strong>
：对前层是全连接的全连接层可以转换为卷积核为1*1的卷积；而前层是卷积层的全连接层可以转换为卷积核为前层卷积输出结果的高和宽一样大小的全局卷积。</p>
<p><img src="https://img2018.cnblogs.com/blog/991470/201902/991470-20190209123250272-1769907112.png"></p>
<p>最后的两列小圆球就是两个全连接层，在最后一层卷积结束后，进行了最后一次池化，输出了20个12*12的图像，然后通过了一个全连接层变成了1*100的向量。</p>
<p>　　这是怎么做到的呢，其实就是 有20*100个12*12的不同卷积核卷积出来的
，我们也可以这样想，就是
每个神经元的输出是12*12*20个输入值与对应的权值乘积的和 。对于输入的
<strong>每一张图，用了一个和图像一样大小的核卷积，这样整幅图就变成了一个数了</strong>
，如果厚度是20就是那20个核卷积完了之后相加求和。这样就能<strong>把一张图高度浓缩成一个数</strong>了。</p>
<p><strong>一、结构与原理</strong></p>
<p>全连接层中的每个神经元都与上一层的所有神经元相连接。这意味着如果上一层有(n)个神经元，那么全连接层中的每个神经元都有(n)个输入权重。</p>
<p>在全连接层中，输入数据通常是一个一维向量，它是通过将上一层的多维特征图展平得到的。每个神经元对输入向量进行加权求和，并通过一个激活函数得到输出。</p>
<p>数学表达式可以表示为：(y = f(Wx +
b))，其中(x)是输入向量，(W)是权重矩阵，(b)是偏置项，(f())是激活函数，(y)是输出。</p>
<p><strong>它是怎么样把3x3x5的输出，转换成<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=1x4096&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A33841176%7D">1x4096</a>的形式</strong></p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-44.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>很简单,可以理解为在中间做了一个卷积 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-45.png" alt="alt text"></p>
<p>从上图我们可以看出，我们用一个3x3x5的filter
去卷积激活函数的输出，得到的结果就是一个fully connected layer
的一个神经元的输出，这个输出就是一个值</p>
<p><strong>因为我们有4096个神经元</strong></p>
<p><strong>我们实际就是用一个3x3x5x4096的卷积层去卷积激活函数的输出</strong></p>
<p>以<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=VGG-16&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A33841176%7D">VGG-16</a>再举个例子吧</p>
<p>再VGG-16全连接层中</p>
<p>对224x224x3的输入，最后一层卷积可得输出为7x7x512，如后层是一层含4096个神经元的FC，则可用<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E5%8D%B7%E7%A7%AF%E6%A0%B8&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A33841176%7D">卷积核</a>为7x7x512x4096的全局卷积来实现这一全连接运算过程。</p>
<p>很多人看到这，可能就恍然大悟</p>
<p>哦，我懂了，就是做个卷积呗</p>
<p>不</p>
<p>你不懂</p>
<p>这一步卷积一个非常重要的作用</p>
<p>就是把分布式特征representation映射到样本标记空间</p>
<p>什么，听不懂</p>
<p>那我说人话</p>
<p><strong>就是它把特征representation整合到一起，输出为一个值</strong></p>
<p><strong>这样做,有一个什么好处？</strong></p>
<p><strong>就是大大减少特征位置对分类带来的影响</strong></p>
<p>来，让我来举个简单的例子 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-46.png" alt="alt text"></p>
<p>这个例子可能过于简单了点</p>
<p>可是我懒得画了，大家将就着看吧</p>
<p>从上图我们可以看出，猫在不同的位置，输出的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=feature%E5%80%BC&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A33841176%7D">feature值</a>相同，但是位置不同</p>
<p>对于电脑来说，<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E7%89%B9%E5%BE%81%E5%80%BC&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A33841176%7D">特征值</a>相同，但是特征值位置不同，那分类结果也可能不一样</p>
<p>而这时全连接层filter的作用就相当于</p>
<p><strong>喵在哪我不管</strong></p>
<p><strong>我只要喵</strong></p>
<p><strong>于是我让filter去把这个喵找到</strong></p>
<p><strong>实际就是把<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=feature%20map&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A33841176%7D">feature
map</a> 整合成一个值</strong></p>
<p><strong>这个值大</strong></p>
<p><strong>哦，有喵</strong></p>
<p><strong>这个值小</strong></p>
<p><strong>那就可能没喵</strong></p>
<p><strong>和这个喵在哪关系不大了有没有</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E9%B2%81%E6%A3%92%E6%80%A7&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A33841176%7D">鲁棒性</a>有大大增强了有没有</strong></p>
<p><strong>喵喵喵</strong></p>
<p><strong>因为空间结构特性被忽略了，所以全连接层不适合用于在方位上找Pattern的任务，比如<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=segmentation&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A33841176%7D">segmentation</a></strong></p>
<p>ok, 我们突然发现全连接层有两层1x4096fully connected
layer平铺结构(有些网络结构有一层的，或者二层以上的)</p>
<p>好吧也不是突然发现，我只是想增加一点戏剧效果 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-47.png" alt="alt text"></p>
<p><strong>但是大部分是两层以上呢</strong></p>
<p><strong>这是为啥子呢</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A33841176%7D">泰勒公式</a>都知道吧</p>
<p>意思就是用多项式函数去拟合<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E5%85%89%E6%BB%91%E5%87%BD%E6%95%B0&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A33841176%7D">光滑函数</a></p>
<p>我们这里的全连接层中一层的一个神经元就可以看成一个多项式</p>
<p>我们用许多神经元去拟合<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A33841176%7D">数据分布</a></p>
<p>但是只用一层fully connected layer 有时候没法解决<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E9%9D%9E%E7%BA%BF%E6%80%A7&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A33841176%7D">非线性</a>问题</p>
<p><strong>而如果有两层或以上fully connected
layer就可以很好地解决非线性问题了</strong></p>
<p>说了这么多，我猜你应该懂的</p>
<p>听不懂？</p>
<p>那我换个方式给你讲</p>
<p><strong>我们都知道，全连接层之前的作用是提取特征</strong></p>
<p><strong><a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E5%85%A8%E7%90%86%E8%A7%A3%E5%B1%82&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A33841176%7D">全理解层</a>的作用是分类</strong></p>
<p>我们现在的任务是去区别一图片是不是猫</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-48.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>假设这个神经<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A33841176%7D">网络模型</a>已经训练完了</p>
<p>全连接层已经知道</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-49.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>当我们得到以上特征，我就可以判断这个东东是猫了</p>
<p>因为全连接层的作用主要就是实现分类（Classification）</p>
<p>从下图，我们可以看出 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-50.png" alt="alt text"></p>
<p><strong>红色的神经元表示这个特征被找到了（激活了）</strong></p>
<p><strong>同一层的其他神经元，要么猫的特征不明显，要么没找到</strong></p>
<p>当我们把这些找到的特征组合在一起，发现最符合要求的是猫</p>
<p>ok，我认为这是猫了</p>
<p>那我们现在往前走一层</p>
<p><strong>那们现在要对子特征分类，也就是对猫头，猫尾巴，猫腿等进行分类</strong></p>
<p>比如我们现在要把猫头找出来 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-51.png" alt="alt text"></p>
<p>猫头有这么些个特征</p>
<p>于是我们下一步的任务</p>
<p>就是把猫头的这么些子特征找到，比如眼睛啊，耳朵啊 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-52.png" alt="alt text"></p>
<p>道理和区别猫一样</p>
<p><strong>当我们找到这些特征，神经元就被激活了（上图红色圆圈）</strong></p>
<p>这细节特征又是怎么来的？</p>
<p>就是从前面的卷积层，<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E4%B8%8B%E9%87%87%E6%A0%B7&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A33841176%7D">下采样</a>层来的</p>
<p>至此，关于全连接层的信息就简单介绍完了</p>
<p>全连接层参数特多（可占整个网络参数80%左右），近期一些性能优异的网络模型如ResNet和GoogLeNet等均用全局平均池化（global
average pooling，GAP）取代全连接层来融合学到的深度特征</p>
<p>需要指出的是，<strong>用GAP替代FC的网络通常有较好的预测性能</strong></p>
<p>于是还出现了</p>
<p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1411.4038">[1411.4038]
Fully Convolutional Networks for Semantic
Segmentation<strong>arxiv.org/abs/1411.4038</strong></a></p>
<p>以后会慢慢介绍的</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p><strong>二、作用</strong></p>
<ol type="1">
<li>特征融合与变换
<ul>
<li>全连接层可以将上一层学习到的局部特征进行融合和变换，提取更高级别的全局特征。通过调整权重和偏置，全连接层可以学习到不同特征之间的复杂关系。</li>
</ul></li>
<li>分类与回归
<ul>
<li>在神经网络的最后几层，全连接层通常用于分类或回归任务。通过将学习到的特征映射到输出类别或数值，全连接层可以对输入数据进行预测。</li>
</ul></li>
</ol>
<p><strong>三、特点</strong></p>
<ol type="1">
<li><strong>参数数量多</strong>
<ul>
<li>由于全连接层中的每个神经元都与上一层的所有神经元相连接，因此全连接层通常具有大量的参数。这使得全连接层在训练过程中需要大量的计算资源和数据，并且容易过拟合。</li>
</ul></li>
<li>可解释性差
<ul>
<li>全连接层中的权重和偏置通常难以解释，因为它们是通过复杂的优化过程学习得到的。这使得全连接层在一些需要可解释性的应用中受到限制。</li>
</ul></li>
</ol>
<p><strong>四、应用场景</strong></p>
<ol type="1">
<li>图像分类
<ul>
<li>在图像分类任务中，全连接层通常用于将卷积神经网络学习到的图像特征映射到不同的类别。例如，在ResNet等经典的图像分类网络中，<strong>最后几层</strong>通常是全连接层。</li>
</ul></li>
<li>自然语言处理
<ul>
<li>在自然语言处理任务中，全连接层可以用于将词向量或句子向量映射到不同的语义类别或情感标签。例如，在文本分类、情感分析等任务中，全连接层通常是<strong>神经网络的最后几层</strong>。</li>
</ul></li>
<li>语音识别
<ul>
<li>在语音识别任务中，全连接层可以用于将声学特征映射到不同的音素或单词。例如，在深度神经网络语音识别系统中，全连接层通常用于将学习到的声学特征进行<strong>分类</strong>。</li>
</ul></li>
</ol>
<h2 id="嵌入embedding层的理解"><a target="_blank" rel="noopener" href="https://www.cnblogs.com/USTC-ZCC/p/11068791.html" title="发布于 2019-06-22 14:40">嵌入(embedding)层的理解</a></h2>
<p><strong>embedding层作用：①降维②对低维的数据进行升维时，可能把一些其他特征给放大了，或者把笼统的特征给分开了。</strong></p>
<p><strong>Embedding其实就是一个映射，从原先所属的空间映射到新的多维空间中，也就是把原先所在空间嵌入到一个新的空间中去。</strong></p>
<h3 id="嵌入层的一个作用降维">嵌入层的一个作用——降维</h3>
<p>首先，我们有一个one-hot编码的概念。</p>
<p>假设，我们中文，一共只有10个字。。。只是假设啊，那么我们用0-9就可以表示完</p>
<p>比如，这十个字就是“我从哪里来，要到何处去”</p>
<p>其分别对应“0-9”，如下：</p>
<p>我 从 哪 里 来 要 到 何 处 去</p>
<p>0 1 2 3 4 5 6 7 8 9</p>
<p>那么，其实我们只用一个列表就能表示所有的对话</p>
<p>如：我 从 哪 里 来 要 到 何 处 去 ——&gt;&gt;&gt;[0 1 2 3 4 5 6 7 8
9]</p>
<p>或：我 从 何 处 来 要 到 哪 里 去 ——&gt;&gt;&gt;[0 1 7 8 4 5 6 2 3
9]</p>
<p>但是，我们看看one-hot编码方式（详见：https://blog.csdn.net/tengyuan93/article/details/78930285）</p>
<p>他把上面的编码方式弄成这样</p>
<hr>
<p>--我从哪里来，要到何处去</p>
<p>[ [1 0 0 0 0 0 0 0 0 0] [0 1 0 0 0 0 0 0 0 0] [0 0 1 0 0 0 0 0 0 0]
[0 0 0 1 0 0 0 0 0 0] [0 0 0 0 1 0 0 0 0 0] [0 0 0 0 0 1 0 0 0 0] [0 0 0
0 0 0 1 0 0 0] [0 0 0 0 0 0 0 1 0 0] [0 0 0 0 0 0 0 0 1 0] [0 0 0 0 0 0
0 0 0 1]]</p>
<hr>
<p>--我从何处来，要到哪里去</p>
<p>[ [1 0 0 0 0 0 0 0 0 0] [0 1 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 1 0 0]
[0 0 0 0 0 0 0 0 1 0] [0 0 0 0 1 0 0 0 0 0] [0 0 0 0 0 1 0 0 0 0] [0 0 0
0 0 0 1 0 0 0] [0 0 1 0 0 0 0 0 0 0] [0 0 0 1 0 0 0 0 0 0] [0 0 0 0 0 0
0 0 0 1]]
即：把每一个字都对应成一个十个（样本总数/字总数）元素的数组/列表，其中每一个字都用唯一对应的数组/列表对应，数组/列表的唯一性用1表示。如上，“我”表示成[1。。。。]，“去”表示成[。。。。1]，这样就把每一系列的文本整合成一个稀疏矩阵。</p>
<p>那问题来了，稀疏矩阵（二维）和列表（一维）相比，有什么优势。</p>
<p>很明显，计算简单嘛，稀疏矩阵做矩阵计算的时候，只需要把1对应位置的数相乘求和就行，也许你心算都能算出来；而一维列表，你能很快算出来？何况这个列表还是一行，如果是100行、1000行和或1000列呢？</p>
<p>所以，one-hot编码的优势就体现出来了，计算方便快捷、表达能力强。</p>
<p>然而，缺点也随着来了。</p>
<p>比如：中文大大小小简体繁体常用不常用有十几万，然后一篇文章100W字，你要表示成100W
X 10W的矩阵？？？</p>
<p>这是它最明显的缺点。过于稀疏时，过度占用资源。</p>
<p>比如：其实我们这篇文章，虽然100W字，但是其实我们整合起来，有99W字是重复的，只有1W字是完全不重复的。那我们用100W
X 10W的岂不是白白浪费了99W X 10W的矩阵存储空间。</p>
<p>那怎么办？？？</p>
<p>这时，Embedding层横空出世。</p>
<p>接下来给大家看一张图 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-42.png" alt="alt text"></p>
<p>假设：我们有一个2 x 6的矩阵，然后乘上一个6 x 3的矩阵后，变成了一个2 x
3的矩阵。</p>
<p>先不管它什么意思，这个过程，我们把一个12个元素的矩阵变成6个元素的矩阵，直观上，大小是不是缩小了一半？</p>
<p>也许你已经想到了！！！对！！！不管你想的对不对，但是embedding层，在某种程度上，就是用来降维的，降维的原理就是矩阵乘法。在卷积网络中，可以理解为特殊全连接层操作，跟1x1卷积核异曲同工！！！484很神奇！！！</p>
<p>也就是说，假如我们有一个100W X10W的矩阵，用它乘上一个10W X
20的矩阵，我们可以把它降到100W X
20，瞬间量级降了。。。10W/20=5000倍！！！</p>
<p>这就是嵌入层的一个作用——降维。</p>
<p>然后<strong>中间那个10W X
20的矩阵</strong>，可以理解为<strong>查询表，也可以理解为映射表</strong>，也可以理解为<strong>过度表</strong>，whatever。</p>
<h3 id="嵌入层的一个作用升维">嵌入层的一个作用——升维</h3>
<p>接着，既然可以降维，当然也可以升维。为什么要升维？------------可能把一些其他特征给放大了，或者把笼统的特征给分开了
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-43.png" alt="alt text"></p>
<p>这张图，我要你<strong>在10米开外</strong>找出五处不同！。。。What？烦请出题者走近两步，我先把我的刀拿出来，您再说一遍题目我没听清。</p>
<p>当然，目测这是不可能完成的。但是我让你<strong>在一米外</strong>，也许你一瞬间就发现衣服上有个心是不同的，然后再<strong>走近半米</strong>，你又发现左上角和右上角也是不同的。再<strong>走近20厘米</strong>，又发现耳朵也不同，最后，在<strong>距离屏幕10厘米的地方</strong>，终于发现第五个不同的地方在耳朵下面一点的云。</p>
<p>但是，其实无限靠近并不代表认知度就高了，比如，你只能距离屏幕1厘米远的地方找，找出五处不同。。。出题人你是不是脑袋被门挤了。。。</p>
<p>由此可见，<strong>距离的远近会影响我们的观察效果</strong>。同理也是一样的，</p>
<p><strong>低维的数据可能包含的特征是非常笼统的，我们需要不停地拉近拉远来改变我们的感受野，让我们对这幅图有不同的观察点，找出我们要的茬。</strong></p>
<p>embedding的又一个作用体现了。<strong>对低维的数据进行升维时，可能把一些其他特征给放大了，或者把笼统的特征给分开了</strong>。同时，这个embedding是一直在学习在优化的，就使得整个拉近拉远的过程<strong>慢慢形成一个良好的观察点</strong>。比如：我来回靠近和远离屏幕，发现45厘米是最佳观测点，这个距离能10秒就把5个不同点找出来了。</p>
<p><strong>回想一下为什么CNN层数越深准确率越高，卷积层卷了又卷，池化层池了又升，升了又降，全连接层连了又连。因为我们也不知道它什么时候突然就学到了某个有用特征</strong>。但是不管怎样，学习都是好事，所以让机器多卷一卷，多连一连，反正错了多少我会用交叉熵告诉你，怎么做才是对的我会用梯度下降算法告诉你，只要给你时间，你迟早会学懂。因此，<strong>理论上，只要层数深，只要参数足够，NN能拟合任何特征</strong>。总之，<strong>它类似于虚拟出一个关系对当前数据进行映射</strong>。这个东西也许一言难尽吧，但是目前各位只需要知道它有这些功能的就行了。</p>
<p>接下来，继续假设我们有一句话，叫“公主很漂亮”，如果我们使用one-hot编码，可能得到的编码如下：</p>
<p>公 [0 0 0 0 1] 主 [0 0 0 1 0] 很 [0 0 1 0 0] 漂 [0 1 0 0 0] 亮 [1 0 0
0 0]
乍一眼看过似乎没毛病，其实本来人家也没毛病，或者假设咱们的词袋更大一些</p>
<p>公 [0 0 0 0 1 0 0 0 0 0] 主 [0 0 0 1 0 0 0 0 0 0] 很 [0 0 1 0 0 0 0 0
0 0] 漂 [0 1 0 0 0 0 0 0 0 0] 亮 [1 0 0 0 0 0 0 0 0 0]
假设吧，就假设咱们的词袋一共就10个字，则这一句话的编码如上所示。</p>
<p>这样的编码，<strong>最大的好处就是，不管你是什么字，我们都能在一个一维的数组里用01给你表示出来。并且不同的字绝对不一样，以致于一点重复都没有，表达本征的能力极强</strong>。</p>
<p><strong>(缺陷)但是，因为其完全独立，其劣势就出来了。表达关联特征的能力几乎为0！！</strong>！</p>
<p>我给你举个例子，我们又有一句话“王妃很漂亮”</p>
<p>那么在这基础上，我们可以把这句话表示为</p>
<p>王 [0 0 0 0 0 0 0 0 0 1] 妃 [0 0 0 0 0 0 0 0 1 0] 很 [0 0 1 0 0 0 0 0
0 0] 漂 [0 1 0 0 0 0 0 0 0 0] 亮 [1 0 0 0 0 0 0 0 0 0]
<strong>从中文表示来看，我们一下就跟感觉到，王妃跟公主其实是有很大关系的</strong>，比如：公主是皇帝的女儿，王妃是皇帝的妃子，<strong>可以从“皇帝”这个词进行关联上</strong>；公主住在宫里，王妃住在宫里，<strong>可以从“宫里”这个词关联上</strong>；公主是女的，王妃也是女的，<strong>可以从“女”这个字关联上</strong>。</p>
<p>但是呢，我们用了one-hot编码，公主和王妃就变成了这样：</p>
<p>公 [0 0 0 0 1 0 0 0 0 0] 主 [0 0 0 1 0 0 0 0 0 0] 王 [0 0 0 0 0 0 0 0
0 1] 妃 [0 0 0 0 0 0 0 0 1 0]
<strong>你说，你要是不看前面的中文注解，你知道这四行向量有什么内部关系吗？看不出来，那怎么办？</strong></p>
<p>既然，通过刚才的假设关联，我们关联出了“皇帝”、“宫里”和“女”三个词，那我们尝试这么去定义公主和王妃</p>
<p>公主一定是皇帝的女儿，我们假设她跟皇帝的关系相似度为1.0；公主从一出生就住在宫里，直到20岁才嫁到府上，活了80岁，我们假设她跟宫里的关系相似度为0.25；公主一定是女的，跟女的关系相似度为1.0；</p>
<p>王妃是皇帝的妃子，没有亲缘关系，但是有存在着某种关系，我们就假设她跟皇帝的关系相似度为0.6吧；妃子从20岁就住在宫里，活了80岁，我们假设她跟宫里的关系相似度为0.75；王妃一定是女的，跟女的关系相似度为1.0；</p>
<p>于是公主王妃四个字我们可以这么表示：</p>
<p>皇 宫 帝 里 女 公主 [ 1.0 0.25 1.0] 王妃 [ 0.6 0.75 1.0]
<strong>这样我们就把公主和王妃两个词，跟皇帝、宫里、女这几个字（特征）关联起来了，我们可以认为：</strong></p>
<p>**公主=1.0 <em>皇帝 +0.25</em>宫里 +1.0*女**</p>
<p>**王妃=0.6 <em>皇帝 +0.75</em>宫里 +1.0*女**</p>
<p>或者这样，我们假设没歌词的每个字都是对等（注意：只是假设，为了方便解释）：</p>
<p>皇 宫 帝 里 女 公 [ 0.5 0.125 0.5] 主 [ 0.5 0.125 0.5] 王 [ 0.3 0.375
0.5] 妃 [ 0.3 0.375 0.5]
<strong>这样，我们就把一些词甚至一个字，用三个特征给表征出来了</strong>。然后，<strong>我们把皇帝叫做特征（1），宫里叫做特征（2），女叫做特征（3），</strong>于是乎，我们就得出了公主和王妃的<strong>隐含特征关系</strong>：</p>
<p><strong>王妃=公主的特征（1） * 0.6 +公主的特征（2） * 3
+公主的特征（3） * 1</strong></p>
<p><strong>于是乎，我们把文字的one-hot编码，从稀疏态变成了密集态，并且让相互独立向量变成了有内在联系的关系向量。</strong></p>
<p>所以，embedding层做了个什么呢？它<strong>把我们的稀疏矩阵，通过一些线性变换</strong>（在CNN中用<strong>全连接层</strong>进行转换，也称为<strong>查表操作</strong>），变成了一个<strong>密集矩阵</strong>，<strong>这个密集矩阵用了N（例子中N=3）个特征来表征所有的文字，在这个密集矩阵中，表象上代表着密集矩阵跟单个字的一一对应关系</strong>，<strong>实际上还蕴含了大量的字与字之间，词与词之间甚至句子与句子之间的内在关系</strong>（如：我们得出的王妃跟公主的关系）。他们之间的关系，用的是嵌入层学习来的参数进行表征。</p>
<p><strong>从稀疏矩阵到密集矩阵的过程</strong>，叫做<strong>embedding</strong>，很多人也把它叫做<strong>查表</strong>，因为他们之间也是一个一一映射的关系。</p>
<p><strong>更重要的是，这种关系在反向传播的过程中，是一直在更新的，因此能在多次epoch后，使得这个关系变成相对成熟</strong>，即：正确的表达整个语义以及各个语句之间的关系。这个成熟的关系，就是embedding层的所有权重参数。</p>
<p>Embedding是NPL领域最重要的发明之一，他把独立的向量一下子就关联起来了。这就相当于什么呢，相当于你是你爸的儿子，你爸是A的同事，B是A的儿子，似乎跟你是八竿子才打得着的关系。结果你一看B，是你的同桌。Embedding层就是用来发现这个秘密的武器。</p>
<h2 id="池化层">池化层</h2>
<p>池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合</p>
<p>简而言之，如果输入是图像的话，那么池化层的最主要作用就是压缩图像</p>
<h3 id="池化层的作用">池化层的作用</h3>
<p>主要是两个作用：</p>
<ol type="1">
<li>invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)</li>
<li>保留主要的特征同时减少参数(降维，效果类似PCA)和计算量，防止过拟合，提高模型泛化能力</li>
</ol>
<p>A:
特征不变性，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。</p>
<p>B.
特征降维，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来，这也是池化操作的一大作用</p>
<ol type="1">
<li><p>translation invariance：
这里举一个直观的例子(数字识别)，假设有一个16x16的图片，里面有个数字1，我们需要识别出来，这个数字1可能写的偏左一点(图1)，这个数字1可能偏右一点(图2)，图1到图2相当于向右平移了一个单位，但是图1和图2经过max
pooling之后它们都变成了相同的8x8特征矩阵，主要的特征我们捕获到了，同时又将问题的规模从16x16降到了8x8，而且具有平移不变性的特点。图中的a（或b）表示，在原始图片中的这些a（或b）位置，最终都会映射到相同的位置。</p></li>
<li><p>rotation invariance：
下图表示汉字“一”的识别，第一张相对于x轴有倾斜角，第二张是平行于x轴，两张图片相当于做了旋转，经过多次max
pooling后具有相同的特征 ————————————————</p>
<p>版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA
版权协议，转载请附上原文出处链接和本声明。</p></li>
</ol>
<p>原文链接：https://blog.csdn.net/weixin_38145317/article/details/89310404</p>
<p><strong>池化层用的方法有Max pooling 和 average pooling</strong></p>
<h3 id="max-pooling">Max pooling:</h3>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-40.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-41.png" alt="alt text"></p>
<p>对于每个2<em>2的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个2</em>2窗口中最大的数是6，那么输出矩阵的第一个元素就是6，如此类推。</p>
<h2 id="逻辑回归">逻辑回归</h2>
<p><a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/2.Logistics%20Regression#1-%E4%BB%80%E4%B9%88%E6%98%AF%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"></a></p>
<h3 id="什么是逻辑回归">1. 什么是逻辑回归</h3>
<p>逻辑回归是用来做分类算法的，大家都熟悉线性回归，一般形式是Y=aX+b，y的取值范围是[-∞,
+∞]，有这么多取值，怎么进行分类呢？不用担心，伟大的数学家已经为我们找到了一个方法。</p>
<p>也就是把Y的结果带入一个非线性变换的<strong>Sigmoid函数</strong>中，即可得到[0,1]之间取值范围的数S，S可以把它看成是一个概率值，如果我们设置概率阈值为0.5，那么S大于0.5可以看成是正样本，小于0.5看成是负样本，就可以进行分类了。</p>
<h3 id="什么是sigmoid函数">2. 什么是Sigmoid函数</h3>
<p>函数公式如下：</p>
<figure>
<img src="https://camo.githubusercontent.com/73bf196741b6133e56170bd6196d3dba2827cd2e3280d0e78bcf1f2889bf5cc5/68747470733a2f2f7778342e73696e61696d672e636e2f6c617267652f30303633304465666c7931673470766b32637461746a333063773062363379712e6a7067" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<p>函数中t无论取什么值，其结果都在[0,-1]的区间内，回想一下，一个分类问题就有两种答案，一种是“是”，一种是“否”，那0对应着“否”，1对应着“是”，那又有人问了，你这不是[0,1]的区间吗，怎么会只有0和1呢？这个问题问得好，我们假设分类的<strong>阈值</strong>是0.5，那么超过0.5的归为1分类，低于0.5的归为0分类，阈值是可以自己设定的。</p>
<p>好了，接下来我们把aX+b带入t中就得到了我们的逻辑回归的一般模型方程：</p>
<p><img src="https://camo.githubusercontent.com/e5f521c76bc20fcfe2eb1ea1513c28f08d30417ebe779d9b0a05fc514cdc0b74/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f4828612c62293d2535436672616325374231253744253742312b652535452537422861582b6229253744253744"></p>
<p>结果P也可以理解为概率，换句话说概率大于0.5的属于1分类，概率小于0.5的属于0分类，这就达到了分类的目的。</p>
<h3 id="损失函数是什么">3. 损失函数是什么</h3>
<p>逻辑回归的损失函数是 <strong>log loss</strong> ，也就是
<strong>对数似然函数</strong> ，函数公式如下：</p>
<figure>
<img src="https://camo.githubusercontent.com/0f83027d44496a9fb9de260dfd72167a89b5c66dd02563ceda33ec2405ec0e2d/68747470733a2f2f7778312e73696e61696d672e636e2f6c617267652f30303633304465666c793167347076747a337477396a333065743034763073772e6a7067" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<p>公式中的 y=1 表示的是真实值为1时用第一个公式，真实 y=0
用第二个公式计算损失。为什么要加上log函数呢？可以试想一下，当真实样本为1是，但h=0概率，那么log0=∞，这就对模型最大的惩罚力度；当h=1时，那么log1=0，相当于没有惩罚，也就是没有损失，达到最优结果。所以数学家就想出了用log函数来表示损失函数。</p>
<p>最后按照梯度下降法一样，求解极小值点，得到想要的模型效果。</p>
<h3 id="可以进行多分类吗">4.可以进行多分类吗？</h3>
<p>可以的，其实我们可以从二分类问题过度到多分类问题(one vs
rest)，思路步骤如下：</p>
<p>1.将类型class1看作正样本，其他类型全部看作负样本，然后我们就可以得到样本标记类型为该类型的概率p1。</p>
<p>2.然后再将另外类型class2看作正样本，其他类型全部看作负样本，同理得到p2。</p>
<p>3.以此循环，我们可以得到该待预测样本的标记类型分别为类型class
i时的概率pi，最后我们<strong>取pi中最大的那个概率对应的样本标记类型作为我们的待预测样本类型</strong>。</p>
<figure>
<img src="https://camo.githubusercontent.com/7d1db9dafb25ace5cf38c61b51be37b119420900bc30868db18ce15f7c388bc2/68747470733a2f2f7778322e73696e61696d672e636e2f6c617267652f30303633304465666c7931673470773131666f31746a3330637630633530746a2e6a7067" alt="image">
<figcaption aria-hidden="true">image</figcaption>
</figure>
<p>总之还是以二分类来依次划分，并求出最大概率结果.</p>
<h3 id="逻辑回归有什么优点">5.逻辑回归有什么优点</h3>
<ul>
<li>LR能以概率的形式输出结果，而非只是0,1判定。</li>
<li>LR的可解释性强，可控度高(你要给老板讲的嘛…)。</li>
<li>训练快，feature engineering之后效果赞。</li>
<li>因为结果是概率，可以做ranking model。</li>
</ul>
<h3 id="逻辑回归有哪些应用">6. 逻辑回归有哪些应用</h3>
<ul>
<li>CTR预估/推荐系统的learning to rank/各种分类场景。</li>
<li>某搜索引擎厂的广告CTR预估基线版是LR。</li>
<li>某电商搜索排序/广告CTR预估基线版是LR。</li>
<li>某电商的购物搭配推荐用了大量LR。</li>
<li>某现在一天广告赚1000w+的新闻app排序基线是LR。</li>
</ul>
<h3 id="逻辑回归常用的优化方法有哪些">7.
逻辑回归常用的优化方法有哪些</h3>
<h4 id="一阶方法">7.1 一阶方法</h4>
<p>梯度下降、随机梯度下降、mini
随机梯度下降降法。随机梯度下降不但速度上比原始梯度下降要快，局部最优化问题时可以一定程度上抑制局部最优解的发生。</p>
<h4 id="二阶方法牛顿法拟牛顿法">7.2 二阶方法：牛顿法、拟牛顿法：</h4>
<p>这里详细说一下<strong>牛顿法</strong>的基本原理和牛顿法的应用方式。<strong>牛顿法其实就是通过切线与x轴的交点不断更新切线的位置，直到达到曲线与x轴的交点得到方程解</strong>。</p>
<p>在实际应用中我们因为常常要求解凸优化问题，也就是要求解函数一阶导数为0的位置，而牛顿法恰好可以给这种问题提供解决方法。实际应用中牛顿法首先选择一个点作为起始点，并进行一次二阶泰勒展开得到导数为0的点进行一个更新，直到达到要求，这时牛顿法也就成了<strong>二阶求解问题</strong>，<strong>比一阶方法更快</strong>。我们常常看到的x通常为一个多维向量，这也就引出了Hessian矩阵的概念（就是x的二阶导数矩阵）。</p>
<p>缺点：牛顿法是定长迭代，没有步长因子，所以不能保证函数值稳定的下降，严重时甚至会失败。还有就是牛顿法要求函数一定是二阶可导的。而且计算Hessian矩阵的逆复杂度很大。</p>
<p>拟牛顿法：
不用二阶偏导而是<strong>构造出Hessian矩阵的近似正定对称矩阵</strong>的方法称为拟牛顿法。拟牛顿法的思路就是用一个特别的表达形式来模拟Hessian矩阵或者是他的逆使得表达式满足拟牛顿条件。主要有DFP法（逼近Hession的逆）、BFGS（直接逼近Hession矩阵）、
L-BFGS（可以减少BFGS所需的存储空间）。</p>
<h3 id="逻辑斯特回归为什么要对特征进行离散化">8.
逻辑斯特回归为什么要对特征进行离散化。</h3>
<ol type="1">
<li>非线性！非线性！非线性！逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
离散特征的增加和减少都很容易，易于模型的快速迭代；</li>
<li>速度快！速度快！速度快！稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；</li>
<li>鲁棒性！鲁棒性！鲁棒性！离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li>
<li>方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；</li>
<li>稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；</li>
<li>简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</li>
<li></li>
</ol>
<h3 id="逻辑回归的目标函数中增大l1正则化会是什么结果">9.
逻辑回归的目标函数中增大L1正则化会是什么结果。</h3>
<p>所有的参数w都会变成0。</p>
<h3 id="代码实现">10. 代码实现</h3>
<p>GitHub：<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/2.Logistics%20Regression/demo/CreditScoring.ipynb">https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/2.Logistics%20Regression/demo/CreditScoring.ipynb</a></p>
<p>.</p>
<p>把数据切分成训练集和测试集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line">x_tran,x_test,y_tran,y_test=model_selection.train_test_split(X,y,test_size=<span class="number">0.2</span>)</span><br><span class="line"><span class="built_in">print</span>(x_test.shape)</span><br></pre></td></tr></table></figure>
<p>使用logistic
regression/决策树/SVM/KNN...等sklearn分类算法进行分类，尝试查sklearn
API了解模型参数含义，调整不同的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="comment">## https://blog.csdn.net/sun_shengyun/article/details/53811483</span></span><br><span class="line"><span class="comment">#multi_class=&#x27;ovr&#x27; 表示处理多分类问题时采用“One-vs-Rest”策略。</span></span><br><span class="line"><span class="comment">#solver=&#x27;sag&#x27; 选择随机平均梯度下降算法来求解优化问题。</span></span><br><span class="line"><span class="comment">#class_weight=&#x27;balanced&#x27; 用于处理类别不平衡的情况，使模型更关注少数类。</span></span><br><span class="line">lr=LogisticRegression(multi_class=<span class="string">&#x27;ovr&#x27;</span>,solver=<span class="string">&#x27;sag&#x27;</span>,class_weight=<span class="string">&#x27;balanced&#x27;</span>)</span><br><span class="line"><span class="comment">#使用训练数据 x_tran 和对应的标签 y_tran 来拟合模型</span></span><br><span class="line">lr.fit(x_tran,y_tran)</span><br><span class="line"><span class="comment">#计算模型在训练数据上的得分</span></span><br><span class="line">score=lr.score(x_tran,y_tran)</span><br><span class="line"><span class="built_in">print</span>(score) <span class="comment">##最好的分数是1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9323730412572769</span><br></pre></td></tr></table></figure>
<p>.</p>
<p>在测试集上进行预测，计算准确度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="comment">## https://blog.csdn.net/qq_16095417/article/details/79590455</span></span><br><span class="line">train_score=accuracy_score(y_tran,lr.predict(x_tran))</span><br><span class="line">test_score=lr.score(x_test,y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练集准确率：&#x27;</span>,train_score)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集准确率：&#x27;</span>,test_score)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">训练集准确率： 0.9323730412572769</span><br><span class="line">测试集准确率： 0.9332719742291763</span><br></pre></td></tr></table></figure>
<p>.</p>
<p>查看sklearn的官方说明，了解分类问题的评估标准，并对此例进行评估。</p>
<p>例如，如果这是一个将水果分为苹果、香蕉、橙子的多分类问题。召回率衡量的是对于每个类别，模型正确预测为该类别的样本数量占实际该类别样本数量的比例。通过计算训练集和测试集的召回率，可以评估模型在不同数据集上对各类别样本的预测能力，从而判断模型是否过拟合或欠拟合，以及模型在新数据上的泛化能力。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">##召回率</span><br><span class="line">from sklearn.metrics import recall_score</span><br><span class="line">train_recall=recall_score(y_tran,lr.predict(x_tran),average=&#x27;macro&#x27;)</span><br><span class="line">test_recall=recall_score(y_test,lr.predict(x_test),average=&#x27;macro&#x27;)</span><br><span class="line">print(&#x27;训练集召回率：&#x27;,train_recall)</span><br><span class="line">print(&#x27;测试集召回率：&#x27;,test_recall)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">训练集召回率： 0.4999938302834368</span><br><span class="line">测试集召回率： 0.4999753463833144</span><br></pre></td></tr></table></figure>
<p>.</p>
<p>银行通常会有更严格的要求，因为fraud带来的后果通常比较严重，一般我们会调整模型的标准。
比如在logistic
regression当中，一般我们的概率判定边界为0.5，但是我们可以把阈值设定低一些，来提高模型的“敏感度”，试试看把阈值设定为0.3，再看看这时的评估指标(主要是准确率和召回率)。</p>
<p>tips:sklearn的很多分类模型，predict_prob可以拿到预估的概率，可以根据它和设定的阈值大小去判断最终结果(分类类别)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">y_pro=lr.predict_proba(x_test) ##获取预测概率值</span><br><span class="line">y_prd2 = [list(p&gt;=0.3).index(1) for i,p in enumerate(y_pro)]   ##设定0.3阈值，把大于0.3的看成1分类。</span><br><span class="line">train_score=accuracy_score(y_test,y_prd2)</span><br><span class="line">print(train_score)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9333179935572941</span><br></pre></td></tr></table></figure>
<h2 id="t-snet-distributed-stochastic-neighbor-embedding">t-SNE（t-distributed
Stochastic Neighbor Embedding）</h2>
<p>t-SNE（t-distributed Stochastic Neighbor
Embedding）是一种用于<strong>降维</strong>和<strong>数据可视化</strong>的机器学习算法。</p>
<p><strong>一、主要原理</strong></p>
<p>t-SNE
的目标是将<strong>高维数据</strong>映射到<strong>低维空间（通常是二维或三维）</strong>，<strong>同时尽可能保持数据点之间的相似性</strong>。</p>
<p>它通过将高维空间中的<strong>欧氏距离</strong>转换为<strong>概率分布</strong>，然后在<strong>低维空间</strong>中也<strong>构建一个概率分布</strong>，并<strong>最小化这两个概率分布之间的差异来实现</strong>。</p>
<p>具体来说，<strong>t-SNE
首先计算高维空间中数据点之间的相似性，通常使用高斯分布来计算两个点之间的相似度</strong>。然后，在低维空间中使用
t
分布来计算数据点之间的相似性。通过优化一个损失函数，使得低维空间中的相似性尽可能接近高维空间中的相似性。</p>
<p><strong>二、应用场景</strong></p>
<ol type="1">
<li>数据可视化：t-SNE
能够将高维数据可视化在二维或三维空间中，帮助人们直观地理解数据的分布和结构。例如，在图像分类任务中，可以将高维的图像特征映射到二维平面上，观察不同类别的图像在低维空间中的分布情况。</li>
<li>特征提取：t-SNE
可以作为一种特征提取方法，将高维数据映射到低维空间后，提取低维空间中的特征用于后续的分析和建模。</li>
<li>异常检测：通过观察数据在低维空间中的分布，可以发现异常点或离群点，这些点可能代表了数据中的异常情况或错误。</li>
</ol>
<p><strong>三、优势与局限性</strong></p>
<ol type="1">
<li><p>优势：</p>
<ul>
<li>能够有效地捕捉高维数据中的局部和全局结构，对于复杂的数据分布具有较好的可视化效果。</li>
<li>可以处理大规模数据集，通过随机采样和近似计算方法，减少计算时间和内存消耗。</li>
</ul></li>
<li><p>局限性：</p>
<ul>
<li>计算复杂度较高，尤其是对于大规模数据集，可能需要较长的计算时间。</li>
<li>结果具有一定的随机性，每次运行可能会得到不同的可视化结果。</li>
<li>在低维空间中的距离并不完全等同于高维空间中的距离，因此不能直接根据低维空间中的距离进行精确的度量和分析。</li>
</ul></li>
</ol>
<h2 id="训练集验证集和测试集">训练集、验证集和测试集</h2>
<p>在机器学习中，训练集、验证集和测试集都有着重要的作用。</p>
<p><strong>一、训练集（Training Set）</strong></p>
<ol type="1">
<li><p>定义：训练集是用来训练模型的数据集合。它包含输入数据和对应的目标输出（标签），模型通过学习这些数据中的模式和规律来调整自身的参数，以最小化预测误差。</p></li>
<li><p>作用：</p>
<ul>
<li>模型学习：训练集是模型学习的基础，通过在训练集上进行迭代训练，模型逐渐提高对数据的拟合能力，学习到数据中的特征和规律。</li>
<li>参数调整：利用训练集上的误差反馈，通过优化算法（如梯度下降）不断调整模型的参数，使模型能够更好地适应数据的分布。</li>
</ul>
<p><strong>二、验证集（Validation Set）</strong></p></li>
<li><p>定义：验证集是在模型训练过程中用于评估模型性能和调整超参数的数据集合。它独立于训练集，通常从原始数据中划分出来。</p></li>
<li><p>作用：</p>
<ul>
<li>性能评估：在模型训练的不同阶段，使用验证集来评估模型的性能，如准确率、损失值等指标。通过观察模型在验证集上的表现，可以了解模型是否过拟合或欠拟合。</li>
<li>超参数调整：验证集用于调整模型的超参数，如学习率、层数、节点数等。通过在验证集上进行实验，选择性能最佳的超参数组合。</li>
<li>早停法（Early
Stopping）：当模型在验证集上的性能不再提升时，可以停止训练，防止过拟合。</li>
</ul>
<p><strong>三、测试集（Test Set）</strong></p></li>
<li><p>定义：测试集是用于最终评估模型性能的数据集合。它与训练集和验证集完全独立，在模型训练过程中不被使用。</p></li>
<li><p>作用：</p>
<ul>
<li>最终性能评估：测试集提供了一个客观的评估标准，用于衡量模型在未知数据上的泛化能力。通过在测试集上的表现，可以确定模型的实际性能。</li>
<li>模型比较：当有多个模型时，可以使用测试集来比较它们的性能，选择性能最佳的模型。</li>
<li>模型部署决策：测试集的结果可以帮助决定是否将模型部署到实际应用中。如果模型在测试集上的性能不理想，可能需要进一步改进模型或重新收集数据。</li>
</ul>
<p>总之，训练集用于模型学习和参数调整，验证集用于性能评估和超参数调整，测试集用于最终评估模型的泛化能力和做出模型部署决策。合理划分和使用这三个数据集对于构建有效的机器学习模型至关重要。</p></li>
</ol>
<h2 id="自注意力机制self-attention-mechanism">自注意力机制（Self-Attention
Mechanism）</h2>
<p>自注意力机制（Self-Attention
Mechanism）是一种在深度学习中广泛应用的技术，尤其在自然语言处理和计算机视觉等领域发挥了重要作用。</p>
<p><strong>一、基本概念</strong></p>
<p>自注意力机制主要是让模型能够根据输入数据的不同部分之间的关系，自动地为不同部分分配不同的注意力权重。它可以捕捉输入数据中的长距离依赖关系，而不需要依赖传统的顺序处理方式。</p>
<p>例如，在处理一个句子时，自注意力机制可以让模型根据句子中各个单词之间的关系，确定每个单词的重要程度，从而更好地理解句子的含义。</p>
<p><strong>二、工作原理</strong></p>
<ol type="1">
<li><p>计算相似度</p>
<ul>
<li>对于给定的输入序列（如一个句子中的单词向量序列），首先计算每个元素（如单词向量）与其他元素之间的相似度。</li>
<li>常用的计算相似度的方法是点积（dot product）或缩放点积（scaled dot
product）。</li>
</ul></li>
<li><p>计算注意力权重</p>
<ul>
<li>根据相似度计算结果，通过softmax函数将其转化为注意力权重。</li>
<li>注意力权重表示每个元素对其他元素的关注程度。</li>
</ul></li>
<li><p>加权求和</p>
<ul>
<li>最后，将每个元素与对应的注意力权重相乘，并进行求和，得到自注意力机制的输出。</li>
<li>输出结果是一个新的向量序列，其中每个向量都融合了输入序列中其他元素的信息，并且根据注意力权重进行了加权。</li>
</ul></li>
</ol>
<p><strong>三、优势</strong></p>
<ol type="1">
<li><p>并行计算</p>
<ul>
<li>自注意力机制可以并行地计算每个元素的注意力权重，不像传统的循环神经网络（RNN）需要按顺序处理输入数据，因此可以大大提高计算效率。</li>
</ul></li>
<li><p>长距离依赖捕捉</p>
<ul>
<li>能够有效地捕捉输入数据中的长距离依赖关系，这对于处理长文本或复杂的图像等数据非常重要。</li>
</ul></li>
<li><p>灵活性</p>
<ul>
<li>可以很容易地与其他深度学习模型结合使用，如卷积神经网络（CNN）和循环神经网络，以提高模型的性能。</li>
</ul></li>
</ol>
<p><strong>四、应用领域</strong></p>
<ol type="1">
<li><p>自然语言处理</p>
<ul>
<li>机器翻译：帮助模型更好地理解源语言和目标语言之间的关系，提高翻译质量。</li>
<li>文本分类：聚焦于文本中的关键信息，提高分类准确性。</li>
<li>阅读理解：分析文章中的不同部分，更准确地找到答案。</li>
</ul></li>
<li><p>计算机视觉</p>
<ul>
<li>图像分类：关注图像中的关键区域，提高分类准确性。</li>
<li>目标检测：确定图像中不同区域对于检测目标的重要性。</li>
<li>图像描述生成：根据图像的不同部分生成更准确的描述。</li>
</ul></li>
</ol>
<h2 id="消融研究ablation-study">消融研究（Ablation Study）</h2>
<p>消融研究（Ablation
Study）是一种在科学研究，特别是在机器学习和计算机科学领域中常用的实验方法。</p>
<p><strong>一、目的</strong></p>
<p>其主要目的是为了理解一个复杂系统中各个组成部分的贡献和重要性。通过逐步去除或修改系统中的特定部分，观察系统性能的变化，从而确定每个部分对整体性能的影响。</p>
<p><strong>二、具体做法</strong></p>
<ol type="1">
<li>确定研究对象：首先明确要进行消融研究的复杂系统，例如一个深度学习模型、一个算法流程等。</li>
<li>选择要消融的部分：确定系统中的哪些组成部分将被逐个去除或修改。这些部分可以是模型的特定模块、算法的某个步骤、特定的特征等。</li>
<li>进行实验：分别对完整系统和去除特定部分后的系统进行实验。在机器学习中，通常使用相同的数据集和评估指标来比较性能变化。</li>
<li>分析结果：观察去除不同部分后系统性能的变化，例如准确率、召回率、F1
分数等指标的变化。通过分析这些结果，可以确定每个部分对整体性能的贡献程度。</li>
</ol>
<p><strong>三、举例</strong></p>
<p>例如在一个图像分类的深度学习模型中，可能包括卷积层、池化层、全连接层等多个模块。为了研究每个模块的重要性，可以进行消融研究：</p>
<ol type="1">
<li>保留完整模型进行实验，得到基准性能。</li>
<li>去除卷积层，观察模型性能的变化，确定卷积层对图像特征提取的贡献。</li>
<li>去除池化层，观察性能变化，了解池化层在降低数据维度和提高模型鲁棒性方面的作用。</li>
<li>去除全连接层，分析其对最终分类结果的影响。</li>
</ol>
<p>通过这样的消融研究，可以深入了解模型中各个模块的功能和重要性，为进一步改进和优化模型提供依据。</p>
<h2 id="对比学习contrastive-learning">对比学习（Contrastive
Learning）</h2>
<p>对比学习（Contrastive
Learning）是一种无监督学习方法，旨在通过学习数据的相似性和差异性来提取有效的特征表示。</p>
<p><strong>一、基本原理</strong></p>
<p>对比学习的核心思想是让模型学习如何区分相似的样本对和不相似的样本对。具体来说，它通过构建正负样本对，并让模型学习使得相似样本对之间的距离尽可能小，不相似样本对之间的距离尽可能大。</p>
<p>例如，对于图像数据，可以通过对同一图像进行不同的数据增强操作（如旋转、裁剪、颜色变换等）来生成正样本对，而不同的图像则构成负样本对。模型通过学习区分这些样本对，从而提取出能够反映图像本质特征的表示。</p>
<p><strong>二、方法流程</strong></p>
<ol type="1">
<li>数据准备
<ul>
<li>收集大量未标注的数据，并进行适当的数据增强操作以生成正样本对和负样本对。</li>
</ul></li>
<li>构建损失函数
<ul>
<li>设计一个对比损失函数，该函数通常基于样本对之间的距离度量。常见的对比损失函数有InfoNCE损失、Triplet损失等。</li>
</ul></li>
<li>模型训练
<ul>
<li>使用对比损失函数对模型进行训练。在训练过程中，模型不断调整参数以最小化损失函数，使得相似样本对的表示更加接近，不相似样本对的表示更加远离。</li>
</ul></li>
<li>特征提取
<ul>
<li>训练完成后，可以使用模型提取数据的特征表示。这些特征表示通常具有较好的区分性和鲁棒性，可以用于后续的任务，如分类、聚类等。</li>
</ul></li>
</ol>
<p><strong>三、优势</strong></p>
<ol type="1">
<li>无需大量标注数据
<ul>
<li>对比学习是一种无监督学习方法，因此可以在没有大量标注数据的情况下进行训练，节省了标注数据的成本和时间。</li>
</ul></li>
<li>学习到有效的特征表示
<ul>
<li>通过学习数据的相似性和差异性，对比学习可以提取出具有区分性和鲁棒性的特征表示，这些特征表示可以提高后续任务的性能。</li>
</ul></li>
<li>可扩展性强
<ul>
<li>对比学习可以很容易地应用于不同类型的数据，如图像、文本、音频等，并且可以与其他深度学习方法结合使用，具有很强的可扩展性。</li>
</ul></li>
</ol>
<p><strong>四、应用场景</strong></p>
<ol type="1">
<li>计算机视觉
<ul>
<li>在图像分类、目标检测、图像检索等任务中，对比学习可以用于预训练模型，提取有效的图像特征，提高模型的性能。</li>
</ul></li>
<li>自然语言处理
<ul>
<li>在文本分类、情感分析、机器翻译等任务中，对比学习可以用于学习词向量或句子向量的表示，提高模型的语言理解能力。</li>
</ul></li>
<li>推荐系统
<ul>
<li>在推荐系统中，对比学习可以用于学习用户和物品的特征表示，提高推荐的准确性和个性化程度。</li>
</ul></li>
</ol>
<h2 id="可变形卷积">可变形卷积</h2>
<h3 id="提出背景">提出背景<a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/CNN/convolution_operator/Deformable_Convolution.html#id2" title="Permalink to this headline">¶</a></h3>
<p>视觉识别的一个关键挑战是如何适应物体尺度、姿态、视点和零件变形的几何变化或模型几何变换。</p>
<p>但对于视觉识别的传统CNN模块，不可避免的都存在<strong>固定几何结构</strong>的缺陷：卷积单元在固定位置对输入特征图进行采样；池化层以固定比率降低空间分辨率；一个ROI（感兴趣区域）池化层将一个ROI分割成固定的空间单元；缺乏处理几何变换的内部机制等。</p>
<p>这些将会引起一些明显的问题。例如，同一CNN层中所有激活单元的感受野大小是相同的，这对于在空间位置上编码语义的高级CNN层是不需要的。而且，对于具有精细定位的视觉识别（例如，使用完全卷积网络的语义分割）的实际问题，由于不同的位置可能对应于具有不同尺度或变形的对象，因此，尺度或感受野大小的自适应确定是可取的。</p>
<p>为了解决以上所提到的局限性，一个自然地想法就诞生了：
<strong>卷积核自适应调整自身的形状</strong>
。这就产生了可变形卷积的方法。</p>
<h3 id="可变形卷积-1">可变形卷积<a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/CNN/convolution_operator/Deformable_Convolution.html#id3" title="Permalink to this headline">¶</a></h3>
<h4 id="dcn-v1">DCN v1<a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/CNN/convolution_operator/Deformable_Convolution.html#dcn-v1" title="Permalink to this headline">¶</a></h4>
<p>可变形卷积顾名思义就是卷积的位置是可变形的，并非在传统的<strong>𝑁</strong>×<strong>𝑁</strong>N×N的网格上做卷积，这样的好处就是更准确地提取到我们想要的特征（传统的卷积仅仅只能提取到矩形框的特征），通过一张图我们可以更直观地了解：</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-53.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>图1 绵羊特征提取</p>
<p>在上面这张图里面，左边传统的卷积显然没有提取到完整绵羊的特征，而右边的可变形卷积则提取到了完整的不规则绵羊的特征。</p>
<p>那可变卷积实际上是怎么做的呢？
<em>其实就是在每一个卷积采样点加上了一个偏移量</em> ，如下图所示： <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-54.png" alt="alt text"></p>
<p>图2 卷积核和可变形卷积核</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1724402152674.png" alt="1724402152674">
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-55.png" alt="alt text"></p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1724402203427.png" alt="1724402203427">
<figcaption aria-hidden="true">1724402203427</figcaption>
</figure>
<h4 id="dcn-v2">DCN v2</h4>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1724402287285.png" alt="1724402287285">
<figcaption aria-hidden="true">1724402287285</figcaption>
</figure>
<h4 id="paddle中的api"><strong>paddle中的API</strong></h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">paddle.vision.ops.deform_conv2d(*x*, *offset*, *weight*, *bias=None*, *stride=1*, *padding=0*, *dilation=1*, *deformable_groups=1*, *groups=1*, *mask=None*, *name=None*);</span><br></pre></td></tr></table></figure>
<p>deform_conv2d 对输入4-D Tensor计算2-D可变形卷积。详情参考<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/vision/ops/deform_conv2d_cn.html#deform-conv2d">deform_conv2d</a>。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1724402366757.png" alt="1724402366757">
<figcaption aria-hidden="true">1724402366757</figcaption>
</figure>
<p><strong>算法实例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#deformable conv v2:</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="built_in">input</span> = paddle.rand((<span class="number">8</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">kh, kw = <span class="number">3</span>, <span class="number">3</span></span><br><span class="line">weight = paddle.rand((<span class="number">16</span>, <span class="number">1</span>, kh, kw))</span><br><span class="line"><span class="comment"># offset shape should be [bs, 2 * kh * kw, out_h, out_w]</span></span><br><span class="line"><span class="comment"># mask shape should be [bs, hw * hw, out_h, out_w]</span></span><br><span class="line"><span class="comment"># In this case, for an input of 28, stride of 1</span></span><br><span class="line"><span class="comment"># and kernel size of 3, without padding, the output size is 26</span></span><br><span class="line">offset = paddle.rand((<span class="number">8</span>, <span class="number">2</span> * kh * kw, <span class="number">26</span>, <span class="number">26</span>))</span><br><span class="line">mask = paddle.rand((<span class="number">8</span>, kh * kw, <span class="number">26</span>, <span class="number">26</span>))</span><br><span class="line">out = paddle.vision.ops.deform_conv2d(<span class="built_in">input</span>, offset, weight, mask=mask)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="comment"># returns</span></span><br><span class="line">[<span class="number">8</span>, <span class="number">16</span>, <span class="number">26</span>, <span class="number">26</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#deformable conv v1: 无mask参数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="built_in">input</span> = paddle.rand((<span class="number">8</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">kh, kw = <span class="number">3</span>, <span class="number">3</span></span><br><span class="line">weight = paddle.rand((<span class="number">16</span>, <span class="number">1</span>, kh, kw))</span><br><span class="line"><span class="comment"># offset shape should be [bs, 2 * kh * kw, out_h, out_w]</span></span><br><span class="line"><span class="comment"># In this case, for an input of 28, stride of 1</span></span><br><span class="line"><span class="comment"># and kernel size of 3, without padding, the output size is 26</span></span><br><span class="line">offset = paddle.rand((<span class="number">8</span>, <span class="number">2</span> * kh * kw, <span class="number">26</span>, <span class="number">26</span>))</span><br><span class="line">out = paddle.vision.ops.deform_conv2d(<span class="built_in">input</span>, offset, weight)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="comment"># returns</span></span><br><span class="line">[<span class="number">8</span>, <span class="number">16</span>, <span class="number">26</span>, <span class="number">26</span>]</span><br></pre></td></tr></table></figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1724402456938.png" alt="1724402456938">
<figcaption aria-hidden="true">1724402456938</figcaption>
</figure>
<p>实例效果<a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/CNN/convolution_operator/Deformable_Convolution.html#id4" title="Permalink to this headline">¶</a></p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-56.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>图4 regular、DCN v1、DCN v2的感受野对比</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1724402549955.png" alt="1724402549955">
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-57.png" alt="alt text"></p>
<p>图5 regular、DCN v1、DCN v2的准确率对比</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1724402630909.png" alt="1724402630909">
<figcaption aria-hidden="true">1724402630909</figcaption>
</figure>
<h2 id="随机森林">随机森林</h2>
<h3 id="什么是随机森林">1.什么是随机森林</h3>
<h4 id="bagging思想">1.1 Bagging思想</h4>
<p>Bagging是bootstrap
aggregating。思想就是从总体样本当中随机取一部分样本进行训练，通过多次这样的结果，进行投票获取平均值作为结果输出，这就极大可能的避免了不好的样本数据，从而提高准确度。因为有些是不好的样本，相当于噪声，模型学入噪声后会使准确度不高。</p>
<p><strong>举个例子</strong> ：</p>
<p>假设有1000个样本，如果按照以前的思维，是直接把这1000个样本拿来训练，但现在不一样，先抽取800个样本来进行训练，假如噪声点是这800个样本以外的样本点，就很有效的避开了。重复以上操作，提高模型输出的平均值。</p>
<h4 id="随机森林-1">1.2 随机森林</h4>
<p>Random
Forest(随机森林)是一种基于树模型的Bagging的优化版本，一棵树的生成肯定还是不如多棵树，因此就有了随机森林，<strong>解决决策树泛化能力弱的特点</strong>。(可以理解成三个臭皮匠顶过诸葛亮)</p>
<p>而同一批数据，用同样的算法只能产生一棵树，这时<strong>Bagging策略可以帮助我们产生不同的数据集</strong>。<strong>Bagging</strong>策略来源于bootstrap
aggregation：<strong>从样本集（假设样本集N个数据点）中重采样选出<span class="math inline">\(N_b\)</span>个样本（有放回的采样，样本数据点个数仍然不变为N），在所有样本上，对这n个样本建立分类器（ID3）</strong>，重复以上两步m次，获得m个分类器，最后根据这m个分类器的投票结果，决定数据属于哪一类。</p>
<p>.</p>
<p><strong>每棵树的按照如下规则生成：</strong></p>
<ol type="1">
<li>如果训练集大小为N，对于每棵树而言，<strong>随机</strong>且有放回地从训练集中的抽取N个训练样本，作为该树的训练集；</li>
<li>如果每个样本的特征维度为M，指定一个常数m&lt;&lt;M，<strong>随机</strong>地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；</li>
<li>每棵树都尽最大程度的生长，并且没有剪枝过程。</li>
</ol>
<p>一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。</p>
<p>总的来说就是随机选择样本数，随机选取特征，随机选择分类器，建立多颗这样的决策树，然后通过这几课决策树来投票，决定数据属于哪一类(
<strong>投票机制有一票否决制、少数服从多数、加权多数</strong> )</p>
<h3 id="随机森林分类效果的影响因素">2. 随机森林分类效果的影响因素</h3>
<ul>
<li>森林中任意两棵树的相关性：相关性越大，错误率越大；</li>
<li>森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。</li>
</ul>
<p>减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。</p>
<p>.</p>
<h3 id="随机森林有什么优缺点">随机森林有什么优缺点</h3>
<p><strong>优点：</strong></p>
<ul>
<li>在当前的很多数据集上，相对其他算法有着很大的优势，表现良好。</li>
<li>它能够处理很高维度（feature很多）的数据，并且不用做特征选择(因为特征子集是随机选择的)。</li>
<li>在训练完后，它能够给出哪些feature比较重要。</li>
<li>训练速度快，容易做成并行化方法(训练时树与树之间是相互独立的)。</li>
<li>在训练过程中，能够检测到feature间的互相影响。</li>
<li>对于不平衡的数据集来说，它可以平衡误差。</li>
<li>如果有很大一部分的特征遗失，仍可以维持准确度。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>随机森林已经被证明在某些<strong>噪音较大</strong>的分类或回归问题上会过拟合。</li>
<li>对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。</li>
</ul>
<h3 id="随机森林如何处理缺失值">4. 随机森林如何处理缺失值？</h3>
<p>根据随机森林创建和训练的特点，随机森林对缺失值的处理还是比较特殊的。</p>
<ul>
<li>首先，给缺失值预设一些估计值，比如数值型特征，选择其余数据的中位数或众数作为当前的估计值</li>
<li>然后，根据估计的数值，建立随机森林，把所有的数据放进随机森林里面跑一遍。记录每一组数据在决策树中一步一步分类的路径.</li>
<li>判断哪组数据和缺失数据路径最相似，引入一个相似度矩阵，来记录数据之间的相似度，比如有N组数据，相似度矩阵大小就是N*N</li>
<li>如果缺失值是类别变量，通过权重投票得到新估计值，如果是数值型变量，通过加权平均得到新的估计值，如此迭代，直到得到稳定的估计值。</li>
</ul>
<p>其实，该缺失值填补过程类似于推荐系统中采用协同过滤进行评分预测，先计算缺失特征与其他特征的相似度，再加权得到缺失值的估计，而随机森林中计算相似度的方法（数据在决策树中一步一步分类的路径）乃其独特之处。</p>
<h3 id="什么是oob随机森林中oob是如何计算的它有什么优缺点">5.
什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？</h3>
<p><strong>OOB</strong> ：</p>
<p>上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob
error（out-of-bag error）。</p>
<p>bagging方法中Bootstrap每次约有1/3的样本不会出现在Bootstrap所采集的样本集合中，当然也就没有参加决策树的建立，把这1/3的数据称为
<strong>袋外数据oob（out of bag）</strong>
,它可以用于取代测试集误差估计方法。</p>
<p><strong>袋外数据(oob)误差的计算方法如下：</strong></p>
<ul>
<li>对于已经生成的随机森林,用袋外数据测试其性能,假设袋外数据总数为O,用这O个袋外数据作为输入,带进之前已经生成的随机森林分类器,分类器会给出O个数据相应的分类</li>
<li>因为这O条数据的类型是已知的,则用正确的分类与随机森林分类器的结果进行比较,统计随机森林分类器分类错误的数目,设为X,则袋外数据误差大小=X/O</li>
</ul>
<p><strong>优缺点</strong> ：</p>
<p>这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。</p>
<h3 id="随机森林的过拟合问题">6. 随机森林的过拟合问题</h3>
<ol type="1">
<li>你已经建了一个有10000棵树的随机森林模型。在得到0.00的训练误差后，你非常高兴。但是，验证错误是34.23。到底是怎么回事？你还没有训练好你的模型吗？
答：该模型过度拟合，因此，为了避免这些情况，我们要用交叉验证来调整树的数量。</li>
</ol>
<h3 id="代码实现-1">7. 代码实现</h3>
<p><a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.1%20Random%20Forest/3.1%20Random%20Forest.md#7-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"></a></p>
<p>GitHub：<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.1%20Random%20Forest/RandomForestRegression.ipynb">https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.1%20Random%20Forest/RandomForestRegression.ipynb</a></p>
<h4 id="import工具库">0.import工具库</h4>
<p>In [1]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from sklearn import preprocessing</span><br><span class="line">from sklearn.ensemble import RandomForestRegressor</span><br><span class="line">from sklearn.datasets import load_boston</span><br></pre></td></tr></table></figure>
<h4 id="加载数据">1.加载数据</h4>
<p>In [2]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">boston_house = load_boston()</span><br></pre></td></tr></table></figure>
<p>In [3]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">boston_feature_name = boston_house.feature_names</span><br><span class="line">boston_features = boston_house.data</span><br><span class="line">boston_target = boston_house.target</span><br></pre></td></tr></table></figure>
<p>In [4]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">boston_feature_name</span><br></pre></td></tr></table></figure>
<p>Out[4]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([&#x27;CRIM&#x27;, &#x27;ZN&#x27;, &#x27;INDUS&#x27;, &#x27;CHAS&#x27;, &#x27;NOX&#x27;, &#x27;RM&#x27;, &#x27;AGE&#x27;, &#x27;DIS&#x27;, &#x27;RAD&#x27;,</span><br><span class="line">       &#x27;TAX&#x27;, &#x27;PTRATIO&#x27;, &#x27;B&#x27;, &#x27;LSTAT&#x27;],</span><br><span class="line">      dtype=&#x27;|S7&#x27;)</span><br></pre></td></tr></table></figure>
<h4 id="构建模型">构建模型</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">help(RandomForestRegressor)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Help on class RandomForestRegressor in module sklearn.ensemble.forest:</span><br><span class="line"></span><br><span class="line">class RandomForestRegressor(ForestRegressor)</span><br><span class="line"> |  A random forest regressor.</span><br><span class="line"> |  </span><br><span class="line"> |  A random forest is a meta estimator that fits a number of classifying</span><br><span class="line"> |  decision trees on various sub-samples of the dataset and use averaging</span><br><span class="line"> |  to improve the predictive accuracy and control over-fitting.</span><br><span class="line"> |  The sub-sample size is always the same as the original</span><br><span class="line"> |  input sample size but the samples are drawn with replacement if</span><br><span class="line"> |  `bootstrap=True` (default).</span><br><span class="line"> |  </span><br><span class="line"> |  Read more in the :ref:`User Guide &lt;forest&gt;`.</span><br><span class="line"> |  </span><br><span class="line"> |  Parameters</span><br><span class="line"> |  ----------</span><br><span class="line"> |  n_estimators : integer, optional (default=10)</span><br><span class="line"> |      The number of trees in the forest.</span><br><span class="line"> |  </span><br><span class="line"> |  criterion : string, optional (default=&quot;mse&quot;)</span><br><span class="line"> |      The function to measure the quality of a split. Supported criteria</span><br><span class="line"> |      are &quot;mse&quot; for the mean squared error, which is equal to variance</span><br><span class="line"> |      reduction as feature selection criterion, and &quot;mae&quot; for the mean</span><br><span class="line"> |      absolute error.</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rgs = RandomForestRegressor(n_estimators=15)  ##随机森林模型</span><br><span class="line">rgs = rgs.fit(boston_features, boston_target)</span><br></pre></td></tr></table></figure>
<p>In [10]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rgs</span><br></pre></td></tr></table></figure>
<p>Out[10]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">RandomForestRegressor(bootstrap=True, criterion=&#x27;mse&#x27;, max_depth=None,</span><br><span class="line">           max_features=&#x27;auto&#x27;, max_leaf_nodes=None,</span><br><span class="line">           min_impurity_decrease=0.0, min_impurity_split=None,</span><br><span class="line">           min_samples_leaf=1, min_samples_split=2,</span><br><span class="line">           min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,</span><br><span class="line">           oob_score=False, random_state=None, verbose=0, warm_start=False)</span><br></pre></td></tr></table></figure>
<p>In [11]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rgs.predict(boston_features)</span><br></pre></td></tr></table></figure>
<p>Out[11]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">array([ 26.16666667,  22.24      ,  33.76666667,  33.67333333,</span><br><span class="line">        35.46      ,  26.74      ,  21.84      ,  26.59333333,</span><br><span class="line">        20.31333333,  19.88      ,  18.53333333,  19.57333333,</span><br><span class="line">        21.64      ,  19.64666667,  19.37333333,  19.92      ,</span><br><span class="line">        22.3       ,  17.8       ,  19.64      ,  18.73333333,</span><br><span class="line">        13.80666667,  18.42666667,  15.5       ,  14.46666667,</span><br><span class="line">        15.54666667,  14.18      ,  16.34666667,  14.58666667,</span><br><span class="line">        18.25333333,  21.46      ,  13.38      ,  15.86      ,</span><br><span class="line">        14.09333333,  13.8       ,  13.71333333,  19.51333333,</span><br><span class="line">        19.96666667,  21.49333333,  23.56666667,  30.07333333,</span><br><span class="line">        34.7       ,  28.31333333,  25.04      ,  24.64666667,</span><br><span class="line">        21.45333333,  19.42      ,  19.74      ,  18.26666667,</span><br><span class="line">        19.57333333,  20.02666667,  20.74666667,  20.84666667,</span><br><span class="line">        25.42666667,  22.25333333,  19.12666667,  34.66666667,</span><br><span class="line">        24.3       ,  32.18666667,  23.42666667,  19.78      ,</span><br><span class="line">        18.78666667,  17.82666667,  23.03333333,  25.81333333,</span><br><span class="line">        32.30666667,  23.85333333,  20.3       ,  21.85333333,</span><br><span class="line">        18.59333333,  21.10666667,  24.42      ,  21.25333333,</span><br><span class="line">        23.2       ,  23.56666667,  24.22      ,  22.24      ,</span><br><span class="line">        20.08      ,  21.34666667,  21.24      ,  20.49333333,</span><br><span class="line">        27.87333333,  24.4       ,  24.16666667,  22.98      ,</span><br><span class="line">        23.48666667,  27.13333333,  21.23333333,  22.28666667,</span><br><span class="line">        26.63333333,  29.01333333,  22.4       ,  22.06666667,</span><br><span class="line">        22.75333333,  24.77333333,  21.38666667,  26.91333333,</span><br><span class="line">        21.57333333,  40.41333333,  44.06      ,  32.64666667,</span><br><span class="line">        26.9       ,  25.90666667,  19.05333333,  19.85333333,</span><br><span class="line">        20.00666667,  19.73333333,  19.09333333,  20.34666667,</span><br><span class="line">        20.21333333,  19.20666667,  21.24      ,  24.06      ,</span><br><span class="line">        18.76666667,  18.66      ,</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import tree</span><br></pre></td></tr></table></figure>
<p>In [13]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rgs2 = tree.DecisionTreeRegressor()           ##决策树模型，比较两个模型的预测结果！</span><br><span class="line">rgs2.fit(boston_features, boston_target)</span><br></pre></td></tr></table></figure>
<p>Out[13]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DecisionTreeRegressor(criterion=&#x27;mse&#x27;, max_depth=None, max_features=None,</span><br><span class="line">           max_leaf_nodes=None, min_impurity_decrease=0.0,</span><br><span class="line">           min_impurity_split=None, min_samples_leaf=1,</span><br><span class="line">           min_samples_split=2, min_weight_fraction_leaf=0.0,</span><br><span class="line">           presort=False, random_state=None, splitter=&#x27;best&#x27;)</span><br></pre></td></tr></table></figure>
<p>In [14]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rgs2.predict(boston_features)</span><br></pre></td></tr></table></figure>
<p>Out[14]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">array([ 24. ,  21.6,  34.7,  33.4,  36.2,  28.7,  22.9,  27.1,  16.5,</span><br><span class="line">        18.9,  15. ,  18.9,  21.7,  20.4,  18.2,  19.9,  23.1,  17.5,</span><br><span class="line">        20.2,  18.2,  13.6,  19.6,  15.2,  14.5,  15.6,  13.9,  16.6,</span><br><span class="line">        14.8,  18.4,  21. ,  12.7,  14.5,  13.2,  13.1,  13.5,  18.9,</span><br><span class="line">        20. ,  21. ,  24.7,  30.8,  34.9,  26.6,  25.3,  24.7,  21.2,</span><br><span class="line">        19.3,  20. ,  16.6,  14.4,  19.4,  19.7,  20.5,  25. ,  23.4,</span><br><span class="line">        18.9,  35.4,  24.7,  31.6,  23.3,  19.6,  18.7,  16. ,  22.2,</span><br><span class="line">        25. ,  33. ,  23.5,  19.4,  22. ,  17.4,  20.9,  24.2,  21.7,</span><br><span class="line">        22.8,  23.4,  24.1,  21.4,  20. ,  20.8,  21.2,  20.3,  28. ,</span><br><span class="line">        23.9,  24.8,  22.9,  23.9,  26.6,  22.5,  22.2,  23.6,  28.7,</span><br><span class="line">        22.6,  22. ,  22.9,  25. ,  20.6,  28.4,  21.4,  38.7,  43.8,</span><br><span class="line">        33.2,  27.5,  26.5,  18.6,  19.3,  20.1,  19.5,  19.5,  20.4,</span><br><span class="line">        19.8,  19.4,  21.7,  22.8,  18.8,  18.7,  18.5,  18.3,  21.2,</span><br><span class="line">        19.2,  20.4,  19.3,  22. ,  20.3,  20.5,  17.3,  18.8,  21.4,</span><br><span class="line">        15.7,  16.2,  18. ,  14.3,  19.2,  19.6,  23. ,  18.4,  15.6,</span><br><span class="line">        18.1,  17.4,  17.1,  13.3,  17.8,  14. ,  14.4,  13.4,  15.6,</span><br><span class="line">        11.8,  13.8,  15.6,  14.6,  17.8,  15.4,  21.5,  19.6,  15.3,</span><br><span class="line">        19.4,  17. ,  15.6,  13.1,  41.3,  24.3,  23.3,  27. ,  50. ,</span><br><span class="line">        50. ,  50. ,  22.7,  25. ,  50. ,  23.8,  23.8,  22.3,  17.4,</span><br><span class="line">        19.1,  23.1,  23.6,  22.6,  29.4,  23.2,  24.6,  29.9,  37.2,</span><br><span class="line">        39.8,  36.2,  37.9,  32.5,  26.4,  29.6,  50. ,  32. ,  29.8,</span><br><span class="line">        34.9,  37. ,  30.5,  36.4,  31.1,  29.1,  50. ,  33.3,  30.3,</span><br><span class="line">        34.6,  34.9,  32.9,  24.1,  42.3,  48.5,  50. ,  22.6,  24.4,</span><br></pre></td></tr></table></figure>
<h2 id="gbdtgradient-boosting-decision-tree全名叫梯度提升决策树">GBDT(Gradient
Boosting Decision Tree)，全名叫梯度提升决策树</h2>
<p>GBDT(Gradient Boosting Decision
Tree)，全名叫梯度提升决策树，使用的是<strong>Boosting</strong>的思想。</p>
<h3 id="解释一下gbdt算法的过程">1. 解释一下GBDT算法的过程</h3>
<p>GBDT(Gradient Boosting Decision
Tree)，全名叫梯度提升决策树，使用的是<strong>Boosting</strong>的思想。</p>
<h4 id="boosting思想">1.1 Boosting思想</h4>
<p>Boosting方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。</p>
<p>Bagging与Boosting的串行训练方式不同，Bagging方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练</p>
<h4 id="gbdt原来是这么回事">1.2 GBDT原来是这么回事</h4>
<p>GBDT的原理很简单，就是<strong>所有弱分类器的结果相加等于预测值</strong>，然后下一个弱分类器去拟合误差函数对预测值的残差(这个残差就是预测值与真实值之间的误差)。当然了，它里面的弱分类器的表现形式就是各棵树。</p>
<p>举一个非常简单的例子，比如我今年30岁了，但计算机或者模型GBDT并不知道我今年多少岁，那GBDT咋办呢？</p>
<ul>
<li>它会在第一个弱分类器（或第一棵树中）随便用一个年龄比如20岁来拟合，然后发现误差有10岁；</li>
<li>接下来在第二棵树中，用6岁去拟合剩下的损失，发现差距还有4岁；</li>
<li>接着在第三棵树中用3岁拟合剩下的差距，发现差距只有1岁了；</li>
<li>最后在第四课树中用1岁拟合剩下的残差，完美。</li>
<li>最终，四棵树的结论加起来，就是真实年龄30岁（实际工程中，gbdt是计算负梯度，用负梯度近似残差）。</li>
</ul>
<p><strong>为何gbdt可以用用负梯度近似残差呢？</strong></p>
<p>回归任务下，GBDT
在每一轮的迭代时对每个样本都会有一个预测值，此时的损失函数为均方差损失函数，</p>
<p><img src="https://camo.githubusercontent.com/56da71ded4d540860f6dcccfeb57da8706c6ac3be305d35f7a8b21a9b2cd3ee6/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353231343936323033343934343633382e676966"></p>
<p>那此时的负梯度是这样计算的</p>
<p><img src="https://camo.githubusercontent.com/132247f994342d19fabbb36540e1e190b2810596766b958c50d072ab0e961b31/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353231343936323431363637303937332e676966"></p>
<p>所以，当损失函数选用均方损失函数是时，每一次拟合的值就是（真实值 -
当前模型预测的值），即残差。此时的变量是<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/a5daaa4e1550d7aab48422ac1e68305a79bdc608b243c0efff33a1742537e864/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353231343936333633333236373933382e676966"><img src="https://camo.githubusercontent.com/a5daaa4e1550d7aab48422ac1e68305a79bdc608b243c0efff33a1742537e864/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135353231343936333633333236373933382e676966"></a>，即“当前预测模型的值”，也就是对它求负梯度。</p>
<p><strong>训练过程</strong></p>
<p>简单起见，假定训练集只有4个人：A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下图所示结果：</p>
<p><img src="https://camo.githubusercontent.com/87e2954d28205a4a68b7086aab3098637a186fd8bbddcfa74340c92af32f4329/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135333433383536383139313330333935382e706e67"></p>
<p>现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点最多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图所示结果：</p>
<p><img src="https://camo.githubusercontent.com/d036bd5e6cbb88324cd280894976060668437f14aebae96d1c9bbe4e4cbafe7d/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135333433383537303532393235363839352e706e67"></p>
<p>在第一棵树分枝和图1一样，由于A,B年龄较为相近，C,D年龄较为相近，他们被分为左右两拨，每拨用平均年龄作为预测值。</p>
<ul>
<li>此时计算残差（残差的意思就是：A的实际值 - A的预测值 =
A的残差），所以A的残差就是实际值14 - 预测值15 = 残差值-1。</li>
<li>注意，A的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是15，如果还有树则需要都累加起来作为A的预测值。</li>
</ul>
<p>然后拿它们的残差-1、1、-1、1代替A B C
D的原值，到第二棵树去学习，第二棵树只有两个值1和-1，直接分成两个节点，即A和C分在左边，B和D分在右边，经过计算（比如A，实际值-1
- 预测值-1 = 残差0，比如C，实际值-1 - 预测值-1 =
0），此时所有人的残差都是0。<strong>残差值都为0</strong>，<strong>相当于第二棵树的预测值和它们的实际值相等</strong>，则<strong>只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了，即每个人都得到了真实的预测值。</strong></p>
<p>换句话说，现在A,B,C,D的预测值都和真实年龄一致了。Perfect！</p>
<ul>
<li>A: 14岁高一学生，购物较少，经常问学长问题，预测年龄A = 15 – 1 =
14</li>
<li>B: 16岁高三学生，购物较少，经常被学弟问问题，预测年龄B = 15 + 1 =
16</li>
<li>C: 24岁应届毕业生，购物较多，经常问师兄问题，预测年龄C = 25 – 1 =
24</li>
<li>D: 26岁工作两年员工，购物较多，经常被师弟问问题，预测年龄D = 25 + 1
= 26</li>
</ul>
<p>所以，GBDT需要将多棵树的得分累加得到最终的预测得分，且每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差。</p>
<h3 id="梯度提升和梯度下降的区别和联系是什么">2.
梯度提升和梯度下降的区别和联系是什么？</h3>
<p>下表是梯度提升算法和梯度下降算法的对比情况。可以发现，两者<strong>都是</strong>在每
一轮迭代中，利用损失函数相对于模型的<strong>负梯度方向的信息</strong>来对当前模型进行更
新，</p>
<p>只不过在<strong>梯度下降</strong>中，模型是以参数化形式表示，从而模型的更新等价于参
数的更新。</p>
<p>而在<strong>梯度提升</strong>中，<strong>模型并不需要进行参数化表示，而是直接定义在函
数空间中，从而大大扩展了可以使用的模型种类。</strong></p>
<p><img src="https://camo.githubusercontent.com/8013736490003624e4964476ec341d28c81cdba8594163f127f2bb28feb53235/687474703a2f2f7778332e73696e61696d672e636e2f6d773639302f3030363330446566677931673474647768717a73646a3330727030616664686f2e6a7067"></p>
<h3 id="gbdt的优点和局限性有哪些">3.
<strong>GBDT</strong>的优点和局限性有哪些？</h3>
<h4 id="优点">3.1 优点</h4>
<ol type="1">
<li>预测阶段的计算速度快，树与树之间可并行化计算。</li>
<li>在分布稠密的数据集上，泛化能力和表达能力都很好，这使得GBDT在Kaggle的众多竞赛中，经常名列榜首。</li>
<li>采用决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系。</li>
</ol>
<h4 id="局限性">3.2 局限性</h4>
<ol type="1">
<li>GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。</li>
<li>GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。</li>
<li>训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。</li>
</ol>
<h3 id="rf随机森林与gbdt之间的区别与联系">4.
RF(随机森林)与GBDT之间的区别与联系</h3>
<p><strong>相同点</strong> ：</p>
<ul>
<li>都是由多棵树组成，最终的结果都是由多棵树一起决定。</li>
<li>RF和GBDT在使用CART树时，可以是分类树或者回归树。</li>
</ul>
<p><strong>不同点</strong> ：</p>
<ul>
<li>组成随机森林的树可以并行生成，而GBDT是<strong>串行</strong>生成</li>
<li>随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和</li>
<li>随机森林对异常值不敏感，而GBDT对异常值比较敏感</li>
<li>随机森林是减少模型的方差，而GBDT是减少模型的偏差</li>
<li>随机森林不需要进行特征归一化。而GBDT则需要进行特征归一化</li>
</ul>
<h3 id="代码实现-2">5. 代码实现</h3>
<p>GitHub：<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.2%20GBDT/GBDT_demo.ipynb">https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.2%20GBDT/GBDT_demo.ipynb</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br></pre></td></tr></table></figure>
<h4 id="获取训练数据">获取训练数据</h4>
<p>In [54]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_feature = np.genfromtxt(<span class="string">&quot;train_feat.txt&quot;</span>,dtype=np.float32)</span><br><span class="line">num_feature = <span class="built_in">len</span>(train_feature[<span class="number">0</span>])</span><br><span class="line">train_feature = pd.DataFrame(train_feature)</span><br><span class="line"></span><br><span class="line">train_label = train_feature.iloc[:, num_feature - <span class="number">1</span>]</span><br><span class="line">train_feature = train_feature.iloc[:, <span class="number">0</span>:num_feature - <span class="number">2</span>]</span><br><span class="line">train_feature</span><br></pre></td></tr></table></figure>
<p>Out[54]:</p>
<table>
<colgroup>
<col style="width: 1%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 13%">
</colgroup>
<thead>
<tr>
<th></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.005988</td>
<td>0.569231</td>
<td>0.647059</td>
<td>0.951220</td>
<td>-0.225434</td>
<td>0.837989</td>
<td>0.357258</td>
<td>-0.003058</td>
</tr>
<tr>
<td>1</td>
<td>0.161677</td>
<td>0.743195</td>
<td>0.682353</td>
<td>0.960976</td>
<td>-0.086705</td>
<td>0.780527</td>
<td>0.282945</td>
<td>0.149847</td>
</tr>
<tr>
<td>2</td>
<td>0.113772</td>
<td>0.744379</td>
<td>0.541176</td>
<td>0.990244</td>
<td>-0.005780</td>
<td>0.721468</td>
<td>0.434110</td>
<td>-0.318043</td>
</tr>
<tr>
<td>3</td>
<td>0.053892</td>
<td>0.608284</td>
<td>0.764706</td>
<td>0.951220</td>
<td>-0.248555</td>
<td>0.821229</td>
<td>0.848604</td>
<td>-0.003058</td>
</tr>
<tr>
<td>4</td>
<td>0.173653</td>
<td>0.866272</td>
<td>0.682353</td>
<td>0.951220</td>
<td>0.017341</td>
<td>0.704709</td>
<td>-0.021002</td>
<td>-0.195719</td>
</tr>
</tbody>
</table>
<p>In [55]:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_label</span><br></pre></td></tr></table></figure>
<p>Out[55]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0    320.0</span><br><span class="line">1    361.0</span><br><span class="line">2    364.0</span><br><span class="line">3    336.0</span><br><span class="line">4    358.0</span><br><span class="line">Name: 9, dtype: float32</span><br></pre></td></tr></table></figure>
<h4 id="获取测试数据">获取测试数据</h4>
<p>In [56]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">test_feature = np.genfromtxt(&quot;test_feat.txt&quot;,dtype=np.float32)</span><br><span class="line">num_feature = len(test_feature[0])</span><br><span class="line">test_feature = pd.DataFrame(test_feature)</span><br><span class="line"></span><br><span class="line">test_label = test_feature.iloc[:, num_feature - 1]</span><br><span class="line">test_feature = test_feature.iloc[:, 0:num_feature - 2]</span><br><span class="line">test_feature</span><br></pre></td></tr></table></figure>
<p>Out[56]:</p>
<table>
<colgroup>
<col style="width: 1%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 13%">
</colgroup>
<thead>
<tr>
<th></th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.005988</td>
<td>0.569231</td>
<td>0.647059</td>
<td>0.951220</td>
<td>-0.225434</td>
<td>0.837989</td>
<td>0.357258</td>
<td>-0.003058</td>
</tr>
<tr>
<td>1</td>
<td>0.161677</td>
<td>0.743195</td>
<td>0.682353</td>
<td>0.960976</td>
<td>-0.086705</td>
<td>0.780527</td>
<td>0.282945</td>
<td>0.149847</td>
</tr>
<tr>
<td>2</td>
<td>0.113772</td>
<td>0.744379</td>
<td>0.541176</td>
<td>0.990244</td>
<td>-0.005780</td>
<td>0.721468</td>
<td>0.434110</td>
<td>-0.318043</td>
</tr>
<tr>
<td>3</td>
<td>0.053892</td>
<td>0.608284</td>
<td>0.764706</td>
<td>0.951220</td>
<td>-0.248555</td>
<td>0.821229</td>
<td>0.848604</td>
<td>-0.003058</td>
</tr>
<tr>
<td>4</td>
<td>0.173653</td>
<td>0.866272</td>
<td>0.682353</td>
<td>0.951220</td>
<td>0.017341</td>
<td>0.704709</td>
<td>-0.021002</td>
<td>-0.195719</td>
</tr>
</tbody>
</table>
<p>In [57]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_label</span><br></pre></td></tr></table></figure>
<p>Out[57]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">0    320.0</span><br><span class="line">1    361.0</span><br><span class="line">2    364.0</span><br><span class="line">3    336.0</span><br><span class="line">4    358.0</span><br><span class="line">Name: 9, dtype: float32</span><br></pre></td></tr></table></figure>
<h4 id="gbdt模型建立">GBDT模型建立</h4>
<p>In [58]:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">gbdt = GradientBoostingRegressor(</span><br><span class="line">  loss = &#x27;ls&#x27;</span><br><span class="line">, learning_rate = 0.1</span><br><span class="line">, n_estimators = 100</span><br><span class="line">, subsample = 1</span><br><span class="line">, min_samples_split = 2</span><br><span class="line">, min_samples_leaf = 1</span><br><span class="line">, max_depth = 3</span><br><span class="line">, init = None</span><br><span class="line">, random_state = None</span><br><span class="line">, max_features = None</span><br><span class="line">, alpha = 0.9</span><br><span class="line">, verbose = 0</span><br><span class="line">, max_leaf_nodes = None</span><br><span class="line">, warm_start = False</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">gbdt.fit(train_feature, train_label)</span><br><span class="line">pred = gbdt.predict(test_feature)</span><br><span class="line">total_err = 0</span><br><span class="line"></span><br><span class="line">for i in range(pred.shape[0]):</span><br><span class="line">    print(&#x27;pred:&#x27;, pred[i], &#x27; label:&#x27;, test_label[i])</span><br><span class="line">print(&#x27;均方误差:&#x27;, np.sqrt(((pred - test_label) ** 2).mean()))</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pred: 320.0008173984891  label: 320.0</span><br><span class="line">pred: 360.99965033119537  label: 361.0</span><br><span class="line">pred: 363.99928183902097  label: 364.0</span><br><span class="line">pred: 336.0002344322584  label: 336.0</span><br><span class="line">pred: 358.0000159974151  label: 358.0</span><br><span class="line">均方误差: 0.0005218003748239915</span><br></pre></td></tr></table></figure>
<h2 id="xgboost">XGBoost</h2>
<h3 id="什么是xgboost">1. 什么是XGBoost</h3>
<p>XGBoost是陈天奇等人开发的一个开源机器学习项目，高效地实现了GBDT算法并进行了算法和工程上的许多改进，被广泛应用在Kaggle竞赛及其他许多机器学习竞赛中并取得了不错的成绩。</p>
<p>说到XGBoost，不得不提GBDT(Gradient Boosting Decision
Tree)。因为XGBoost本质上还是一个GBDT，但是力争把速度和效率发挥到极致，所以叫X
(Extreme) GBoosted。包括前面说过，两者都是boosting方法。</p>
<p>关于GBDT，这里不再提，可以查看我前一篇的介绍，<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.2%20GBDT/3.2%20GBDT.md">点此跳转</a>。</p>
<h4 id="xgboost树的定义">1.1 XGBoost树的定义</h4>
<p>先来举个 <strong>例子</strong>
，我们要预测一家人对电子游戏的喜好程度，考虑到年轻和年老相比，年轻更可能喜欢电子游戏，以及男性和女性相比，男性更喜欢电子游戏，故先根据年龄大小区分小孩和大人，然后再通过性别区分开是男是女，逐一给各人在电子游戏喜好程度上打分，如下图所示。</p>
<p><img src="https://camo.githubusercontent.com/5f23a430faa189019542bc487f4efc8682de3a61e35a5f4dcc9d057359dcc316/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135333433383537373233323531363830302e706e67"></p>
<p>就这样，训练出了2棵树tree1和tree2，类似之前gbdt的原理，两棵树的结论累加起来便是最终的结论，所以小孩的预测分数就是两棵树中小孩所落到的结点的分数相加：2
+ 0.9 = 2.9。爷爷的预测分数同理：-1 + （-0.9）=
-1.9。具体如下图所示：</p>
<p><img src="https://camo.githubusercontent.com/7792e7b3d54f94926b16ada4e12427f4f3b15428d2b94584323512f7efb682c6/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135333433383537383733393139383433332e706e67"></p>
<p>恩，你可能要拍案而起了，惊呼，这不是跟上文介绍的GBDT乃异曲同工么？</p>
<p>事实上，如果不考虑工程实现、解决问题上的一些差异，XGBoost与GBDT比较大的不同就是目标函数的定义。XGBoost的目标函数如下图所示：</p>
<p><img src="https://camo.githubusercontent.com/2a5735441ade41b49e3b6664ab018b0b1afa8739db9fc0e8feb23d7605d9b040/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135333433383538303133393135393539332e706e67"></p>
<p>其中：</p>
<ul>
<li>红色箭头所指向的L 即为损失函数（比如平方损失函数：<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/3a3f2e8158b229d9e112baa70d76a54816e53061ba969099de0988c610e09a0c/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f6c28795f692c7925354569293d28795f692d79253545692925354532"><img src="https://camo.githubusercontent.com/3a3f2e8158b229d9e112baa70d76a54816e53061ba969099de0988c610e09a0c/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f6c28795f692c7925354569293d28795f692d79253545692925354532"></a>)</li>
<li>红色方框所框起来的是正则项（包括L1正则、L2正则）</li>
<li>红色圆圈所圈起来的为常数项</li>
<li>对于f(x)，XGBoost利用泰勒展开三项，做一个近似。<strong>f(x)表示的是其中一颗回归树。</strong></li>
</ul>
<p>看到这里可能有些读者会头晕了，这么多公式，
<strong>我在这里只做一个简要式的讲解，具体的算法细节和公式求解请查看这篇博文，讲得很仔细</strong>
：<a target="_blank" rel="noopener" href="https://blog.csdn.net/v_JULY_v/article/details/81410574">通俗理解kaggle比赛大杀器xgboost</a></p>
<p>XGBoost的<strong>核心算法思想</strong>不难，基本就是：</p>
<ol type="1">
<li>不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数
<strong>f(x)</strong> ，去拟合上次预测的残差。</li>
<li>当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数</li>
<li>最后只需要将每棵树对应的分数加起来就是该样本的预测值。</li>
</ol>
<p>显然，我们的目标是要使得树群的预测值<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/d1fa2600bb2fdc31c245153451e1b86af718480370457ed0c1d9954d3e632ad4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f795f6925354525374227253744"><img src="https://camo.githubusercontent.com/d1fa2600bb2fdc31c245153451e1b86af718480370457ed0c1d9954d3e632ad4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f795f6925354525374227253744" alt="img"></a>尽量接近真实值<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/0e28c816835a8a00c28dab343e8b8cb90100c7b4832cea590264c9e22cdeb712/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f795f69"><img src="https://camo.githubusercontent.com/0e28c816835a8a00c28dab343e8b8cb90100c7b4832cea590264c9e22cdeb712/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f795f69" alt="img"></a>，而且有尽量大的泛化能力。类似之前GBDT的套路，XGBoost也是需要将多棵树的得分累加得到最终的预测得分（每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差）。</p>
<p><img src="https://camo.githubusercontent.com/6e419fb4eee31d5c36e86f39552628c57ea58f918f8d081cb4add7228f86f47c/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135333433383635373236313833333439332e706e67"></p>
<p>那接下来，我们如何选择每一轮加入什么 f 呢？答案是非常直接的，选取一个
f 来使得我们的目标函数尽量最大地降低。这里 f
可以使用泰勒展开公式近似。</p>
<p><img src="https://camo.githubusercontent.com/b629fad546d9f568a29c6bd5b7d5c85bc26394cdd0e3b33fa74a063a31991b9c/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f7175657362617365363431353334333836353836373533303132302e706e67"></p>
<p>实质是把样本分配到叶子结点会对应一个obj，优化过程就是obj优化。也就是分裂节点到叶子不同的组合，不同的组合对应不同obj，所有的优化围绕这个思想展开。到目前为止我们讨论了目标函数中的第一个部分：训练误差。接下来我们讨论目标函数的第二个部分：正则项，即如何定义树的复杂度。</p>
<h4 id="正则项树的复杂度">1.2 正则项：树的复杂度</h4>
<p>XGBoost对树的复杂度包含了两个部分：</p>
<ul>
<li>一个是树里面叶子节点的个数T</li>
<li>一个是树上叶子节点的得分w的L2模平方（对w进行L2正则化，相当于针对每个叶结点的得分增加L2平滑，目的是为了避免过拟合）</li>
</ul>
<p><img src="https://camo.githubusercontent.com/ad4e284372673bd457bd3cd18fb3cbf5b169760eda736d442e0dd20d7a8328f7/68747470733a2f2f6a756c796564752d696d672e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f717565736261736536343135333433383637343139393437313438332e706e67"></p>
<p>我们再来看一下XGBoost的目标函数（损失函数揭示训练误差 +
正则化定义复杂度）：</p>
<p><img src="https://camo.githubusercontent.com/242a53033214ad30be8bb3b112a5379ca4f47383832fa6cddeb84f4ae5b810ab/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f4c28253543706869293d25354373756d5f253742692537446c28795f69253545253742272537442d795f69292b25354373756d5f6b2535434f6d65676128665f7429"></p>
<p>正则化公式也就是目标函数的后半部分，对于上式而言，<a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/d1fa2600bb2fdc31c245153451e1b86af718480370457ed0c1d9954d3e632ad4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f795f6925354525374227253744"><img src="https://camo.githubusercontent.com/d1fa2600bb2fdc31c245153451e1b86af718480370457ed0c1d9954d3e632ad4/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f795f6925354525374227253744"></a>是整个累加模型的输出，正则化项∑kΩ(ft)是则表示树的复杂度的函数，值越小复杂度越低，泛化能力越强。</p>
<h4 id="树该怎么长">1.3 树该怎么长</h4>
<p><a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/3.3%20XGBoost#13-%E6%A0%91%E8%AF%A5%E6%80%8E%E4%B9%88%E9%95%BF"></a></p>
<p>很有意思的一个事是，我们从头到尾了解了xgboost如何优化、如何计算，但树到底长啥样，我们却一直没看到。很显然，一棵树的生成是由一个节点一分为二，然后不断分裂最终形成为整棵树。那么树怎么分裂的就成为了接下来我们要探讨的关键。对于一个叶子节点如何进行分裂，XGBoost作者在其原始论文中给出了一种分裂节点的方法：<strong>枚举所有不同树结构的贪心法</strong></p>
<p>不断地枚举不同树的结构，然后利用打分函数来寻找出一个最优结构的树，接着加入到模型中，不断重复这样的操作。这个寻找的过程使用的就是
<strong>贪心算法</strong> 。选择一个feature分裂，计算loss
function最小值，然后再选一个feature分裂，又得到一个loss
function最小值，你枚举完，找一个效果最好的，把树给分裂，就得到了小树苗。</p>
<p>总而言之，XGBoost使用了和CART回归树一样的想法，利用贪婪算法，遍历所有特征的所有特征划分点，不同的是使用的目标函数不一样。具体做法就是分裂后的目标函数值比单子叶子节点的目标函数的增益，同时为了限制树生长过深，还加了个阈值，只有当增益大于该阈值才进行分裂。从而继续分裂，形成一棵树，再形成一棵树，<strong>每次在上一次的预测基础上取最优进一步分裂/建树。</strong></p>
<h4 id="如何停止树的循环生成">1.4 如何停止树的循环生成</h4>
<p>凡是这种循环迭代的方式必定有停止条件，什么时候停止呢？简言之，设置树的最大深度、当样本权重和小于设定阈值时停止生长以防止过拟合。具体而言，则</p>
<ol type="1">
<li>当引入的分裂带来的增益小于设定阀值的时候，我们可以忽略掉这个分裂，所以并不是每一次分裂loss
function整体都会增加的，有点预剪枝的意思，阈值参数为（即正则项里叶子节点数T的系数）；</li>
<li>当树达到最大深度时则停止建立决策树，设置一个超参数max_depth，避免树太深导致学习局部样本，从而过拟合；</li>
<li>样本权重和小于设定阈值时则停止建树。什么意思呢，即涉及到一个超参数-最小的样本权重和min_child_weight，和GBM的
min_child_leaf
参数类似，但不完全一样。大意就是一个叶子节点样本太少了，也终止同样是防止过拟合；</li>
</ol>
<h3 id="xgboost与gbdt有什么不同">2. XGBoost与GBDT有什么不同</h3>
<p>除了算法上与传统的GBDT有一些不同外，XGBoost还在工程实现上做了大量的优化。总的来说，两者之间的区别和联系可以总结成以下几个方面。</p>
<ol type="1">
<li>GBDT是机器学习算法，XGBoost是该算法的工程实现。</li>
<li>在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模
型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。</li>
<li>GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代
价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。</li>
<li>传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类
器，比如线性分类器。</li>
<li>传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机
森林相似的策略，支持对数据进行采样。</li>
<li>传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺
失值的处理策略。</li>
</ol>
<h3 id="为什么xgboost要用泰勒展开优势在哪里">3.
为什么XGBoost要用泰勒展开，优势在哪里？</h3>
<p>XGBoost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准.
使用泰勒展开取得函数做自变量的二阶导数形式,
可以在不选定损失函数具体形式的情况下,
仅仅依靠输入数据的值就可以进行叶子分裂优化计算,
本质上也就把损失函数的选取和模型算法优化/参数选择分开了.
这种去耦合增加了XGBoost的适用性, 使得它按需选取损失函数, 可以用于分类,
也可以用于回归。</p>
<h3 id="代码实现-3">4. 代码实现</h3>
<p>GitHub：<a target="_blank" rel="noopener" href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.3%20XGBoost/3.3%20XGBoost.ipynb">点击进入</a></p>
<h2 id="损失函数">损失函数:</h2>
<p>在机器学习中，损失函数（Loss
Function）是用于衡量模型预测结果与真实结果之间差异的函数。</p>
<p>损失函数的主要作用是评估模型在给定数据上的性能表现。它为模型的优化提供了一个明确的目标，通过最小化损失函数的值，来调整模型的参数，使得模型的预测结果越来越接近真实结果。</p>
<p>不同的机器学习任务通常会使用不同的损失函数。</p>
<h3 id="常见的损失函数">常见的损失函数:</h3>
<ol type="1">
<li><p>回归问题：</p>
<ul>
<li>均方误差（Mean Squared
Error，MSE）：计算预测值与真实值之差的平方的平均值，即 <span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n}(y_i -
\hat{y_i})^2\)</span>，其中 <span class="math inline">\(y_i\)</span>
是真实值，<span class="math inline">\(\hat{y_i}\)</span> 是预测值，<span class="math inline">\(n\)</span> 是样本数量。</li>
<li>平均绝对误差（Mean Absolute
Error，MAE）：计算预测值与真实值之差的绝对值的平均值，即 <span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n}|y_i -
\hat{y_i}|\)</span> 。</li>
</ul></li>
<li><p>分类问题：</p>
<ul>
<li>二分类交叉熵损失（Binary Cross Entropy
Loss）：常用于二分类问题，如逻辑回归。对于单个样本，其公式为 <span class="math inline">\(- [y \log(\hat{y}) + (1 - y) \log(1 -
\hat{y})]\)</span>，其中 <span class="math inline">\(y\)</span>
是真实标签（0 或 1），<span class="math inline">\(\hat{y}\)</span>
是预测的概率。</li>
<li>多分类交叉熵损失（Categorical Cross Entropy
Loss）：用于多分类问题，公式为 <span class="math inline">\(-\sum_{i=1}^{C} y_i \log(\hat{y_i})\)</span>，其中
<span class="math inline">\(C\)</span> 是类别数量，<span class="math inline">\(y_i\)</span> 是第 <span class="math inline">\(i\)</span> 类的真实标签（如果样本属于该类为
1，否则为 0），<span class="math inline">\(\hat{y_i}\)</span>
是模型预测样本属于第 <span class="math inline">\(i\)</span>
类的概率。</li>
</ul></li>
</ol>
<p>选择合适的损失函数对于模型的训练和性能至关重要。它会影响模型的学习速度、收敛性以及最终的泛化能力。</p>
<p>以下为您介绍几种常见的损失函数：</p>
<ol type="1">
<li><p><strong>均方误差（Mean Squared Error，MSE）</strong>：</p>
<ul>
<li>公式：<span class="math inline">\(MSE = \frac{1}{n}
\sum_{i=1}^{n}(y_i - \hat{y_i})^2\)</span></li>
<li>其中，<span class="math inline">\(y_i\)</span> 是真实值，<span class="math inline">\(\hat{y_i}\)</span> 是预测值，<span class="math inline">\(n\)</span> 是样本数量。</li>
<li>常用于回归问题，对较大的误差给予更高的惩罚。</li>
<li>例如，预测房价时，如果预测值与真实房价相差较大，MSE 会较大。</li>
</ul></li>
<li><p><strong>平均绝对误差（Mean Absolute Error，MAE）</strong>：</p>
<ul>
<li>公式：<span class="math inline">\(MAE = \frac{1}{n}
\sum_{i=1}^{n}|y_i - \hat{y_i}|\)</span></li>
<li>同样常用于回归问题，相比 MSE 对异常值更鲁棒。</li>
<li>比如，在预测股票价格时，MAE
可能更能承受个别极端价格波动的影响。</li>
</ul></li>
<li><p><strong>交叉熵损失（Cross Entropy Loss）</strong>：</p>
<ul>
<li>二分类交叉熵：<span class="math inline">\(L = - [y \log(\hat{y}) +
(1 - y) \log(1 - \hat{y})]\)</span> ，其中 <span class="math inline">\(y\)</span> 是真实标签（0 或 1），<span class="math inline">\(\hat{y}\)</span> 是预测的概率。</li>
<li>多分类交叉熵：<span class="math inline">\(L = -\sum_{i=1}^{C} y_i
\log(\hat{y_i})\)</span> ，其中 <span class="math inline">\(C\)</span>
是类别数量，<span class="math inline">\(y_i\)</span> 是第 <span class="math inline">\(i\)</span> 类的真实标签（如果样本属于该类为
1，否则为 0），<span class="math inline">\(\hat{y_i}\)</span>
是模型预测样本属于第 <span class="math inline">\(i\)</span>
类的概率。</li>
<li>广泛应用于分类问题，衡量预测概率分布与真实分布之间的差异。</li>
</ul>
<p>交叉熵损失函数（Cross-Entropy
Loss）主要用于衡量模型预测的概率分布与真实的概率分布之间的差异，它在机器学习，特别是深度学习的分类问题中被广泛使用。</p>
<p>其作用主要体现在以下方面：</p>
<ul>
<li><strong>评估模型性能</strong>：通过计算预测分布和真实分布之间的差距，来确定模型在分类任务中的表现优劣。交叉熵越小，表示两个概率分布越接近，说明模型的预测结果越接近真实标签，模型的性能也就越好。</li>
<li><strong>指导模型优化</strong>：在模型训练过程中，交叉熵损失函数的值被用于反向传播，以调整模型的参数，使得损失不断减小，从而使模型的预测逐渐逼近真实标签。</li>
<li><strong>处理多分类问题</strong>：适用于多类别分类任务，可以方便地处理具有多个类别的情况。对于每个样本，模型输出每个类别的预测概率，而交叉熵损失函数会综合考虑所有类别的预测概率和真实标签来计算损失。</li>
</ul>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720089363560.png" alt="1720089363560">
<figcaption aria-hidden="true">1720089363560</figcaption>
</figure>
<p>例如，在一个图像分类任务中，模型需要判断图片属于多个类别中的哪一个。模型的输出是每个类别的预测概率，而真实标签则是图片实际所属的类别。通过计算交叉熵损失，可以衡量模型的预测结果与真实类别之间的差异，并利用这个差异来调整模型的参数，以提高模型的分类准确性。</p>
<p>相比其他一些损失函数，交叉熵损失函数具有一些优点，例如易于理解和计算，对噪声数据具有一定的鲁棒性等。然而，它也存在一些缺点，比如对类别不平衡的数据可能较为敏感，在类别不平衡的数据集上，可能会过于关注多数类别而导致模型性能下降；并且它对输出概率分布的平滑性要求较高，如果输出概率分布过于离散，可能会导致损失值较大。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720089568706.png" alt="1720089568706">
<figcaption aria-hidden="true">1720089568706</figcaption>
</figure></li>
<li><p><strong>Hinge 损失</strong>：</p>
<ul>
<li>常用于支持向量机（SVM）中，特别是在二分类问题中。</li>
<li>对于二分类，公式为：<span class="math inline">\(L = \max(0, 1 - y
\cdot \hat{y})\)</span> ，其中 <span class="math inline">\(y\)</span>
是真实标签（1 或 -1），<span class="math inline">\(\hat{y}\)</span>
是预测值。</li>
</ul></li>
<li><p><strong>KL 散度（Kullback-Leibler Divergence）</strong>：</p>
<ul>
<li>用于衡量两个概率分布之间的差异。</li>
<li>公式：<span class="math inline">\(KL(P || Q) = \sum_{x} P(x) \log
\frac{P(x)}{Q(x)}\)</span></li>
</ul></li>
</ol>
<p>这些损失函数在不同的机器学习任务和场景中各有优缺点，选择合适的损失函数对于模型的性能和训练效果至关重要。</p>
<h3 id="损失函数的导数计算">损失函数的导数计算</h3>
<p>以下是几种常见损失函数及其导数的计算方法：</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720880004976.png" alt="1720880004976">
<figcaption aria-hidden="true">1720880004976</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720880028751.png" alt="1720880028751">
<figcaption aria-hidden="true">1720880028751</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720880055767.png" alt="1720880055767">
<figcaption aria-hidden="true">1720880055767</figcaption>
</figure>
<h2 id="监督对比损失supervised-contrastive-loss">监督对比损失（Supervised
Contrastive Loss）</h2>
<p>监督对比损失（Supervised Contrastive
Loss）是一种在深度学习中用于有监督学习任务的损失函数，<strong>主要用于提高模型对相似样本的区分能力和对不同类别样本的聚类效果</strong>。</p>
<p><strong>一、基本原理</strong></p>
<p>监督对比损失的核心思想是<strong>在有标签信息的情况下，通过拉近同一类别的样本之间的距离，同时推远不同类别的样本之间的距离，从而使模型学习到更具判别性的特征表示。</strong></p>
<p>具体来说，对于一个包含多个样本的批次，首先计算每个样本的特征向量。然后，对于每个样本，找到与其具有相同标签的正样本和不同标签的负样本。接着，计算该样本与正样本和负样本之间的相似度，通常使用余弦相似度或点积等方法。最后，根据相似度计算监督对比损失，使得正样本之间的相似度尽可能高，负样本之间的相似度尽可能低。</p>
<p><strong>二、计算方法</strong></p>
<p>假设一个批次中有 <span class="math inline">\(N\)</span>
个样本，每个样本的特征向量为 <span class="math inline">\(h_i\)</span>，对应的标签为 <span class="math inline">\(y_i\)</span>。监督对比损失的计算公式如下：</p>
<p><span class="math inline">\(L_{contrastive}=\frac{1}{2N}\sum_{i=1}^{N}[l(1-s(h_i,h_{pos(i)}))+\sum_{j=1,j\neq
i}^{N}l(s(h_i,h_{neg(j)}))]\)</span></p>
<p>其中，<span class="math inline">\(h_{pos(i)}\)</span> 表示与样本
<span class="math inline">\(i\)</span>
具有相同标签的正样本的特征向量，<span class="math inline">\(h_{neg(j)}\)</span> 表示与样本 <span class="math inline">\(i\)</span> 具有不同标签的负样本的特征向量，<span class="math inline">\(s(\cdot,\cdot)\)</span>
表示相似度函数，通常为余弦相似度，<span class="math inline">\(l(x)=-\log\frac{e^{x/T}}{e^{x/T}+\sum_{k=1,k\neq
i}^{N}e^{s(h_i,h_{neg(k)})/T}}\)</span> 是一个损失函数，<span class="math inline">\(T\)</span>
是一个温度参数，用于控制相似度的缩放。</p>
<p><strong>三、作用和优势</strong></p>
<ol type="1">
<li><p>提高特征表示的判别性</p>
<ul>
<li>通过强制同一类别的样本靠近，不同类别的样本远离，监督对比损失可以使模型学习到更具判别性的特征表示，从而提高模型在分类、聚类等任务中的性能。</li>
</ul></li>
<li><p>增强模型的鲁棒性</p>
<ul>
<li>对数据中的噪声和异常值具有一定的鲁棒性，因为它关注的是样本之间的相对关系，而不是绝对的值。</li>
</ul></li>
<li><p>适用于大规模数据集</p>
<ul>
<li>可以有效地处理大规模数据集，因为它可以在批次内进行计算，不需要对整个数据集进行计算。</li>
</ul></li>
<li><p>与其他损失函数结合使用</p>
<ul>
<li>可以与其他损失函数（如交叉熵损失）结合使用，以进一步提高模型的性能。</li>
</ul></li>
</ol>
<p><strong>四、应用场景</strong></p>
<ol type="1">
<li><p>图像分类</p>
<ul>
<li>在图像分类任务中，监督对比损失可以帮助模型学习到更具判别性的图像特征，提高分类准确性。</li>
</ul></li>
<li><p>自然语言处理</p>
<ul>
<li>例如文本分类、情感分析等任务中，监督对比损失可以使模型更好地理解文本的语义，提高任务的性能。</li>
</ul></li>
<li><p>推荐系统</p>
<ul>
<li>通过学习用户和物品的特征表示，监督对比损失可以提高推荐系统的准确性和个性化程度。</li>
</ul></li>
</ol>
<h2 id="激活函数">激活函数:</h2>
<p>在神经网络中，激活函数是一种对神经元的输入进行非线性变换的函数。</p>
<p>激活函数的主要作用包括：</p>
<ol type="1">
<li>引入非线性：如果没有激活函数，神经网络仅仅是对输入进行线性组合，其表达能力非常有限，无法处理复杂的非线性问题。通过引入非线性的激活函数，可以使神经网络能够拟合各种复杂的函数和模式。</li>
<li>控制神经元的输出范围：不同的激活函数会将输入映射到不同的输出范围，例如
<code>Sigmoid</code> 函数将输出限制在 <code>(0, 1)</code>
之间，<code>Tanh</code> 函数将输出限制在 <code>(-1, 1)</code>
之间。</li>
<li>增加网络的稀疏性：某些激活函数，如 <code>ReLU</code> （Rectified
Linear Unit），当输入为负数时输出为
0，这有助于在网络中引入稀疏性，减少计算量，并可能有助于防止过拟合。</li>
</ol>
<p>常见的激活函数有：</p>
<ol type="1">
<li><p><code>Sigmoid</code>
函数：<code>f(x) = 1 / (1 + e^(-x))</code>，输出范围在
<code>(0, 1)</code> 之间，常用于二分类问题的输出层。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image.png" alt="alt text"></p>
<p>可以看出，sigmoid函数连续，光滑，严格单调，以(0,0.5)中心对称，是一个非常良好的阈值函数。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719936221442.png" alt="1719936221442">
<figcaption aria-hidden="true">1719936221442</figcaption>
</figure></li>
<li><p><code>Tanh</code>
函数：<code>f(x) = (e^x - e^(-x)) / (e^x + e^(-x))</code>，输出范围在
<code>(-1, 1)</code> 之间。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-1.png" alt="alt text"></p></li>
<li><p><code>ReLU</code>
函数：<code>f(x) = max(0, x)</code>，计算简单，在很多深度神经网络中广泛使用。
线性整流函数，又称修正线性单元ReLU，是一种人工神经网络中常用的激活函数，通常指代以斜坡函数及其变种为代表的非线性函数。</p>
<p>线性整流函数（ReLU函数）的特点：</p>
<p>当输入为正时，不存在梯度饱和问题。 计算速度快得多。ReLU
函数中只存在线性关系，因此它的计算速度比Sigmoid函数和tanh函数更快。 Dead
ReLU问题。当输入为负时，ReLU完全失效，在正向传播过程中，这不是问题。有些区域很敏感，有些则不敏感。但是在反向传播过程中，如果输入负数，则梯度将完全为零，Sigmoid函数和tanh函数也具有相同的问题
ReLU函数的输出为0或正数，这意味着ReLU函数不是以0为中心的函数。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-2.png" alt="alt text"></p></li>
<li><p><code>Leaky ReLU</code>
函数：<code>f(x) = max(0.01x, x)</code>，是 <code>ReLU</code>
的改进版，解决了 <code>ReLU</code> 中神经元可能“死亡”的问题。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719937549074.png" alt="1719937549074">
<figcaption aria-hidden="true">1719937549074</figcaption>
</figure></li>
</ol>
<p>Leaky ReLU函数的特点：</p>
<p>Leaky ReLU函数通过把x xx的非常小的线性分量给予负输入0.01 x
0.01x0.01x来调整负值的零梯度问题。 Leaky有助于扩大ReLU函数的范围，通常α
0.01左右。 Leaky ReLU的函数范围是负无穷到正无穷。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-3.png" alt="alt text"></p>
<p>下面是一个使用 <code>Sigmoid</code> 激活函数的简单示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line">x = np.array([-<span class="number">2</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(sigmoid(x))</span><br></pre></td></tr></table></figure>
<p>激活函数的选择对神经网络的性能有很大影响，需要根据具体问题和网络结构进行合适的选择。</p>
<h3 id="常见的激活函数">常见的激活函数：</h3>
<ol type="1">
<li><p><strong>Sigmoid 函数</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = 1 / (1 + e^(-x))</code></li>
<li>特点：将输入值压缩到 0 到 1
之间，具有平滑的曲线。但在输入值较大或较小时，梯度接近
0，可能导致梯度消失问题。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image.png" alt="alt text"></li>
</ul></li>
<li><p><strong>Tanh 函数</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = (e^x - e^(-x)) / (e^x + e^(-x))</code></li>
<li>特点：将输入值压缩到 -1 到 1 之间，相比 Sigmoid 函数，以 0 为中心。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-1.png" alt="alt text"></li>
</ul></li>
<li><p><strong>ReLU 函数（Rectified Linear Unit）</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = max(0, x)</code></li>
<li>特点：计算简单，在正半轴上梯度恒为
1，有效缓解了梯度消失问题。但存在神经元“死亡”的可能，即输入为负时永远不被激活。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-2.png" alt="alt text"></li>
</ul></li>
<li><p><strong>Leaky ReLU 函数</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = max(ax, x)</code> ，其中 <code>a</code>
是一个较小的正数（如 0.01）</li>
<li>特点：对 ReLU 进行改进，解决了神经元“死亡”的问题。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719937549074.png" alt="1719937549074"></li>
</ul></li>
<li><p><strong>ELU 函数（Exponential Linear Unit）</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = x if x &gt; 0 else a(e^x - 1)</code> ，其中
<code>a</code> 是一个常数</li>
<li>特点：具有 ReLU 的优点，同时在输入为负时输出不为
0，使得平均输出更接近 0。</li>
<li><figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719937783555.png" alt="1719937783555">
<figcaption aria-hidden="true">1719937783555</figcaption>
</figure></li>
</ul></li>
<li><p><strong>Softmax 函数</strong>：</p>
<ul>
<li>常用于多分类问题的输出层，将多个神经元的输出值映射为概率分布。</li>
</ul></li>
</ol>
<p>以下是一个使用 Python 绘制部分常见激活函数图像的示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tanh</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.tanh(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">leaky_relu</span>(<span class="params">x, a=<span class="number">0.01</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(a * x, x)</span><br><span class="line"></span><br><span class="line">x = np.linspace(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">plt.plot(x, sigmoid(x), label=<span class="string">&#x27;Sigmoid&#x27;</span>)</span><br><span class="line">plt.plot(x, tanh(x), label=<span class="string">&#x27;Tanh&#x27;</span>)</span><br><span class="line">plt.plot(x, relu(x), label=<span class="string">&#x27;ReLU&#x27;</span>)</span><br><span class="line">plt.plot(x, leaky_relu(x), label=<span class="string">&#x27;Leaky ReLU&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Input&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Output&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Common Activation Functions&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>不同的激活函数在不同的场景和网络结构中表现各异，需要根据具体问题进行选择和调整。</p>
<h2 id="学习率">学习率</h2>
<p>学习率（Learning Rate）在机器学习中是一个非常关键的超参数。</p>
<p>学习率决定了模型在训练过程中参数更新的步长大小。</p>
<p>较小的学习率意味着模型在参数更新时采取较小的步幅，可能会使训练过程更加稳定，但收敛速度较慢，需要更多的训练迭代次数才能达到较好的效果。</p>
<p>例如，如果学习率过小，如 0.0001
，模型可能会在训练中进展缓慢，需要大量的训练轮数才能逐渐优化参数。</p>
<p>较大的学习率会使模型在参数更新时采取较大的步幅，可能会加快收敛速度，但也可能导致模型跳过最优解，甚至无法收敛。</p>
<p>比如，学习率过大，如 10
，模型可能会在训练中出现剧烈的波动，无法稳定地优化参数。</p>
<p>选择合适的学习率通常需要通过试验和错误来确定。常见的方法包括使用固定的学习率、学习率衰减（随着训练的进行逐渐减小学习率）、自适应学习率算法（如
Adam 优化器中的自适应学习率调整）等。</p>
<p>例如，在深度学习中，训练开始时可以使用较大的学习率，如 0.1
，然后随着训练的进行，逐渐将学习率减小到 0.001
，以实现更精细的参数调整和更好的收敛效果。</p>
<h3 id="以下是一些常见的学习率调整方法">以下是一些常见的学习率调整方法：</h3>
<ol type="1">
<li><p><strong>固定学习率</strong>：在整个训练过程中保持学习率不变。这种方法简单，但可能不是最优的，因为在训练的不同阶段可能需要不同的学习率。</p></li>
<li><p><strong>分段常数学习率</strong>：将训练过程分为几个阶段，每个阶段使用不同的固定学习率。例如，在前几个
epoch 使用较大的学习率，然后在后续阶段使用较小的学习率。</p></li>
<li><p><strong>学习率衰减</strong>：</p>
<ul>
<li><strong>按步长衰减</strong>：每隔一定的训练步数或
epoch，将学习率乘以一个小于 1 的衰减因子。</li>
<li><strong>指数衰减</strong>：学习率按照指数形式衰减，例如
<code>learning_rate = initial_learning_rate * decay_rate ^ (epoch / decay_steps)</code>
。</li>
<li><strong>多项式衰减</strong>：学习率按照多项式的形式逐渐减小。</li>
</ul></li>
<li><p><strong>自适应学习率算法</strong>：</p>
<ul>
<li><strong>Adagrad</strong>：根据每个参数之前的梯度历史来调整学习率，对于不常更新的参数给予较大的学习率，对于频繁更新的参数给予较小的学习率。</li>
<li><strong>Adadelta</strong>：是对 Adagrad
的改进，避免了学习率单调递减的问题。</li>
<li><strong>RMSProp</strong>：类似于
Adadelta，对梯度的二阶矩进行指数加权平均来调整学习率。</li>
<li><strong>Adam</strong>：结合了动量和 RMSProp
的优点，能够自适应地调整学习率。</li>
</ul></li>
<li><p><strong>余弦退火</strong>：学习率按照余弦函数的形式进行周期性的变化，在每个周期内从较高的值逐渐降低到较低的值，然后再上升。</p></li>
<li><p><strong>基于验证集性能调整</strong>：根据模型在验证集上的性能来动态调整学习率。如果验证集性能在一段时间内没有改善，就降低学习率。</p></li>
</ol>
<p>这些方法各有优缺点，具体选择哪种方法取决于数据集、模型架构和训练需求等因素。通常需要通过实验来找到最适合特定问题的学习率调整策略。</p>
<h2 id="optimizer的概念">optimizer的概念</h2>
<p>在机器学习中，<code>optimizer</code>（优化器）是用于调整模型参数以最小化损失函数或最大化目标函数的算法或策略。</p>
<p>优化器的主要作用是根据模型的当前状态（包括参数值和计算得到的梯度）来决定如何更新模型的参数，以使得模型在训练数据上的性能逐渐提高。</p>
<p>常见的优化器有随机梯度下降（Stochastic Gradient
Descent，SGD）、Adagrad、Adadelta、RMSprop、Adam 等。</p>
<p>以随机梯度下降（SGD）为例，它的基本思想是沿着梯度的反方向，以一定的学习率来更新参数。假设参数为
<code>w</code> ，梯度为 <code>g</code> ，学习率为 <code>lr</code>
，则更新公式为 <code>w = w - lr * g</code> 。</p>
<p>Adagrad
则根据每个参数之前的梯度历史来自适应地调整学习率。对于频繁更新的参数，学习率会逐渐减小，而对于很少更新的参数，学习率会相对较大。</p>
<p>RMSprop 类似于
Adagrad，但它不是累积所有的梯度平方，而是使用指数加权平均来计算梯度平方的估计值。</p>
<p>Adam 结合了动量和 RMSprop
的优点，同时考虑了梯度的一阶矩和二阶矩来动态调整学习率。</p>
<p>选择合适的优化器对于模型的训练效率和最终性能至关重要。例如，在数据量较大且模型较复杂时，Adam
通常能取得较好的效果；而在一些简单的模型和小数据集上，SGD
可能表现不错，并且通过适当的调整学习率等超参数，也能获得较好的结果。</p>
<p>再比如，在处理具有稀疏特征的问题时，Adagrad
可能更合适。总之，优化器的选择需要根据具体的问题和数据特点来决定，并可能需要通过实验来找到最优的选择。</p>
<h2 id="图像分割image-segmentation">图像分割(Image Segmentation)</h2>
<p>原文:
https://0809zheng.github.io/2020/05/07/semantic-segmentation.html</p>
<p>图像分割 (Image
Segmentation)是对图像中的每个像素进行分类，可以细分为：</p>
<p>语义分割 (semantic
segmentation)：注重类别之间的区分，而不区分同一类别的不同个体； 实例分割
(instance segmentation)：注重类别以及同一类别的不同个体之间的区分；
全景分割 (panoptic
segmentation)：对于可数的对象实例(如行人、汽车)做实例分割，对于不可数的语义区域(如天空、地面)做语义分割。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-58.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>语义分割模型可以直接根据图像像素进行分组，转换为密集的分类问题。实例分割一般可分为“自上而下”
和
“自下而上”的方法，自上而下的框架是先计算实例的检测框，在检测框内进行分割；自下而上的框架则是先进行语义分割，在分割结果上对实例对象进行检测。全景分割在实例分割框架上添加语义分割分支，或基于语义分割方法采用不同的像素分组策略。本文重点关注语义分割方法。</p>
<p>本文目录：</p>
<p>图像分割模型 图像分割的评估指标 图像分割的损失函数
常用的图像分割数据集</p>
<h3 id="图像分割模型">1、 图像分割模型</h3>
<p>图像分割的任务是使用深度学习模型处理输入图像，得到<strong>带有语义标签的相同尺寸的输出图像</strong>。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-59.png" alt="alt text"></p>
<p>图像分割模型通常采用编码器-解码器(encoder-decoder)结构。编码器从预处理的图像数据中提取特征，解码器把特征解码为分割热图。图像分割模型的发展趋势可以大致总结为：</p>
<ul>
<li>全卷积网络：FCN, SegNet, RefineNet, U-Net, V-Net, M-Net, W-Net,
Y-Net, UNet++, Attention U-Net, GRUU-Net, BiSeNet V1,2, DFANet,
SegNeXt</li>
<li>上下文模块：DeepLab v1,2,3,3+, PSPNet, FPN, UPerNet, EncNet, PSANet,
APCNet, DMNet, OCRNet, PointRend, K-Net</li>
<li>基于Transformer：SETR, TransUNet, SegFormer, Segmenter, MaskFormer,
SAM</li>
<li>通用技巧：Deep Supervision, Self-Correction</li>
</ul>
<h4 id="基于全卷积网络的图像分割模型">(1)
基于全卷积网络的图像分割模型</h4>
<p>标准卷积神经网络包括卷积层、下采样层和全连接层。早期基于深度学习的图像分割模型为生成与输入图像尺寸一致的分割结果，丢弃了全连接层，并引入一系列上采样操作。因此这一阶段的模型旨在解决如何更好从卷积下采样中恢复丢掉的信息损失，逐渐形成了以U-Net为核心的对称编码器-解码器结构。</p>
<p><strong>FCN</strong></p>
<p>FCN提出用全卷积网络来处理语义分割问题。首先通过全卷积网络进行特征提取和下采样，然后通过双线性插值进行上采样。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-60.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p><strong>SegNet</strong></p>
<p>SegNet设计了对称的编码器-解码器结构，通过反池化进行上采样。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-61.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p><strong>RefineNet</strong></p>
<p>RefineNet把编码器产生的多个分辨率特征进行一系列卷积、融合、池化。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-62.png" alt="alt text">
U-Net使用对称的U型网络设计，在对应的下采样和上采样之间引入跳跃连接。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-63.png" alt="alt text"></p>
<p>⚪ V-Net V-Net是3D版本的U-Net，下采样使用步长为2的卷积。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-64.png" alt="alt text"></p>
<p>⚪ M-Net M-Net在U-Net的基础上引入了left leg和right leg。left
leg使用最大池化不断下采样数据，right
leg则对数据进行上采样并叠加到每一层次的输出后。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-65.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ W-Net
W-Net通过堆叠两个U-Net实现无监督的图像分割。编码器U-Net提取分割表示，解码器U-Net重构原始图像。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-66.png" alt="alt text"></p>
<p>⚪ Y-Net
Y-Net在U-Net的编码位置后增加了一个概率图预测结构，在分割任务的基础上额外引入了分类任务。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-67.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ UNet++
UNet++通过跳跃连接融合了不同深度的U-Net，并为每级U-Net引入深度监督。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-68.png" alt="alt text"></p>
<p>⚪ Attention U-Net Attention U-Net通过引入Attention
gate模块将空间注意力机制集成到U-Net的跳跃连接和上采样模块中。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-69.png" alt="alt text"></p>
<p>⚪ GRUU-Net
GRUU-Net通过循环神经网络构造U型网络，根据多个尺度上的CNN和RNN特征聚合来细化分割结果。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-70.png" alt="alt text"></p>
<p>⚪ BiSeNet BiSeNet设计了一个双边结构，分别为空间路径（Spatial
Path）和上下文路径（Context
Path）。通过一个特征融合模块（FFM）将两个路径的特征进行融合，可以实现实时语义分割。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-71.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ BiSeNet V2 BiSeNet V2精心设计了Detail Branch和Semantic
Branch，使用更加轻巧的深度可分离卷积来加速模型；通过Aggregation
Layer进行特征聚合；并额外引入辅助损失。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-72.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ DFANet
DFANet以修改过的Xception为backbone网络，设计了一种多分支的特征重用框架来融合空间细节和上下文信息。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-73.png" alt="alt text"></p>
<p>⚪ SegNeXt
SegNeXt的编码器部分采用ViT的结构，自注意力模块通过一种多尺度卷积注意力模块MSCA实现。解码器部分采用轻量型Hamberger模块对后三个阶段的特性进行聚合。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-74.png" alt="alt text"></p>
<ol start="2" type="1">
<li>基于上下文模块的图像分割模型
多尺度问题是指当图像中的目标对象存在不同大小时，分割效果不佳的现象。比如同样的物体，在近处拍摄时物体显得大，远处拍摄时显得小。解决多尺度问题的目标就是不论目标对象是大还是小，网络都能将其分割地很好。</li>
</ol>
<p>随着图像分割模型的效果不断提升，分割任务的主要矛盾逐渐从恢复像素信息逐渐演变为如何更有效地利用上下文(context)信息，并基于此设计了一系列用于提取多尺度特征的网络结构。</p>
<p>这一时期的分割网络的基本结构为：首先使用预训练模型(如ResNet)提取图像特征(通常8×下采样)，然后应用精心设计的上下文模块增强多尺度特征信息，最后对特征应用上采样(通常为8x上采样)和1x1分割头生成分割结果。</p>
<p>有一些方法把自注意力机制引入图像分割任务，通过自注意力机制的全局交互性来捕获视觉场景中的全局依赖，并以此构造上下文模块。对于这些方法的讨论详见卷积神经网络中的自注意力机制。</p>
<p>⚪ Deeplab
Deeplab引入空洞卷积进行图像分割任务，并使用全连接条件随机场精细化分割结果。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-75.png" alt="alt text"></p>
<p>⚪ DeepLab v2 Deeplab v2引入了空洞空间金字塔池化层
ASPP，即带有不同扩张率的空洞卷积的金字塔池化。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-76.png" alt="alt text"></p>
<p>⚪ DeepLab v3 Deeplab v3对ASPP模块做了升级，把扩张率调整为[1, 6, 12,
18]，并增加了全局平均池化： <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-77.png" alt="alt text"></p>
<p>⚪ DeepLab v3+ Deeplab v3+采用了编码器-解码器结构。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-78.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>上述Deeplab模型的对比如下：</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-79.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ PSPNet PSPNet引入了金字塔池化模块
PPM。PPM模块并联了四个不同大小的平均池化层，经过卷积和上采样恢复到原始大小。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-80.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ FPN 特征金字塔网络
FPN金字塔把编码器每一层的特征通过卷积和上采样合并为输出语义特征。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-81.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ UPerNet
UPerNet设计了一个基于FPN和PPM的多任务分割范式，为每一个task设计了不同的检测头，可执行场景分类、目标和部位分割、材质和纹理检测。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-82.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ EncNet EncNet引入了上下文编码模块
CEM，通过字典学习和残差编码捕获全局场景上下文信息；并通过语义编码损失
SE-loss强化网络学习上下文语义。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-83.png" alt="alt text"></p>
<p>⚪ PSANet PSANet引入了逐点空间注意力
PSA建立每个特征像素与其他像素之间的联系，并且设计了双向的信息流传播路径。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-84.png" alt="alt text"></p>
<p>⚪ APCNet
APCNet使用了自适应上下文模块ACM计算每个局部位置的上下文向量，并与原始特征图进行加权实现聚合上下文信息的作用。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-85.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ DMNet
DMNet使用了动态卷积模块DCM来捕获多尺度语义信息，每一个DCM模块都可以处理与输入尺寸相关的比例变化。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-86.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ OCRNet
OCRNet根据预测结果和输出像素特征计算类别特征，然后计算像素特征与类别特征的相似度，用于增强特征的上下文表示。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-87.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ PointRend PointRend从coarse
prediction中挑选N个“难点”，根据其fine-grained features和coarse
prediction构造点特征向量，通过MLP网络对这些难点进行重新预测。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-88.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ K-Net
K-Net提出了一种基于动态内核的分割模型，为每个任务分配不同的核来实现语义分割、实例分割和全景分割的统一。具体地，使用N个Kernel将图像划分为N组，每个Kernel都负责找到属于其相应组的像素，并应用Kernel
Update Head增强Kernel的特征提取能力。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-89.png" alt="alt text"></p>
<ol start="3" type="1">
<li>基于Transformer的图像分割模型
Transformer是一种基于自注意力机制的序列处理模型，该模型在任意层都能实现全局的感受野，建立全局依赖；而且无需进行较大程度的下采样就能实现特征提取，保留了图像的更多信息。</li>
</ol>
<p>⚪ SETR
SETR采取了ViT作为编码器提取图像特征；通过基于卷积的渐进上采样或者多层次特征聚合生成分割结果。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-90.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ TransUNet
TransUNet的Encoder部分主要由ResNet50和ViT组成，其中前三个模块为两倍下采样的ResNet
Block，最后一个模块为12层Transformer Layer。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-91.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ SegFormer
SegFormer包括用于生成多尺度特征的分层Encoder（包含Efficient
Self-Attention、Mix-FFN和Overlap Patch
Embedding三个模块）和仅由MLP层组成的轻量级All-MLP Decoder。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-92.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ Segmenter
Segmenter完全基于Transformer的编码器-解码器架构。图像块序列由Transformer编码器编码，并由mask
Transformer解码。Mask
Transformer引入一组个可学习的类别嵌入，通过计算其与解码序列特征的乘积来生成每个图像块的分割图。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-93.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ MaskFormer
MaskFormer把分割问题看作掩码级的分类问题。对输入图像生成<span class="math inline">\(N\)</span>个二值掩码，并为每个掩码预测<span class="math inline">\(K+1\)</span>个类别中的某个。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-94.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>⚪ Segment Anything
SAM是一个图像分割的基础模型，模型的设计和训练是通过提示工程实现的。SAM采用一种简单的设计：一个图像编码器生成图像嵌入，一个提示编码器生成提示嵌入，然后将这两种嵌入组合后通过一个轻量级掩码解码器预测分割掩码。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-95.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<ol start="4" type="1">
<li>分割模型中的通用技巧 ⚪ Deep Supervision
Reference：Deeply-Supervised Nets、Training Deeper Convolutional
Networks with Deep Supervision 深度监督（Deep
Supervision）是在深度神经网络的某些隐藏层后增加一个辅助的分类器作为一种网络分支来对主干网络进行监督的技巧，用来解决深度神经网络训练梯度消失和收敛速度过慢等问题。</li>
</ol>
<p>一个带有深度监督的八层卷积网络结构如下图所示。在Conv4之后添加了一个监督分类器作为分支。Conv4输出的特征图除了随着主网络进入Conv5之外，也作为输入进入分支分类器。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-96.png" alt="alt text"></p>
<p>⚪ Self-Correction Reference：Self-Correction for Human Parsing
图像分割任务的标签可能存在噪声。自校正（Self-Correction）是一种净化分割标签噪声的方法。模型训练从具有噪声的标签出发，通过聚合当前模型和前一个最优模型的参数来推断更可靠的标签，并用这些更正的标签训练更鲁棒的模型。</p>
<p>自校正包括模型聚合和标签聚合两个过程。对于模型聚合，记录当前轮数的训练权重<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-97.png" alt="alt text">与之前训练的最优权重<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-98.png" alt="alt text">，得到融合权重并更新历史最优权重：</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-99.png" alt="alt text">
标签的更新类似，通过融合当前预测结果<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-102.png" alt="alt text">和前一轮标签<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-101.png" alt="alt text">获得类别关系更明确的新标签：</p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-100.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-103.png" alt="alt text"></p>
<p>参考文献 Fully Convolutional Networks for Semantic
Segmentation：(arXiv1411)FCN: 语义分割的全卷积网络。 Semantic Image
Segmentation with Deep Convolutional Nets and Fully Connected
CRFs：(arXiv1412)DeepLab:
通过深度卷积网络和全连接条件随机场实现图像语义分割。 U-Net:
Convolutional Networks for Biomedical Image
Segmentation：(arXiv1505)U-Net: 用于医学图像分割的卷积网络。 SegNet: A
Deep Convolutional Encoder-Decoder Architecture for Image
Segmentation：(arXiv1511)SegNet: 图像分割的深度卷积编码器-解码器结构。
V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image
Segmentation：(arXiv1606)V-Net：用于三维医学图像分割的全卷积网络。
DeepLab: Semantic Image Segmentation with Deep Convolutional Nets,
Atrous Convolution, and Fully Connected CRFs：(arXiv1606)DeepLab v2:
通过带有空洞卷积的金字塔池化实现图像语义分割。 RefineNet: Multi-Path
Refinement Networks for High-Resolution Semantic
Segmentation：(arXiv1611)RefineNet: 高分辨率语义分割的多路径优化网络。
Pyramid Scene Parsing Network：(arXiv1612)PSPNet: 金字塔场景解析网络。
M-Net: A Convolutional Neural Network for Deep Brain Structure
Segmentation：(ISBI 2017)M-Net：用于三维脑结构分割的二维卷积神经网络。
Rethinking Atrous Convolution for Semantic Image
Segmentation：(arXiv1706)DeepLab v3: 重新评估图像语义分割中的扩张卷积。
W-Net: A Deep Model for Fully Unsupervised Image
Segmentation：(arXiv1711)W-Net：一种无监督的图像分割方法。
Encoder-Decoder with Atrous Separable Convolution for Semantic Image
Segmentation：(arXiv1802)DeepLab v3+: 图像语义分割中的扩张可分离卷积。
Context Encoding for Semantic Segmentation：(arXiv1803)EncNet:
语义分割的上下文编码。 Attention U-Net: Learning Where to Look for the
Pancreas：(arXiv1804)Attention U-Net: 向U-Net引入注意力机制。 Y-Net:
Joint Segmentation and Classification for Diagnosis of Breast Biopsy
Images：(arXiv1806)Y-Net：乳腺活检图像的分割和分类。 UNet++: A Nested
U-Net Architecture for Medical Image
Segmentation：(arXiv1807)UNet++：用于医学图像分割的巢型UNet。 Unified
Perceptual Parsing for Scene Understanding：(arXiv1807)UPerNet:
场景理解的统一感知解析。 BiSeNet: Bilateral Segmentation Network for
Real-time Semantic Segmentation：(arXiv1808)BiSeNet:
实时语义分割的双边分割网络。 PSANet: Point-wise Spatial Attention
Network for Scene Parsing：(ECCV2018)PSANet:
场景解析的逐点空间注意力网络。 GRUU-Net: Integrated convolutional and
gated recurrent neural network for cell segmentation：(Medical Image
Analysis2018)GRUU-Net: 细胞分割的融合卷积门控循环神经网络。 Panoptic
Feature Pyramid Networks：(arXiv1901)全景特征金字塔网络。 DFANet: Deep
Feature Aggregation for Real-Time Semantic
Segmentation：(arXiv1904)DFANet: 实时语义分割的深度特征聚合。
Object-Contextual Representations for Semantic
Segmentation：(arXiv1909)OCRNet：语义分割中的目标上下文表示。 PointRend:
Image Segmentation as Rendering：(arXiv1912)PointRend:
把图像分割建模为渲染。 Adaptive Pyramid Context Network for Semantic
Segmentation：(CVPR2019)APCNet: 语义分割的自适应金字塔上下文网络。
Dynamic Multi-Scale Filters for Semantic Segmentation：(ICCV2019)DMNet:
语义分割的动态多尺度滤波器。 BiSeNet V2: Bilateral Network with Guided
Aggregation for Real-time Semantic Segmentation：(arXiv2004)BiSeNet V2:
实时语义分割的带引导聚合的双边网络。 Rethinking Semantic Segmentation
from a Sequence-to-Sequence Perspective with
Transformers：(arXiv2012)用Transformer从序列到序列的角度重新思考语义分割。
TransUNet: Transformers Make Strong Encoders for Medical Image
Segmentation：(arXiv2102)TransUNet：用Transformer为医学图像分割构造强力编码器。
SegFormer: Simple and Efficient Design for Semantic Segmentation with
Transformers：(arXiv2105)SegFormer：为语义分割设计的简单高效的Transformer模型。
Segmenter: Transformer for Semantic
Segmentation：(arXiv2105)Segmenter：为语义分割设计的视觉Transformer。
K-Net: Towards Unified Image Segmentation：(arXiv2106)K-Net:
面向统一的图像分割。 Per-Pixel Classification is Not All You Needfor
Semantic
Segmentation：(arXiv2107)MaskFormer：逐像素分类并不是语义分割所必需的。
SegNeXt: Rethinking Convolutional Attention Design for Semantic
Segmentation：(arXiv2209)SegNeXt：重新思考语义分割中的卷积注意力设计。
Segment Anything：(arXiv2304)SAM：分割任意模型</p>
<h2 id="pytorch代码分析">pytorch代码分析:</h2>
<p>如何理解下面的代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">		<span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">		self.layer = nn.Linear(n_feature, n_hidden)</span><br><span class="line">		self.all_act = nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">		x = self.layer(x)</span><br><span class="line">		x = torch.tanh(x)</span><br><span class="line">		x = self.all_act(x)</span><br><span class="line">		<span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这段代码是一个简单的神经网络模型的定义，使用 PyTorch
框架来构建。让我解释一下这段代码的含义：</p>
<ol type="1">
<li><strong>class Net(nn.Module):</strong>：定义了一个名为
<strong>Net</strong> 的类，继承自 <strong>nn.Module</strong>
类，表示这个类是一个神经网络模型。</li>
</ol>
<p><strong>2. </strong>def <strong>init</strong>(self, n_feature,
n_hidden, n_output):<strong>：定义了 </strong>Net** 类的构造函数
<strong><strong>init</strong></strong>，该函数接受三个参数
<strong>n_feature</strong>、<strong>n_hidden</strong> 和
<strong>n_output</strong>，分别表示输入特征的数量、隐藏层神经元的数量和输出的数量。**</p>
<p><strong>3. </strong>super(Net, self).__init__()<strong>：调用父类
</strong>nn.Module** 的构造函数，初始化神经网络模型。**</p>
<p><strong>4. </strong>self.layer = nn.Linear(n_feature,
n_hidden)<strong>：定义了一个线性层
</strong>layer<strong>，输入特征数量为
</strong>n_feature<strong>，输出特征数量为
</strong>n_hidden<strong>。</strong></p>
<p><strong>5. </strong>self.all_act = nn.Linear(n_hidden,
n_output)<strong>：定义了另一个线性层
</strong>all_act<strong>，输入特征数量为
</strong>n_hidden<strong>，输出特征数量为
</strong>n_output<strong>。</strong></p>
<ol start="6" type="1">
<li><strong>def forward(self, x):</strong>：定义了前向传播函数
<strong>forward</strong>，接受输入
<strong>x</strong>，表示对输入数据进行前向传播计算。</li>
<li><strong>x = self.layer(x)</strong>：将输入 <strong>x</strong> 经过
<strong>layer</strong> 线性层的计算，得到隐藏层的输出。</li>
<li><strong>x =
torch.tanh(x)</strong>：对隐藏层的输出应用双曲正切函数（tanh）作为激活函数，增加模型的非线性能力。</li>
<li><strong>x =
self.all_act(x)</strong>：将经过激活函数后的隐藏层输出再经过
<strong>all_act</strong> 线性层的计算，得到最终的输出。</li>
<li><strong>return x</strong>：返回神经网络模型的输出。</li>
</ol>
<p>这段代码定义了一个简单的神经网络模型，包括一个输入层到隐藏层的线性变换和隐藏层到输出层的线性变换，中间使用了双曲正切函数作为激活函数。在神经网络的训练过程中，可以通过调用
<strong>forward</strong> 函数来进行前向传播计算。</p>
<h3 id="tf.one_hot">tf.one_hot</h3>
<p><code>tf.one_hot</code> 是 TensorFlow
中的一个函数，用于将输入的索引值转换为独热编码（One-Hot
Encoding）的张量。</p>
<p>独热编码是一种将类别变量转换为二进制向量的编码方式，其中只有一个元素为
1 ，其余元素为 0 。</p>
<p>以下是 <code>tf.one_hot</code> 函数的一般用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">indices = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">depth = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">one_hot_encoded = tf.one_hot(indices, depth)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(one_hot_encoded.numpy())</span><br></pre></td></tr></table></figure>
<p>在上述示例中，<code>indices</code>
是要编码的索引值列表，<code>depth</code>
表示编码的维度（即类别数量）。</p>
<p>例如，对于索引值 <code>0</code> ，在维度为 4
的编码中，得到的独热编码为 <code>[1, 0, 0, 0]</code> ；对于索引值
<code>2</code> ，得到的编码为 <code>[0, 0, 1, 0]</code> 。</p>
<p><code>tf.one_hot</code>
常用于将分类标签转换为适合神经网络输入的形式，方便模型进行处理和计算。</p>
<p>上述代码中，<code>indices = [0, 2, 1, 3]</code> 且
<code>depth = 4</code> ，使用 <code>tf.one_hot</code>
函数进行独热编码后的输出结果应该是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[1 0 0 0]</span><br><span class="line"> [0 0 1 0]</span><br><span class="line"> [0 1 0 0]</span><br><span class="line"> [0 0 0 1]]</span><br></pre></td></tr></table></figure>
<p>即索引为 0 的位置编码为 <code>[1, 0, 0, 0]</code> ，索引为 2
的位置编码为 <code>[0, 0, 1, 0]</code> ，索引为 1 的位置编码为
<code>[0, 1, 0, 0]</code> ，索引为 3 的位置编码为
<code>[0, 0, 0, 1]</code> 。</p>
<h3 id="self.critic_net.parameters">self.critic_net.parameters()</h3>
<p><code>self.critic_net.parameters()</code> 通常是在 Python
的深度学习框架（如 PyTorch）中使用的代码。</p>
<p><code>critic_net</code>
可能是您定义的一个神经网络模型（例如，用于评估或批评某些数据的模型）。</p>
<p><code>parameters()</code>
方法返回模型中的可学习参数，这些参数通常是权重（weights）和偏置（biases）。</p>
<p>例如，如果 <code>critic_net</code>
是一个简单的全连接神经网络，那么通过
<code>self.critic_net.parameters()</code>
您将获取到该网络中所有层的权重和偏置的迭代器。</p>
<p>您可以使用这个返回值来进行一些操作，比如：</p>
<ol type="1">
<li><p>在优化器中使用它来进行参数的更新，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(self.critic_net.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>对参数进行一些统计或分析，比如计算参数的数量、查看参数的值等。</p></li>
</ol>
<p>总之，<code>self.critic_net.parameters()</code>
为您提供了对模型可学习参数的访问，以便进行各种与模型训练和优化相关的操作。</p>
<h3 id="self.optimizer.zero_grad">self.optimizer.zero_grad()</h3>
<p><code>self.optimizer.zero_grad()</code>
通常用于将模型参数的梯度清零。</p>
<p>当进行反向传播计算梯度后，如果不将梯度清零，那么在后续的迭代中，新计算的梯度会与之前的梯度累加。这会导致梯度计算的错误，影响模型的训练效果</p>
<h3 id="loss.backwardretain_graphtrue">loss.backward(retain_graph=True)</h3>
<p>在深度学习中，<code>loss.backward(retain_graph=True)</code>
用于计算损失函数 <code>loss</code> 关于模型参数的梯度，同时通过设置
<code>retain_graph=True</code> 来保留计算图。</p>
<p>通常，在进行一次反向传播计算梯度后，计算图会被释放以节省内存。但当您需要多次对同一计算图执行反向传播时，就需要设置
<code>retain_graph=True</code> 。</p>
<p>例如，如果您在一个循环中多次计算梯度并更新参数，像下面这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">    <span class="comment"># 前向传播，计算损失</span></span><br><span class="line">    loss = some_operation()</span><br><span class="line">    <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">    loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 根据梯度更新参数</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>在上述示例中，由于在循环内多次执行反向传播，所以需要保留计算图，以便下一次迭代能够再次基于相同的计算图计算梯度。</p>
<p>如果不设置 <code>retain_graph=True</code> ，在第二次执行
<code>loss.backward()</code> 时会报错，因为计算图已经被释放。</p>
<p>需要注意的是，一直保留计算图会占用较多内存，所以在不需要多次使用计算图时，应避免设置
<code>retain_graph=True</code> ，以节省内存资源。</p>
<p>在 PyTorch 中，<code>retain_graph=True</code>
的作用是在执行反向传播（<code>backward</code>）后保留计算图。</p>
<p>PyTorch
采用动态图机制，默认情况下，每次反向传播之后都会释放计算图以节省内存。然而，当需要对同一个计算图进行多次反向传播时，就需要设置
<code>retain_graph=True</code>。</p>
<p>例如，假设有一个输入 <code>x</code>，经过一些计算得到
<code>y</code>，然后有两个输出 <code>output_1</code> 和
<code>output_2</code>，都需要对其执行反向传播。如果在第一次对
<code>output_1</code> 执行 <code>backward</code> 时没有设置
<code>retain_graph=True</code>，那么在执行完后计算图就会被释放。当再对
<code>output_2</code> 执行 <code>backward</code>
时，由于计算图已经不存在，就会引发错误。</p>
<p>通过设置 <code>retain_graph=True</code>，可以保留中间参数，使得多次
<code>backward</code> 之间不会相互影响。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss1.backward(retain_graph=<span class="literal">True</span>)  <span class="comment"># 第一次 backward，保留计算图</span></span><br><span class="line">loss2.backward()  <span class="comment"># 第二次 backward，此时计算图仍然存在</span></span><br></pre></td></tr></table></figure>
<p>需要注意的是，一直保留计算图会占用较多内存，所以在不需要多次使用计算图时，应避免设置
<code>retain_graph=True</code>，以节省内存资源。</p>
<p>另外，<code>create_graph</code>
参数用于创建导数计算图，允许进行高阶导数的计算；而
<code>grad_tensors</code>
可用于设置多梯度的权重，当有多个损失函数需要计算梯度时，可以设置各个损失之间权重的比例。</p>
<h4 id="如何判断是否需要多次使用计算图">如何判断是否需要多次使用计算图？</h4>
<p>判断是否需要多次使用计算图，主要考虑以下几个方面：</p>
<ol type="1">
<li>模型的结构和需求：如果你的模型在一次前向传播后，需要对不同的输出或中间结果进行多次反向传播来计算梯度，以实现不同的目的，例如优化不同的部分或计算多个相关的梯度，那么就可能需要多次使用计算图。</li>
<li>是否涉及多个相关的损失函数：当存在多个损失函数，且需要分别或同时根据这些损失函数计算梯度并更新参数时，通常需要多次使用计算图。</li>
<li>循环或迭代的操作：在某些情况下，可能会在一个循环或迭代中多次执行类似的计算和反向传播操作。例如，在强化学习中，可能需要在每个时间步都进行前向传播和反向传播。</li>
<li>中间结果的重用：如果在计算过程中需要多次访问或使用某些中间结果的梯度，那么就需要保留计算图以进行多次反向传播。</li>
</ol>
<p>例如，如果你有一个神经网络模型，其中包含多个子模块，每个子模块都有自己的损失，并且你希望分别根据这些子模块的损失来更新它们的参数，那么就需要多次使用计算图，对每个子模块的损失进行单独的反向传播。</p>
<p>又或者在训练过程中，你可能想要尝试不同的优化策略或超参数，需要在同一次前向传播后，多次计算梯度并更新参数，来比较不同策略或参数的效果，这也需要多次使用计算图。</p>
<p>另外，一些复杂的模型结构或自定义的计算流程可能会导致需要多次使用计算图的情况。但需注意，一直保留计算图会占用较多内存，所以在不需要多次使用计算图时，应避免设置
<code>retain_graph=True</code> ，以节省内存资源。</p>
<p>如果你不确定是否需要多次使用计算图，可以先尝试在不保留计算图的情况下（即不设置
<code>retain_graph=True</code> ）运行代码，观察是否会出现报错“trying to
backward through the graph a second time, but the buffers have already
been freed”。如果出现该报错，且确实需要进行多次反向传播，那么就需要设置
<code>retain_graph=True</code>
。同时，为了避免内存过度占用，在完成所有需要的反向传播后，应及时释放不再使用的计算图和相关数据。</p>
<h3 id="tf.reduce_mean">tf.reduce_mean</h3>
<p><code>tf.reduce_mean</code> 是 TensorFlow
中的一个函数，用于计算张量在指定维度上的平均值。</p>
<p>以下是一个示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个张量</span></span><br><span class="line">tensor = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算所有元素的平均值</span></span><br><span class="line">average = tf.reduce_mean(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每一行的平均值</span></span><br><span class="line">row_averages = tf.reduce_mean(tensor, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每一列的平均值</span></span><br><span class="line">column_averages = tf.reduce_mean(tensor, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;所有元素的平均值：&quot;</span>, sess.run(average))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;每一行的平均值：&quot;</span>, sess.run(row_averages))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;每一列的平均值：&quot;</span>, sess.run(column_averages))</span><br></pre></td></tr></table></figure>
<p>在上述示例中：</p>
<ul>
<li>当不指定 <code>axis</code>
参数时，<code>tf.reduce_mean(tensor)</code> 计算张量 <code>tensor</code>
中所有元素的平均值。</li>
<li><code>tf.reduce_mean(tensor, axis=1)</code>
计算每一行的平均值，得到一个长度为行数的张量。</li>
<li><code>tf.reduce_mean(tensor, axis=0)</code>
计算每一列的平均值，得到一个长度为列数的张量。</li>
</ul>
<p>例如，对于上述示例中的张量 <code>[[1, 2, 3], [4, 5, 6]]</code> ：</p>
<ul>
<li>所有元素的平均值为 <code>(1 + 2 + 3 + 4 + 5 + 6) / 6 = 3.5</code>
。</li>
<li>每一行的平均值分别为 <code>(1 + 2 + 3) / 3 = 2</code> 和
<code>(4 + 5 + 6) / 3 = 5</code> 。</li>
<li>每一列的平均值分别为 <code>(1 + 4) / 2 = 2.5</code> 、
<code>(2 + 5) / 2 = 3.5</code> 和 <code>(3 + 6) / 2 = 4.5</code> 。</li>
</ul>
<h3 id="tf.placeholder">tf.placeholder</h3>
<p>用于表示强化学习或神经网络中的状态输入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with tf.name_scope(&#x27;S&#x27;):#状态</span><br><span class="line">    S = tf.placeholder(tf.float32, shape=[None, state_dim], name=&#x27;s&#x27;)</span><br></pre></td></tr></table></figure>
<ol type="1">
<li>tf.name_scope('S')</li>
</ol>
<p>这行代码使用 TensorFlow 的 tf.name_scope 创建一个命名空间
S。命名空间在 TensorFlow
中用于组织图中的节点，使其更具可读性和结构化。在 TensorBoard
中查看图时，可以更清晰地看到节点的组织结构。</p>
<ol start="2" type="1">
<li>tf.placeholder(tf.float32, shape=[None, state_dim], name='s')</li>
</ol>
<p>这行代码定义了一个占位符 S。占位符在 TensorFlow
中用于在执行图时提供输入数据。这里的占位符有以下几个参数：</p>
<p>tf.float32：占位符的数据类型是 32 位浮点数。</p>
<p><strong>shape=[None, state_dim]：占位符的形状。shape
参数定义了输入数据的维度</strong>：</p>
<p>None
表示这个维度可以是任意长度，通常用于<strong>批次（batch）的大小</strong>。使用
None 是因为在实际运行时，批次的大小可能会有所不同。</p>
<p>state_dim
表示状态的维度。这是一个整数，定义了每个状态的特征数量。在强化学习中，状态通常是一个向量，其长度由具体的环境或问题决定。</p>
<p>name='s'：给占位符命名为
's'。这个名字在构建和调试图时有助于识别。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720887133485.png" alt="1720887133485">
<figcaption aria-hidden="true">1720887133485</figcaption>
</figure>
<h3 id="tf.variable_scope">tf.variable_scope</h3>
<p><code>tf.variable_scope</code> 是 TensorFlow
中用于管理变量作用域的一个重要工具。它帮助组织和复用变量，特别是在构建复杂的神经网络时</p>
<h4 id="什么是-tf.variable_scope">1. 什么是
<code>tf.variable_scope</code>？</h4>
<p><code>tf.variable_scope</code>
提供了一种机制来创建和管理变量范围（scope），这对变量进行命名和复用非常有帮助。通过使用变量范围，可以确保变量命名的一致性和避免命名冲突。</p>
<h4 id="为什么使用-tf.variable_scope">2. 为什么使用
<code>tf.variable_scope</code>？</h4>
<ul>
<li><strong>组织变量</strong>：可以将相关的变量组织在一起，使代码更具可读性和结构性。</li>
<li><strong>复用变量</strong>：在共享参数的模型（如共享权重的神经网络层）中，复用变量非常重要。</li>
<li><strong>命名管理</strong>：自动处理变量命名，避免命名冲突。</li>
</ul>
<h4 id="如何使用-tf.variable_scope">3. 如何使用
<code>tf.variable_scope</code>？</h4>
<p>使用 <code>tf.variable_scope</code>
创建一个新的变量作用域，可以在该作用域内定义和复用变量。</p>
<h5 id="示例-1创建变量作用域">示例 1：创建变量作用域</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope1&#x27;</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope2&#x27;</span>):</span><br><span class="line">    var2 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">2.0</span>))</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>var1</code> 和 <code>var2</code>
是在不同的变量作用域中创建的，尽管它们的名字相同，但在图中它们是不同的变量，分别命名为
<code>scope1/var</code> 和 <code>scope2/var</code>。</p>
<h5 id="示例-2复用变量">示例 2：复用变量</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope&#x27;</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope&#x27;</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">    var2 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>var2</code> 复用了 <code>var1</code>
的变量。通过设置
<code>reuse=True</code>，确保在相同的作用域内复用已经存在的变量。</p>
<h5 id="tf.get_variable-和-tf.variable-的区别">4.
<code>tf.get_variable</code> 和 <code>tf.Variable</code> 的区别</h5>
<ul>
<li><strong><code>tf.get_variable</code></strong>：用于创建或获取变量，通常与
<code>tf.variable_scope</code> 一起使用。它支持变量复用机制。</li>
<li><strong><code>tf.Variable</code></strong>：直接创建新变量，不支持复用机制。</li>
</ul>
<h5 id="示例-3使用-tf.get_variable-和-tf.variable">示例 3：使用
<code>tf.get_variable</code> 和 <code>tf.Variable</code></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope1&#x27;</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line">    var2 = tf.Variable([<span class="number">1.0</span>], name=<span class="string">&#x27;var2&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>var1</code> 是通过 <code>tf.get_variable</code>
创建的，支持复用机制，而 <code>var2</code> 是通过
<code>tf.Variable</code> 创建的，不支持复用。</p>
<h4 id="总结">5. 总结</h4>
<p><code>tf.variable_scope</code> 是 TensorFlow
中用于管理变量作用域的工具，帮助组织和复用变量，提供了一种结构化和高效的变量管理方式。通过合理使用变量作用域，可以避免命名冲突，实现变量复用，特别是在构建共享参数的复杂神经网络时非常有用。</p>
<h3 id="tf.session">tf.Session</h3>
<p><code>tf.Session</code> 是 TensorFlow 1.x
中一个重要的类，用于执行定义好的计算图（computation graph）。在
TensorFlow 2.x 中，<code>tf.Session</code> 被逐渐弃用，更多地采用了
Eager Execution 模式，但了解 <code>tf.Session</code>
对于维护和理解旧代码还是非常重要的。以下是对 <code>tf.Session</code>
的详细解释及其使用方法。</p>
<h4 id="什么是-tf.session">1. 什么是 <code>tf.Session</code>？</h4>
<p><code>tf.Session</code> 提供了一个运行 TensorFlow 操作的环境。它管理
TensorFlow 运行时的所有资源，包括变量的内存分配、执行设备的选择等。</p>
<h4 id="为什么需要-tf.session">2. 为什么需要
<code>tf.Session</code>？</h4>
<p>在 TensorFlow 1.x
中，计算图的定义和执行是分开的。你首先定义一个计算图，然后在
<code>tf.Session</code> 中执行它。<code>tf.Session</code>
提供了以下功能：</p>
<ul>
<li><strong>控制和管理资源</strong>：例如内存、线程等。</li>
<li><strong>分配计算设备</strong>：决定在 CPU 还是 GPU 上执行操作。</li>
<li><strong>执行计算图</strong>：运行图中的操作，获取结果。</li>
</ul>
<h4 id="如何使用-tf.session">3. 如何使用 <code>tf.Session</code>？</h4>
<h5 id="示例-1基本使用">示例 1：基本使用</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义计算图</span></span><br><span class="line">a = tf.constant(<span class="number">2</span>)</span><br><span class="line">b = tf.constant(<span class="number">3</span>)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话并运行计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result = sess.run(c)</span><br><span class="line">    <span class="built_in">print</span>(result)  <span class="comment"># 输出 5</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，首先定义了一个简单的计算图，然后创建一个
<code>tf.Session</code> 来运行这个图，并获取结果。</p>
<h5 id="示例-2使用-tf.placeholder-和-feed_dict">示例 2：使用
<code>tf.placeholder</code> 和 <code>feed_dict</code></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义占位符和操作</span></span><br><span class="line">a = tf.placeholder(tf.int32)</span><br><span class="line">b = tf.placeholder(tf.int32)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话并运行计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result = sess.run(c, feed_dict=&#123;a: <span class="number">2</span>, b: <span class="number">3</span>&#125;)</span><br><span class="line">    <span class="built_in">print</span>(result)  <span class="comment"># 输出 5</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，使用 <code>tf.placeholder</code>
定义了占位符，可以在运行时通过 <code>feed_dict</code> 提供输入数据。</p>
<h5 id="示例-3管理变量">示例 3：管理变量</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义变量和初始化操作</span></span><br><span class="line">var = tf.Variable([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话并初始化变量</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    result = sess.run(var)</span><br><span class="line">    <span class="built_in">print</span>(result)  <span class="comment"># 输出 [1. 2.]</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，定义了一个变量，并在会话中运行初始化操作来分配和初始化变量。</p>
<h4 id="tensorflow-2.x-中的变化">4. TensorFlow 2.x 中的变化</h4>
<p>在 TensorFlow 2.x 中，引入了 Eager
Execution，默认情况下无需显式创建会话，可以直接执行操作。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorFlow 2.x 默认启用 Eager Execution</span></span><br><span class="line">a = tf.constant(<span class="number">2</span>)</span><br><span class="line">b = tf.constant(<span class="number">3</span>)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c)  <span class="comment"># 直接输出 &lt;tf.Tensor: shape=(), dtype=int32, numpy=5&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="总结-1">5. 总结</h4>
<p><code>tf.Session</code> 是 TensorFlow 1.x
中用于执行计算图的核心工具，帮助管理和控制资源，执行计算操作。在
TensorFlow 2.x 中，虽然 <code>tf.Session</code>
被弃用，但理解它的工作机制对于维护旧代码和理解 TensorFlow
的运行原理仍然非常重要。</p>
<h3 id="tf.random_normal_initializer">tf.random_normal_initializer</h3>
<p>这是 TensorFlow
提供的一个初始化器，用于从正态分布（高斯分布）中随机抽取样本来初始化变量。</p>
<h4 id="背景知识">背景知识</h4>
<p>在神经网络中，权重和偏置的初始化对模型的收敛速度和效果有很大影响。TensorFlow
提供了多种初始化器来帮助我们初始化这些变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init_w = tf.random_normal_initializer(<span class="number">0.</span>, <span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>
<p><strong>参数解释</strong> ：</p>
<ul>
<li>第一个参数
<code>0.</code>：表示正态分布的均值（mean）。在这里，均值被设置为
<code>0</code>。</li>
<li>第二个参数 <code>0.3</code>：表示正态分布的标准差（standard
deviation）。在这里，标准差被设置为 <code>0.3</code>。
因此，<code>tf.random_normal_initializer(0., 0.3)</code> 表示使用均值为
<code>0</code>、标准差为 <code>0.3</code> 的正态分布来初始化变量。</li>
</ul>
<h4 id="实例说明">实例说明</h4>
<p>以下是一个简单的示例，展示如何使用这个初始化器来初始化一个 TensorFlow
变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个变量并使用 tf.random_normal_initializer 初始化</span></span><br><span class="line">init_w = tf.random_normal_initializer(<span class="number">0.</span>, <span class="number">0.3</span>)</span><br><span class="line">var = tf.Variable(initial_value=init_w(shape=[<span class="number">2</span>, <span class="number">3</span>], dtype=tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化全局变量</span></span><br><span class="line">tf.compat.v1.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建并运行一个会话以查看变量的值</span></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.compat.v1.global_variables_initializer())</span><br><span class="line">    <span class="built_in">print</span>(sess.run(var))</span><br></pre></td></tr></table></figure>
<h4 id="解释这个示例">解释这个示例</h4>
<ol type="1">
<li><strong>定义初始化器</strong>
：<code>init_w = tf.random_normal_initializer(0., 0.3)</code>
定义了一个初始化器，它会生成均值为 <code>0</code>、标准差为
<code>0.3</code> 的随机数。</li>
<li><strong>初始化变量</strong>
：<code>var = tf.Variable(initial_value=init_w(shape=[2, 3], dtype=tf.float32))</code>
使用这个初始化器来初始化一个形状为 <code>[2, 3]</code> 的变量。</li>
<li><strong>初始化全局变量</strong>
：<code>tf.compat.v1.global_variables_initializer()</code>
用于初始化所有的 TensorFlow 变量。</li>
<li><strong>运行会话</strong> ：创建并运行一个 TensorFlow
会话以查看变量的值。</li>
</ol>
<h3 id="tf.compat.v1.global_variables_initializer">tf.compat.v1.global_variables_initializer</h3>
<p><code>tf.compat.v1.global_variables_initializer</code> 是 TensorFlow
1.x 中的一个函数，用于初始化所有的全局变量。在 TensorFlow 2.x
中，这个函数被包括在 <code>compat.v1</code> 模块中，以便兼容旧代码。</p>
<h4 id="作用">作用</h4>
<p><code>tf.compat.v1.global_variables_initializer()</code>
函数会创建一个操作（operation），这个操作会初始化 TensorFlow
图中的所有全局变量。执行这个操作能够将所有变量赋值为它们的初始值。</p>
<h4 id="变量初始化的重要性">变量初始化的重要性</h4>
<p>在使用 TensorFlow
构建和训练模型时，变量（如权重和偏置）需要先被初始化，然后才能在计算图中使用。未初始化的变量在使用时会导致错误。</p>
<h3 id="使用示例">使用示例</h3>
<p>下面是一个简单的示例，展示了如何使用
<code>tf.compat.v1.global_variables_initializer()</code>
来初始化变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建变量</span></span><br><span class="line">var1 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">&quot;var1&quot;</span>)</span><br><span class="line">var2 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">&quot;var2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建初始化操作</span></span><br><span class="line">init_op = tf.compat.v1.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建并运行会话以初始化变量</span></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init_op)  <span class="comment"># 初始化所有变量</span></span><br><span class="line">    <span class="built_in">print</span>(sess.run(var1))</span><br><span class="line">    <span class="built_in">print</span>(sess.run(var2))</span><br></pre></td></tr></table></figure>
<h3 id="解释这个示例-1">解释这个示例</h3>
<ol type="1">
<li><p><strong>创建变量</strong>：我们创建了两个变量 <code>var1</code>
和 <code>var2</code>，它们的初始值是从正态分布中随机抽取的。</p></li>
<li><p><strong>创建初始化操作</strong>：使用
<code>tf.compat.v1.global_variables_initializer()</code>
创建一个初始化操作 <code>init_op</code>。</p></li>
<li><p><strong>运行会话</strong>：</p>
<ul>
<li>使用 <code>with tf.compat.v1.Session() as sess:</code> 创建一个
TensorFlow 会话。</li>
<li>使用 <code>sess.run(init_op)</code>
来运行初始化操作，初始化所有变量。</li>
<li>之后，通过 <code>sess.run(var1)</code> 和
<code>sess.run(var2)</code> 来查看变量的值。</li>
</ul></li>
</ol>
<h3 id="tensorflow-2.x-的变化">TensorFlow 2.x 的变化</h3>
<p>在 TensorFlow 2.x 中，变量的初始化通常由更高级别的 API（如
Keras）自动处理。直接使用
<code>tf.compat.v1.global_variables_initializer()</code>
的情况较少见，除非是在兼容旧代码或使用低级别 API 时。</p>
<p>以下是 TensorFlow 2.x 的一个简单示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建变量</span></span><br><span class="line">var1 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">&quot;var1&quot;</span>)</span><br><span class="line">var2 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">&quot;var2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化所有变量</span></span><br><span class="line">tf.compat.v1.global_variables_initializer().run(session=tf.compat.v1.Session())</span><br></pre></td></tr></table></figure>
<h3 id="总结-2">总结</h3>
<p><code>tf.compat.v1.global_variables_initializer()</code>
是用于初始化所有全局变量的函数。在 TensorFlow 2.x 中，通常通过更高级别的
API 处理变量的初始化，但在需要兼容 TensorFlow 1.x
代码时，<code>tf.compat.v1.global_variables_initializer()</code>
仍然非常有用。通过初始化变量，确保在计算图中使用这些变量时不会遇到未初始化变量的错误。</p>
<h3 id="tf.constant_initializer">tf.constant_initializer</h3>
<p><code>tf.constant_initializer</code> 是 TensorFlow
中的一个初始化器，用于创建一个常量初始化器对象，该对象用于将变量初始化为指定的常量值。</p>
<h4 id="使用方法">使用方法</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">initializer = tf.constant_initializer(value)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>value</code> 是要初始化的常量值。</li>
</ul>
<h4 id="示例">示例</h4>
<p>以下是一个简单的示例，展示如何使用
<code>tf.constant_initializer</code> 来初始化一个 TensorFlow
变量为常量值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个常量初始化器，将变量初始化为常量值 42.0</span></span><br><span class="line">initializer = tf.constant_initializer(<span class="number">42.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个变量，并使用常量初始化器初始化</span></span><br><span class="line">var = tf.Variable(initializer(shape=[<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">&quot;constant_var&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化所有变量</span></span><br><span class="line">tf.compat.v1.global_variables_initializer().run(session=tf.compat.v1.Session())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印变量的值</span></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(sess.run(var))</span><br></pre></td></tr></table></figure>
<h4 id="解释这个示例-2">解释这个示例</h4>
<ol type="1">
<li><p><strong>创建初始化器</strong>：使用
<code>tf.constant_initializer(42.0)</code>
创建一个常量初始化器，它会将变量初始化为常量值
<code>42.0</code>。</p></li>
<li><p><strong>初始化变量</strong>：使用
<code>initializer(shape=[2, 3])</code> 创建一个形状为
<code>[2, 3]</code> 的变量，并使用常量初始化器初始化它。</p></li>
<li><p><strong>运行会话</strong>：</p>
<ul>
<li>使用 <code>tf.compat.v1.global_variables_initializer()</code>
初始化所有变量。</li>
<li>使用 <code>sess.run(var)</code>
在会话中运行并打印变量的值，输出将会是一个形状为 <code>[2, 3]</code>
的数组，其所有元素都是 <code>42.0</code>。</li>
</ul></li>
</ol>
<h4 id="应用场景">应用场景</h4>
<ul>
<li><strong>固定初始化值</strong>：当你希望将所有权重或偏置初始化为固定的常量值时，可以使用
<code>tf.constant_initializer</code>。</li>
<li><strong>保证初始化的一致性</strong>：有时在调试和开发过程中，需要确保变量被初始化为固定的常量，以便进行稳定性检查和测试。</li>
</ul>
<h4 id="tensorflow-2.x-的变化-1">TensorFlow 2.x 的变化</h4>
<p>在 TensorFlow 2.x 中，变量的初始化通常由更高级别的 API（如
Keras）自动处理。直接使用 <code>tf.constant_initializer</code>
的情况较少见，除非是在兼容旧代码或使用低级别 API 时。</p>
<h4 id="总结-3">总结</h4>
<p><code>tf.constant_initializer</code> 是 TensorFlow
中的一个初始化器，用于将变量初始化为指定的常量值。通过设置常量值，可以确保在构建和训练神经网络时，某些变量始终具有固定的初始值。</p>
<h3 id="tf.layers.dense">tf.layers.dense</h3>
<p><code>tf.layers.dense</code> 是 TensorFlow
提供的一个高级API，用于创建全连接层。它自动创建并管理权重变量（kernel）和偏置变量（bias），并且可以通过参数来控制激活函数、初始化器、层名称等。这样可以简化神经网络的构建过程，并提供了一致性的接口。</p>
<p><strong>示例解释</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 s 是输入张量，init_w 和 init_b 是初始化器</span></span><br><span class="line">s = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, input_dim])  <span class="comment"># input_dim 是输入的特征数</span></span><br><span class="line"></span><br><span class="line">init_w = tf.random_normal_initializer(<span class="number">0.</span>, <span class="number">0.1</span>)</span><br><span class="line">init_b = tf.constant_initializer(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个全连接层</span></span><br><span class="line">net = tf.layers.dense(s, <span class="number">30</span>, activation=tf.nn.relu,</span><br><span class="line">                      kernel_initializer=init_w, bias_initializer=init_b, name=<span class="string">&#x27;l1&#x27;</span>,</span><br><span class="line">                      trainable=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>这段代码的作用是在输入张量 <code>s</code> 上创建一个具有 30
个神经元、ReLU
激活函数的全连接层，使用指定的权重和偏置初始化器，并指定该层为可训练的。这样，TensorFlow
将自动处理该层的变量初始化和管理，使得神经网络的搭建更加高效和便捷。</p>
<h4 id="上面30-个神经元的含义">上面30 个神经元的含义</h4>
<p>神经网络中的“神经元”是指一个计算单元，它接收输入、应用权重和偏置，并通过激活函数产生输出。在全连接层（dense
layer）中，神经元的数量决定了该层的输出维度。</p>
<p>在代码
<code>net = tf.layers.dense(s, 30, activation=tf.nn.relu, kernel_initializer=init_w, bias_initializer=init_b, name='l1', trainable=trainable)</code>
中，指定了这一层有 30 个神经元。具体来说：</p>
<ol type="1">
<li><strong>输入张量</strong> ：假设输入张量 <code>s</code> 的形状为
<code>[batch_size, input_dim]</code>，其中 <code>batch_size</code>
是批次大小（每次训练或推理使用的样本数量），<code>input_dim</code>
是输入特征的数量。</li>
<li><strong>神经元的作用</strong> ：每个神经元都接收输入张量
<code>s</code>
的所有特征，将这些特征与它自身的权重（weights）相乘，然后加上一个偏置（bias），再通过激活函数（如
ReLU）计算输出。</li>
<li><strong>输出维度</strong> ：具有 30 个神经元意味着输出张量的形状将是
<code>[batch_size, 30]</code>。也就是说，每个输入样本将被转换为一个 30
维的输出向量。</li>
</ol>
<h5 id="详细步骤">详细步骤</h5>
<ol type="1">
<li><strong>权重矩阵</strong> ：如果输入张量 <code>s</code> 的形状为
<code>[batch_size, input_dim]</code>，则权重矩阵 <code>W</code> 的形状为
<code>[input_dim, 30]</code>。</li>
<li><strong>计算加权和</strong> ：每个神经元计算加权和
<code>z = W*x + b</code>，其中 <code>x</code> 是输入特征，<code>W</code>
是权重，<code>b</code> 是偏置。</li>
<li><strong>应用激活函数</strong> ：计算出加权和 <code>z</code>
后，应用激活函数
<code>a = activation(z)</code>。在这个例子中，激活函数是 ReLU，即
<code>a = max(0, z)</code>。</li>
<li><strong>输出张量</strong> ：最终，每个输入样本被转换为一个 30
维的输出向量，所有样本组成的输出张量形状为
<code>[batch_size, 30]</code>。</li>
</ol>
<h5 id="举例说明">举例说明</h5>
<p>假设我们有一个输入张量 <code>s</code>，形状为
<code>[batch_size=2, input_dim=5]</code>：</p>
<p>我们定义一个具有 30 个神经元的全连接层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = tf.layers.dense(s, <span class="number">30</span>, activation=tf.nn.relu)</span><br></pre></td></tr></table></figure>
<p>输入张量 <code>s</code> 的形状是 <code>[2, 5]</code>，经过具有 30
个神经元的全连接层后，输出张量 <code>net</code> 的形状将是
<code>[2, 30]</code>。每个输入样本（2 个样本）被转换为一个 30
维的输出向量。</p>
<p>总之，具有 30 个神经元意味着这一层的输出维度是
30，每个输入样本将被转化为一个 30
维的向量。这有助于神经网络在不同层之间传递和处理信息，提取和学习更复杂的特征。</p>
<h3 id="tf.multiply">tf.multiply ：</h3>
<ul>
<li><code>tf.multiply</code> 是 TensorFlow
的一个操作，用于逐元素相乘。它的输入是两个张量，输出是这两个张量对应位置元素相乘的结果。</li>
</ul>
<h3 id="tf.get_collection">tf.get_collection</h3>
<p><code>tf.get_collection</code> 是 TensorFlow
中用于获取一个集合中的所有元素的函数。集合在 TensorFlow
中是一种组织和管理变量、操作等对象的方式，可以通过集合来方便地访问和操作这些对象。</p>
<h4 id="函数原型">函数原型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.get_collection(key, scope=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<h4 id="参数">参数</h4>
<ol type="1">
<li><p><strong><code>key</code></strong>：</p>
<ul>
<li>表示要获取的集合的键（名称），是一个字符串。</li>
<li>TensorFlow 中有一些预定义的集合键，例如
<code>tf.GraphKeys.GLOBAL_VARIABLES</code>、<code>tf.GraphKeys.TRAINABLE_VARIABLES</code>
等。</li>
</ul></li>
<li><p><strong><code>scope</code></strong>（可选）：</p>
<ul>
<li>表示要获取的集合中元素的作用域（scope）。如果指定了
<code>scope</code>，则只返回该作用域内的元素。</li>
</ul></li>
</ol>
<h4 id="返回值">返回值</h4>
<p>返回与 <code>key</code>
对应的集合中的所有元素，通常是一个列表。如果集合不存在，返回空列表。</p>
<h4 id="举例说明-1">举例说明</h4>
<p>假设我们在构建神经网络时，将一些变量添加到集合中，然后在训练或评估时需要获取这些变量，可以使用
<code>tf.get_collection</code>。</p>
<h5 id="示例代码">示例代码</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建变量并将其添加到集合中</span></span><br><span class="line">var1 = tf.Variable(<span class="number">1.0</span>, name=<span class="string">&#x27;var1&#x27;</span>)</span><br><span class="line">var2 = tf.Variable(<span class="number">2.0</span>, name=<span class="string">&#x27;var2&#x27;</span>)</span><br><span class="line">tf.add_to_collection(<span class="string">&#x27;my_collection&#x27;</span>, var1)</span><br><span class="line">tf.add_to_collection(<span class="string">&#x27;my_collection&#x27;</span>, var2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取集合中的所有变量</span></span><br><span class="line">collection_vars = tf.get_collection(<span class="string">&#x27;my_collection&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(collection_vars)</span><br></pre></td></tr></table></figure>
<h4 id="预定义集合键">预定义集合键</h4>
<p>TensorFlow 提供了一些常用的集合键，方便用户管理变量和操作：</p>
<ul>
<li><code>tf.GraphKeys.GLOBAL_VARIABLES</code>：所有全局变量。</li>
<li><code>tf.GraphKeys.TRAINABLE_VARIABLES</code>：所有可训练的变量。</li>
<li><code>tf.GraphKeys.SUMMARIES</code>：所有摘要操作。</li>
<li><code>tf.GraphKeys.UPDATE_OPS</code>：所有需要在训练时执行的更新操作。</li>
</ul>
<h4 id="在实际应用中的使用">在实际应用中的使用</h4>
<h5 id="获取所有可训练变量">获取所有可训练变量</h5>
<p>在训练神经网络时，我们通常需要获取所有可训练的变量，例如在优化器中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.01</span>)</span><br><span class="line">train_op = optimizer.minimize(loss, var_list=trainable_vars)</span><br></pre></td></tr></table></figure>
<h5 id="获取特定作用域内的变量">获取特定作用域内的变量</h5>
<p>我们还可以使用 <code>scope</code> 参数来获取特定作用域内的变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;layer1&#x27;</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">&#x27;var1&#x27;</span>, shape=[<span class="number">10</span>])</span><br><span class="line">  </span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;layer2&#x27;</span>):</span><br><span class="line">    var2 = tf.get_variable(<span class="string">&#x27;var2&#x27;</span>, shape=[<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">layer1_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="string">&#x27;layer1&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(layer1_vars)  <span class="comment"># 只会打印 layer1 作用域内的变量</span></span><br></pre></td></tr></table></figure>
<h4 id="总结-4">总结</h4>
<p><code>tf.get_collection</code>
是一个强大的函数，允许用户根据集合键和作用域来获取 TensorFlow
图中的变量和操作。它在组织和管理复杂模型中的变量和操作时非常有用，尤其是在训练、保存和恢复模型时。通过合理使用集合和
<code>tf.get_collection</code>，可以使代码更加清晰和易于维护。</p>
<h3 id="tf.stop_gradient">tf.stop_gradient</h3>
<p><code>tf.stop_gradient</code> 是 TensorFlow
中用于阻止梯度反向传播的操作。它返回一个与输入张量具有相同内容的新张量，但在计算梯度时会被视为常量。换句话说，<code>tf.stop_gradient</code>
可以用来“截断”梯度的传播，这在某些情况下（例如实现某些特殊的梯度更新规则或避免某些变量的梯度更新）非常有用。</p>
<h4 id="函数原型-1">函数原型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.stop_gradient(</span><br><span class="line">    <span class="built_in">input</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="参数-1">参数</h4>
<ul>
<li><strong><code>input</code></strong>：输入张量。</li>
<li><strong><code>name</code></strong>（可选）：操作名。</li>
</ul>
<h4 id="返回值-1">返回值</h4>
<p>返回一个与输入张量具有相同内容的新张量，但在反向传播过程中，该张量的梯度将被视为零。</p>
<h4 id="使用示例-1">使用示例</h4>
<h5 id="示例1简单示例">示例1：简单示例</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个变量和一个操作</span></span><br><span class="line">x = tf.Variable(<span class="number">2.0</span>, name=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 tf.stop_gradient 阻止梯度传播</span></span><br><span class="line">y_no_grad = tf.stop_gradient(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数</span></span><br><span class="line">loss = y_no_grad + x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算梯度</span></span><br><span class="line">gradients = tf.gradients(loss, [x])</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>y_no_grad</code> 是通过
<code>tf.stop_gradient</code> 创建的，因此在计算 <code>loss</code>
的梯度时，<code>y_no_grad</code> 不会对 <code>x</code>
的梯度产生影响。最终结果是 <code>gradients</code> 只包含 <code>x</code>
的梯度，即 1，而不是 <code>y = x * 2</code> 导致的 2。</p>
<h5 id="示例2在神经网络中的应用">示例2：在神经网络中的应用</h5>
<p>在一些强化学习算法中，我们可能希望通过停止梯度来实现某些特殊的策略。例如，在
DDPG 算法中，actor 网络的输出动作传递给 critic 网络，但我们不希望通过
critic 网络更新 actor 网络的参数，可以使用 <code>tf.stop_gradient</code>
实现这一点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一个 actor 网络输出动作 a</span></span><br><span class="line">a = actor_network(state)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们希望通过 critic 网络计算梯度，但不希望更新 actor 网络</span></span><br><span class="line">q_value = critic_network(tf.stop_gradient(a))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失并更新 critic 网络</span></span><br><span class="line">critic_loss = -tf.reduce_mean(q_value)</span><br><span class="line">critic_optimizer = tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">critic_train_op = critic_optimizer.minimize(critic_loss)</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>tf.stop_gradient(a)</code> 确保了
<code>critic_network</code> 的梯度不会影响 <code>actor_network</code>
的参数更新。</p>
<h4 id="总结-5">总结</h4>
<p><code>tf.stop_gradient</code>
是一个非常有用的操作，特别是在需要控制梯度传播路径或实现自定义梯度更新规则时。通过合理使用
<code>tf.stop_gradient</code>，可以更灵活地设计和训练复杂的模型。</p>
<h3 id="软替换代码">软替换代码</h3>
<p>代码理解:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.soft_replace = [tf.assign(t, (<span class="number">1</span> - self.replacement[<span class="string">&#x27;tau&#x27;</span>]) * t + self.replacement[<span class="string">&#x27;tau&#x27;</span>] * e)</span><br><span class="line">                     <span class="keyword">for</span> t, e <span class="keyword">in</span> <span class="built_in">zip</span>(self.t_params, self.e_params)]</span><br></pre></td></tr></table></figure>
<p>这一行代码的作用是实现软替换（soft
replacement），用来逐步更新目标网络的参数，使其接近评估网络的参数。在强化学习中，软替换是一种常见的参数更新策略，常用于深度确定性策略梯度（DDPG）等算法。</p>
<h4 id="让我们逐步解析这段代码">让我们逐步解析这段代码：</h4>
<ol type="1">
<li><p><strong><code>zip(self.t_params, self.e_params)</code></strong>：</p>
<ul>
<li><code>self.t_params</code> 包含目标网络的所有参数。</li>
<li><code>self.e_params</code> 包含评估网络的所有参数。</li>
<li><code>zip</code>
函数将这两个参数列表配对，使得每对包含一个目标网络的参数和一个评估网络的参数。</li>
</ul></li>
<li><p><strong>列表推导式</strong>：</p>
<ul>
<li>列表推导式用于生成一个包含多个 TensorFlow
操作的列表。在这里，每个操作都是一个 <code>tf.assign</code>
操作，用于更新目标网络的参数。</li>
</ul></li>
<li><p><strong><code>tf.assign</code></strong>：</p>
<ul>
<li><code>tf.assign</code> 是 TensorFlow
中用于更新变量值的操作。它接受两个参数：要更新的变量和新的值。</li>
<li><code>t</code> 是目标网络的参数，<code>e</code>
是评估网络的参数。</li>
</ul></li>
<li><p><strong>软更新公式</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span> - self.replacement[<span class="string">&#x27;tau&#x27;</span>]) * t + self.replacement[<span class="string">&#x27;tau&#x27;</span>] * e</span><br></pre></td></tr></table></figure>
<ul>
<li><code>self.replacement['tau']</code>
是软更新的速率，通常是一个小值（如
0.01）。它控制了目标网络参数向评估网络参数靠拢的速度。</li>
<li><code>(1 - self.replacement['tau']) * t</code>
保留了目标网络参数的大部分值。</li>
<li><code>self.replacement['tau'] * e</code>
引入了一小部分评估网络参数的值。</li>
<li>通过这种加权平均的方式，目标网络参数逐步向评估网络参数靠拢，但不会立即完全相同。</li>
</ul></li>
<li><p><strong>生成软替换操作列表</strong>：</p>
<ul>
<li>对于每一对目标网络参数 <code>t</code> 和评估网络参数
<code>e</code>，生成一个 <code>tf.assign</code>
操作，将目标网络参数更新为加权平均后的值。</li>
<li><code>self.soft_replace</code> 最终是一个包含所有这些
<code>tf.assign</code> 操作的列表。</li>
</ul></li>
</ol>
<h4 id="软替换的具体作用">软替换的具体作用</h4>
<p>在强化学习算法中，目标网络和评估网络的更新策略对算法的稳定性至关重要。软替换通过逐步更新目标网络的参数，可以减少训练过程中的振荡和不稳定性。相对于硬替换（每隔一段时间直接将评估网络的参数复制到目标网络），软替换更为平滑和稳定，有助于提高算法的训练效果。</p>
<h4 id="示例-1">示例</h4>
<p>假设评估网络和目标网络的参数分别为 <code>e_params = [1, 2, 3]</code>
和 <code>t_params = [4, 5, 6]</code>，且
<code>tau = 0.1</code>，则软替换后的目标网络参数计算如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">new_t_params = [(<span class="number">1</span> - <span class="number">0.1</span>) * <span class="number">4</span> + <span class="number">0.1</span> * <span class="number">1</span>, (<span class="number">1</span> - <span class="number">0.1</span>) * <span class="number">5</span> + <span class="number">0.1</span> * <span class="number">2</span>, (<span class="number">1</span> - <span class="number">0.1</span>) * <span class="number">6</span> + <span class="number">0.1</span> * <span class="number">3</span>]</span><br><span class="line">              = [<span class="number">3.7</span>, <span class="number">4.7</span>, <span class="number">5.7</span>]</span><br></pre></td></tr></table></figure>
<p>每次更新后，目标网络的参数都会逐渐向评估网络的参数靠拢，但不会立即相同，从而实现平滑的参数更新。</p>
<h3 id="硬替换代码">硬替换代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.t_replace_counter = <span class="number">0</span></span><br><span class="line">            self.hard_replace = [tf.assign(t, e) <span class="keyword">for</span> t, e <span class="keyword">in</span> <span class="built_in">zip</span>(self.t_params, self.e_params)]</span><br></pre></td></tr></table></figure>
<h3 id="tf.gradients">tf.gradients</h3>
<p><code>tf.gradients</code> 是 TensorFlow
中用于计算梯度的函数。它的主要功能是计算某个或某些张量（<code>ys</code>）相对于另一些张量（<code>xs</code>）的导数。</p>
<p>其函数签名如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.gradients(ys, xs, grad_ys=<span class="literal">None</span>, name=<span class="string">&#x27;gradients&#x27;</span>, colocate_gradients_with_ops=<span class="literal">False</span>, gate_gradients=<span class="literal">False</span>, aggregation_method=<span class="literal">None</span>, stop_gradients=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数说明如下：</p>
<ul>
<li><code>ys</code>：表示要计算梯度的张量或张量列表。</li>
<li><code>xs</code>：表示要对哪些张量计算梯度的张量或张量列表。</li>
<li><code>grad_ys</code>（可选）：也是一个张量或张量列表，用于对
<code>ys</code> 中的每个元素进行加权。如果未提供，则默认为每个元素都是
1。</li>
<li><code>name</code>（可选）：操作的名称。</li>
<li><code>colocate_gradients_with_ops</code>（可选）：通常不需要设置。</li>
<li><code>gate_gradients</code>（可选）：通常不需要设置。</li>
<li><code>aggregation_method</code>（可选）：聚合方法，一般使用默认值即可。</li>
<li><code>stop_gradients</code>（可选）：是一个张量或张量列表，用于指定在反向传播中梯度停止的节点。这意味着这些节点的梯度不会再向后传播，就好像它们被明确地断开了一样。</li>
</ul>
<p>例如，如果 <code>ys</code> 是一个张量，<code>xs</code>
是一个张量列表，那么返回的是一个与 <code>xs</code>
长度相同的列表，其中每个元素是 <code>ys</code> 相对于 <code>xs</code>
中对应张量的导数。</p>
<p>如果 <code>ys</code> 和 <code>xs</code>
都是张量列表，那么它会计算每个 <code>ys</code> 中的张量相对于每个
<code>xs</code> 中的张量的导数，并将结果以适当的方式组合成一个列表。</p>
<p>下面是一个简单的示例，假设有两个变量 <code>w1</code> 和
<code>w2</code>，通过矩阵乘法等操作得到结果 <code>res</code>，然后使用
<code>tf.gradients</code> 计算 <code>res</code> 相对于 <code>w1</code>
和 <code>w2</code> 的梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(((<span class="number">1</span>, <span class="number">2</span>)))  </span><br><span class="line">w2 = tf.Variable(((<span class="number">3</span>, <span class="number">4</span>)))  </span><br><span class="line">res = tf.matmul(w1, ((<span class="number">2</span>), (<span class="number">1</span>))) + tf.matmul(w2, ((<span class="number">3</span>), (<span class="number">5</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 res 相对于 w1 和 w2 的梯度</span></span><br><span class="line">grads = tf.gradients((res,), (w1, w2))  </span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()  </span><br><span class="line">    re = sess.run(grads)  </span><br><span class="line">    <span class="built_in">print</span>(re) </span><br></pre></td></tr></table></figure>
<p>在上述示例中，<code>tf.gradients</code>
返回了一个包含两个梯度张量的列表，分别对应 <code>res</code> 相对于
<code>w1</code> 和 <code>w2</code> 的梯度。</p>
<p><code>stop_gradients</code> 参数的使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant((<span class="number">0.0</span>))  </span><br><span class="line">b = a * <span class="number">2</span>  </span><br><span class="line"><span class="comment"># 计算 a+b 相对于 a 和 b 的梯度，stop_gradients 指定为 (a, b)，表示 a 和 b 的梯度不会再向后传播</span></span><br><span class="line">pg = tf.gradients(a+b, (a, b), stop_gradients=(a, b))  </span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    res = sess.run(pg)  </span><br><span class="line">    <span class="built_in">print</span>(res) </span><br></pre></td></tr></table></figure>
<p>在这个例子中，由于使用了 <code>stop_gradients=(a, b)</code>，所以
<code>a</code> 和 <code>b</code>
的梯度在计算后不会再向后传播，得到的梯度结果为
<code>(1.0, 1.0)</code>，而不是通常情况下的
<code>(3.0, 1.0)</code>。</p>
<p>如何理解stop_gradients</p>
<p>具体来说，对于表达式
<code>pg = tf.gradients(a+b, (a, b), stop_gradients=(a, b))</code>，意味着在计算
<code>a+b</code> 相对于 <code>a</code> 和 <code>b</code>
的梯度时，<code>a</code> 和 <code>b</code>
本身的梯度不会再进一步向后传播。也就是说，<code>a</code>
的梯度不会影响到依赖于 <code>a</code> 的其他操作，<code>b</code>
的梯度也不会影响到依赖于 <code>b</code> 的其他操作。</p>
<p>这在一些情况下是有用的，例如当你不希望某些变量的梯度继续传播，或者想要模拟某些部分的梯度为固定值的情况。通过设置
<code>stop_gradients</code>，可以更灵活地控制梯度的计算和传播。</p>
<p>例如，如果 <code>a</code> 和 <code>b</code>
是神经网络中的某些中间节点，并且你不希望它们的梯度对网络的其他部分产生影响，就可以将它们放入
<code>stop_gradients</code> 中。这样，在反向传播时，就只会计算到
<code>a+b</code> 这一步，而不会将梯度继续传播到 <code>a</code> 和
<code>b</code> 所依赖的其他变量上。</p>
<p>举个简单的例子，假设有如下计算图：<code>c = a + b</code>，<code>d = c * x</code>，如果计算
<code>d</code> 相对于 <code>a</code> 的梯度，正常情况下梯度会从
<code>d</code> 传播到 <code>c</code>，再传播到
<code>a</code>。但如果使用
<code>stop_gradients=(a, b)</code>，那么梯度只会传播到
<code>c</code>，而不会再传播到 <code>a</code> 和 <code>b</code>，即
<code>a</code> 和 <code>b</code>
的梯度不会再向后传播。这样可以在一定程度上实现对梯度传播的控制和干预。</p>
<p>需要注意的是，<code>tf.stop_gradient</code>
是在构建图的过程中使用，用来指定停止链式法则的节点；而
<code>stop_gradients</code> 是在构建图之后使用，当程序运行碰到在
<code>stop_gradients</code>
中定义的节点时，均会停止链式法则，进而求得部分偏导。它们的作用都是阻止梯度的进一步传播，但使用的时机略有不同。</p>
<p><code>grad_ys</code> 参数用于对梯度进行加权，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, shape=(<span class="number">3</span>))  </span><br><span class="line">w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, shape=(<span class="number">3</span>))  </span><br><span class="line">w3 = tf.get_variable(<span class="string">&#x27;w3&#x27;</span>, shape=(<span class="number">3</span>))  </span><br><span class="line">w4 = tf.get_variable(<span class="string">&#x27;w4&#x27;</span>, shape=(<span class="number">3</span>))  </span><br><span class="line">z1 = <span class="number">3</span>*w1 + <span class="number">2</span>*w2 + w3  </span><br><span class="line">z2 = -<span class="number">1</span>*w3 + w4</span><br><span class="line"></span><br><span class="line">grad_ys = (tf.convert_to_tensor((<span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>)), tf.convert_to_tensor((<span class="number">3.0</span>, <span class="number">2.0</span>, <span class="number">4.0</span>)))</span><br><span class="line">grads = tf.gradients((z1, z2), (w1, w2, w3, w4), grad_ys=grad_ys)  </span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()  </span><br><span class="line">    <span class="built_in">print</span>(sess.run(grads)) </span><br></pre></td></tr></table></figure>
<p>这里的 <code>grad_ys</code> 对 <code>z1</code> 和 <code>z2</code>
的梯度进行了加权，最终得到的梯度结果会相应地受到加权的影响。</p>
<p>在 TensorFlow 2.x 中也可以使用
<code>tf.gradients</code>，但通常更推荐使用 <code>tf.GradientTape</code>
来进行自动求导，它在使用上更加灵活和方便。不过，在某些情况下，仍然可能需要使用
<code>tf.gradients</code>，例如在一些特定的模型架构或代码结构中。但要注意将其放在计算图中使用。如果不在计算图中使用，就需要使用
<code>tf.GradientTape</code> 来替代。</p>
<h4 id="tf.gradients的输入和输出之间的形状的关系"><code>tf.gradients</code>的输入和输出之间的形状的关系</h4>
<p>在 TensorFlow 中，<code>tf.gradients()</code>
用于计算梯度。它接受求导的目标值 <code>ys</code> 和自变量
<code>xs</code>，返回的梯度值与 <code>ys</code> 和 <code>xs</code>
张量之间的形状关系如下：</p>
<hr>
<p><strong><code>tf.gradients()</code> 返回的结果是一个长度为
<code>len(xs)</code>
的张量列表</strong>。<strong>--------------这是重点</strong></p>
<p><strong>一般情况下，<code>tf.gradients(self.q, self.a)</code>
的返回值形状应该是与 <code>self.a</code> 的形状相同</strong></p>
<hr>
<p>如果 <code>ys</code> 是单个张量，那么每个返回的张量表示相应的
<code>xs</code> 元素对 <code>ys</code> 的梯度。如果 <code>ys</code>
是张量列表，那么返回的张量列表中的每个张量是所有 <code>ys</code>
张量对相应 <code>xs</code> 元素的梯度之和。</p>
<p>具体来说，假设 <code>ys = (y1, y2,..., ym)</code>
是一个张量列表，<code>xs = (x1, x2,..., xn)</code>
也是一个张量列表。那么返回的梯度列表中第 <code>i</code>
个张量表示的是所有 <code>y</code> 张量对 <code>xi</code>
的梯度之和，即：</p>
<p><span class="math inline">\(\frac{d(y1 + y2 +... +
ym)}{dx_i}\)</span></p>
<p>如果 <code>ys</code>
是单个张量，那么返回的列表中只有一个张量，其形状与 <code>xs</code>
中的元素形状相对应，表示该 <code>ys</code> 张量对每个 <code>xs</code>
元素的梯度。</p>
<p>例如，当 <code>ys = (y1, y2)</code>，<code>xs = (x1, x2, x3)</code>
时，返回的梯度结果为 <code>(grad1, grad2, grad3)</code>，其中：</p>
<ul>
<li><code>grad1</code> 表示的是 <span class="math inline">\(\frac{y1}{x1} + \frac{y2}{x1}\)</span></li>
<li><code>grad2</code> 表示的是 <span class="math inline">\(\frac{y1}{x2} + \frac{y2}{x2}\)</span></li>
<li><code>grad3</code> 表示的是 <span class="math inline">\(\frac{y1}{x3} + \frac{y2}{x3}\)</span></li>
</ul>
<p>另外，<code>tf.gradients()</code> 还有一个参数
<code>grad_ys</code>，它也是一个张量列表，用于对 <code>ys</code>
中的每个张量进行加权。如果提供了
<code>grad_ys</code>，则计算梯度时会将每个 <code>y</code> 张量乘以对应的
<code>grad_ys</code> 张量后再进行求导。</p>
<p>例如，在上述同样的 <code>ys</code> 和 <code>xs</code> 的情况下，如果
<code>grad_ys = (g1, g2)</code>，那么计算的梯度将变为：</p>
<ul>
<li><code>grad1</code> 表示的是 <span class="math inline">\(\frac{g1 *
y1}{x1} + \frac{g2 * y2}{x1}\)</span></li>
<li><code>grad2</code> 表示的是 <span class="math inline">\(\frac{g1 *
y1}{x2} + \frac{g2 * y2}{x2}\)</span></li>
<li><code>grad3</code> 表示的是 <span class="math inline">\(\frac{g1 *
y1}{x3} + \frac{g2 * y2}{x3}\)</span></li>
</ul>
<h4 id="如何理解-grad_ys">如何理解 <code>grad_ys</code></h4>
<p><code>grad_ys</code>（可选）是一个张量或张量列表，用于对
<code>ys</code> 中的每个元素进行加权。</p>
<p>当使用 <code>tf.gradients(ys, xs, grad_ys)</code> 时，如果
<code>grad_ys</code> 不为空，就需要使用梯度求导的链式法则来计算相对于
<code>xs</code> 的导数。</p>
<p>假设 <code>grad_ys = (grad_ys1, grad_ys2,..., grad_ysn)</code>，其中
<code>n</code> 是 <code>ys</code>
中元素的个数，<code>xs = (x1, x2,..., xm)</code>。那么对于
<code>xs</code> 中的每个元素 <code>xi</code>，其梯度计算方式为：</p>
<p>新的梯度
<code>new_grad(i) = ∂/∂xi = ∂/∂z1 * ∂z1/∂xi + ∂/∂z2 * ∂z2/∂xi +... + ∂/∂zn * ∂zn/∂xi</code>，其中
<code>z1, z2,..., zn</code> 是 <code>ys</code> 中的元素。</p>
<p>也就是说，<code>grad_ys</code> 中的每个张量会分别与对应的
<code>ys</code> 元素相乘，然后在计算相对于 <code>xs</code>
的梯度时，这些加权后的结果会参与到链式法则的计算中。</p>
<p>例如，在代码示例中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, shape=(<span class="number">3</span>))</span><br><span class="line">w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, shape=(<span class="number">3</span>))</span><br><span class="line">w3 = tf.get_variable(<span class="string">&#x27;w3&#x27;</span>, shape=(<span class="number">3</span>))</span><br><span class="line">w4 = tf.get_variable(<span class="string">&#x27;w4&#x27;</span>, shape=(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">z1 = <span class="number">3</span>*w1 + <span class="number">2</span>*w2 + w3</span><br><span class="line">z2 = -<span class="number">1</span>*w3 + w4</span><br><span class="line"></span><br><span class="line">grads = tf.gradients((z1, z2), (w1, w2, w3, w4), grad_ys=((-<span class="number">2.0</span>,-<span class="number">3.0</span>,-<span class="number">4.0</span>), (-<span class="number">2.0</span>,-<span class="number">3.0</span>,-<span class="number">4.0</span>)))</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    <span class="built_in">print</span>(sess.run(grads))</span><br></pre></td></tr></table></figure>
<p>如果不考虑 <code>grad_ys</code>，输出应该是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(array((3., 3., 3.), dtype=float32),</span><br><span class="line"> array((2., 2., 2.), dtype=float32),</span><br><span class="line"> array((1., 1., 1.), dtype=float32),</span><br><span class="line"> array((1., 1., 1.), dtype=float32))</span><br></pre></td></tr></table></figure>
<p>现在在权重参数
<code>grad_ys = ((-2.0,-3.0,-4.0), (-2.0,-3.0,-4.0))</code>
的加权下，输出实际为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(array((-6., -9., -12.), dtype=float32),</span><br><span class="line"> array((-4., -6., -8.), dtype=float32),</span><br><span class="line"> array((0., 0., 0.), dtype=float32),</span><br><span class="line"> array((1., 1., 1.), dtype=float32))</span><br></pre></td></tr></table></figure>
<p>具体计算过程为，对于 <code>w1</code> 的梯度，原来的梯度是
<code>3</code>（来自 <code>z1</code> 对 <code>w1</code>
的偏导），现在加权后为 <code>-2.0 * 3 = -6.0</code>；对于
<code>w2</code> 的梯度，原来的梯度是 <code>2</code>（来自
<code>z1</code> 对 <code>w2</code> 的偏导），加权后为
<code>-3.0 * 2 = -6.0</code>；对于 <code>w3</code> 的梯度，来自
<code>z1</code> 的部分为 <code>-2.0 * 1 = -2.0</code>，来自
<code>z2</code> 的部分为 <code>-2.0 * (-1) = 2.0</code>，相加得到
<code>0.0</code>。</p>
<p>这样，通过设置 <code>grad_ys</code>，可以对 <code>ys</code>
中每个元素的梯度进行不同的加权，从而更灵活地控制梯度的计算。在实际应用中，这可以根据具体的需求和模型架构来调整梯度的传播和计算方式。</p>
<h5 id="为什么需要加权">为什么需要加权</h5>
<p>在某些情况下，需要对梯度进行加权是为了在计算梯度时对不同的部分进行不同程度的强调或重视。</p>
<p>例如，在深度学习中，不同的输出或误差项可能对最终的结果有不同的重要性。通过为
<code>grad_ys</code>中的每个张量设置合适的权重，可以根据这些重要性的差异来调整梯度的计算。</p>
<p>具体来说，加权可以用于以下几种情况：</p>
<ol type="1">
<li><strong>强调重要的输出或误差</strong>：如果某些输出或误差对于模型的性能或目标更关键，那么可以给它们的梯度分配较大的权重，以便在反向传播过程中更显著地影响参数的调整。</li>
<li><strong>平衡不同规模或量级的因素</strong>：当不同的输出或误差具有不同的量级时，加权可以帮助平衡它们对梯度的贡献，避免较大量级的部分主导了梯度的计算。</li>
<li><strong>实现特定的优化目标</strong>：根据具体的优化目标或任务需求，设置特定的权重来引导模型的学习方向。</li>
<li><strong>处理多任务学习</strong>：在多任务学习场景中，不同的任务可能具有不同的重要性，通过加权梯度可以更好地协调不同任务之间的学习。</li>
</ol>
<p>例如，假设有两个输出 <code>y1</code>和
<code>y2</code>，对应的梯度分别为 <code>dy1/dx</code>和
<code>dy2/dx</code>。如果直接计算梯度之和，那么它们对最终梯度的贡献是相等的。但如果
<code>y1</code>相对更重要，就可以为
<code>dy1/dx</code>设置较大的权重，使得它在决定参数调整时起到更大的作用。</p>
<p>在 TensorFlow 中，通过设置
<code>grad_ys</code>参数来实现加权梯度计算。<code>grad_ys</code>是一个与
<code>ys</code>长度相同的张量列表，其中每个张量用于对相应的
<code>ys</code>元素的梯度进行加权。这样，在计算梯度时，就会考虑这些加权值，从而实现对不同部分梯度的不同程度的重视。</p>
<p>例如，在上述代码示例中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grad_ys = (tf.variable(((<span class="number">1</span>))), tf.variable(((<span class="number">2</span>))))</span><br><span class="line">grads = tf.gradients((res2a, res2b), (w1, w2), grad_ys=grad_ys)</span><br></pre></td></tr></table></figure>
<p>这里 <code>grad_ys</code>被设置为
<code>(tf.variable(((1))), tf.variable(((2))))</code>，表示在计算梯度时，<code>res2a</code>的梯度将乘以
1，而 <code>res2b</code>的梯度将乘以 2，从而实现了对
<code>res2b</code>的梯度相对更重视的效果。这样，在后续的反向传播和参数更新中，<code>res2b</code>的梯度将对参数调整产生更大的影响。</p>
<p>总的来说，加权梯度提供了一种灵活的方式，根据具体问题和需求，对不同部分的梯度进行定制化的处理，以更好地实现模型的学习和优化目标。但确定合适的权重通常需要对问题有深入的理解，并可能需要通过试验和调优来找到最优的设置。</p>
<h5 id="如何为-grad_ys-中的每个张量设置合适的权重">如何为
<code>grad_ys</code> 中的每个张量设置合适的权重？</h5>
<p>为
<code>grad_ys</code>中的每个张量设置合适的权重，需要根据具体的问题和模型需求来决定。设置权重的目的是为了在计算梯度时对不同的部分进行不同程度的加权。</p>
<p>一种常见的方法是根据对每个元素的重要性或关注度来分配权重。例如，如果某个元素对于最终结果的影响较大，或者你希望在梯度计算中更强调该元素的变化，那么可以为其分配较大的权重；反之，如果某个元素相对不那么重要，可以分配较小的权重。</p>
<p>另外，也可以基于一些先验知识或经验来设置权重。例如，在某些情况下，可能知道某些部分的梯度对于整体优化的重要性程度。</p>
<p>然而，确定最合适的权重通常需要通过试验和调优来找到。这可能涉及尝试不同的权重组合，并观察模型在训练或优化过程中的性能表现，以找到能够达到最佳效果的权重设置。</p>
<p>例如，在上述代码示例中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grad_ys = (tf.variable(((<span class="number">1</span>))), tf.variable(((<span class="number">2</span>))))</span><br><span class="line">grads = tf.gradients((res2a, res2b), (w1, w2), grad_ys=grad_ys)</span><br></pre></td></tr></table></figure>
<p>这里 <code>grad_ys</code>被设置为
<code>(tf.variable(((1))), tf.variable(((2))))</code>，表示在计算梯度时，<code>res2a</code>的梯度将乘以
1，而 <code>res2b</code>的梯度将乘以
2，从而实现了对不同部分梯度的不同加权。</p>
<p>具体到你的问题中，如何设置权重取决于你的具体任务和数据特点。你可能需要对问题有深入的理解，并结合一些实验和观察来找到合适的权重设置。这可能需要一些尝试和错误，以及对模型输出和性能的仔细分析。同时，也可以考虑使用一些自动调参的技术或算法来帮助找到最优的权重配置。</p>
<h3 id="np.hstack">np.hstack</h3>
<p><code>np.hstack</code> 是 NumPy
库中的一个函数，用于水平（按列）堆叠数组。</p>
<p>其函数签名通常为 <code>np.hstack(tup)</code> ，其中 <code>tup</code>
是一个元组，包含要堆叠的数组。</p>
<p>例如，如果有两个数组 <code>a = np.array([1, 2, 3])</code> 和
<code>b = np.array([4, 5, 6])</code> ，使用
<code>np.hstack((a, b))</code> 将会得到一个新的数组
<code>[1, 2, 3, 4, 5, 6]</code> 。</p>
<p>再比如，如果 <code>a = np.array([[1, 2], [3, 4]])</code>
，<code>b = np.array([[5, 6], [7, 8]])</code> ，那么
<code>np.hstack((a, b))</code> 的结果是
<code>[[1, 2, 5, 6], [3, 4, 7, 8]]</code> 。</p>
<p><code>np.hstack</code>
主要用于将多个数组在水平方向上拼接成一个新的数组，前提是参与拼接的数组在除了要拼接的维度（这里是列维度）之外的其他维度上形状要匹配。</p>
<h3 id="td-error">td error</h3>
<p>TD error 即时间差分误差（Temporal Difference
error），它是强化学习中一个重要的概念，用于衡量当前估计值与目标值之间的差异。</p>
<p>其定义为：当前时刻的状态值或动作值的估计与基于即时回报和下一时刻状态值或动作值的估计所构成的目标值之间的差值。</p>
<p>例如，在某个时刻 t，TD error 可以表示为：<span class="math inline">\(\delta_t = V_t(s_t) - (r_{t+1} + \gamma
V_t(s_{t+1}))\)</span> 或者 <span class="math inline">\(\delta_t =
Q_t(s_t, a_t) - (r_{t+1} + \gamma Q_t(s_{t+1}, a_{t+1}))\)</span> ，其中
<span class="math inline">\(V_t(s_t)\)</span> 表示 t 时刻对状态 <span class="math inline">\(s_t\)</span> 的状态值估计，<span class="math inline">\(Q_t(s_t, a_t)\)</span> 表示 t 时刻对状态 <span class="math inline">\(s_t\)</span> 采取动作 <span class="math inline">\(a_t\)</span> 的动作值估计，<span class="math inline">\(r_{t+1}\)</span> 是即时回报，<span class="math inline">\(\gamma\)</span> 是折扣系数，<span class="math inline">\(V_t(s_{t+1})\)</span> 和 <span class="math inline">\(Q_t(s_{t+1}, a_{t+1})\)</span>
分别是下一时刻的状态值估计和动作值估计。</p>
<p>TD error 的主要作用是指导强化学习算法的学习和更新过程。通过计算 TD
error，算法可以了解当前的估计与实际情况之间的差距，并据此调整模型的参数，以使得估计值更接近真实值。较小的
TD error 表示当前的估计较为准确，而较大的 TD error
则意味着模型需要进行更大的调整。</p>
<p>在强化学习中，一些算法（如 Q-learning、SARSA 等）会使用 TD error
来更新价值函数或策略，以逐步优化智能体的行为，使其能够在环境中获得更好的累积回报。</p>
<p>TD error
具有一些优点，例如它可以在没有完整状态序列的情况下进行学习，能够更快速灵活地更新状态或动作的价值估计。然而，它得到的价值是一种有偏估计，但其方差通常比其他方法（如蒙特卡洛方法）得到的方差要低。</p>
<p>不同的强化学习场景和算法中，TD error
的具体形式和应用可能会有所不同，但总体上都是用于衡量估计值与目标值的差异，以推动学习和优化的进行。如果在强化学习训练中遇到
TD error
值很大的情况，可能的原因包括网络架构不够复杂、训练样本数量不足、学习率过大或没有使用足够多的经验回放等。可以尝试使用更复杂的网络架构、增加训练数据、调低学习率、增加经验回放或使用更多的超参数调整方法来解决。</p>
<h3 id="tf.distributions.normal">tf.distributions.Normal</h3>
<p><code>tf.distributions.Normal</code> 是 TensorFlow
中用于定义正态分布（高斯分布）的类。</p>
<p>它的初始化函数 <code>__init__</code> 的参数如下：</p>
<ul>
<li><code>loc</code>：表示正态分布的均值（μ），是一个浮点型张量。</li>
<li><code>scale</code>：表示正态分布的标准差（σ），也是一个浮点型张量，且必须包含只正数值。</li>
<li><code>validate_args</code>：一个 Python 布尔值，默认为
<code>False</code>。当为 <code>True</code> 时，会进行参数验证。</li>
<li><code>allow_nan_stats</code>：一个 Python 布尔值，默认为
<code>True</code>，用于描述在统计量未定义时的行为。</li>
</ul>
<p>通过创建 <code>tf.distributions.Normal</code>
对象，可以进行与正态分布相关的操作，例如采样、计算概率、计算熵、交叉熵、KL
散度等。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow_probability <span class="keyword">as</span> tfp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个均值为 0，标准差为 1 的正态分布</span></span><br><span class="line">dist = tfp.distributions.Normal(loc=<span class="number">0.</span>, scale=<span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从该正态分布中采样</span></span><br><span class="line">samples = dist.sample((<span class="number">3</span>,)) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算某个值的概率密度函数（pdf）值</span></span><br><span class="line">pdf_value = dist.prob(<span class="number">0.5</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算累积分布函数（cdf）值</span></span><br><span class="line">cdf_value = dist.cdf(<span class="number">0.5</span>) </span><br></pre></td></tr></table></figure>
<p>在正态分布中，概率密度函数（pdf）的公式为：</p>
<p><span class="math display">\[
pdf(x; \mu, \sigma) = \frac{exp(-0.5(x - \mu)^2 / \sigma^2)}{\sqrt{(2\pi
\sigma^2)}}
\]</span></p>
<p>其中，<code>loc = mu</code> 是均值，<code>scale = sigma</code>
是标准差，<code>z</code> 是归一化常数。</p>
<p>正态分布是位置-尺度族的一员，也就是说，可以通过以下方式构建：</p>
<p><span class="math display">\[
x \sim Normal(loc=0, scale=1)
\]</span></p>
<p><span class="math display">\[
y = loc + scale * x
\]</span></p>
<p><code>tf.distributions.Normal</code> 类中还提供了其他一些方法，如
<code>covariance</code>（协方差）、<code>cross_entropy</code>（交叉熵）、<code>entropy</code>（熵）、<code>log_cdf</code>（对数累积分布函数）、<code>log_prob</code>（对数概率）、<code>log_survival_function</code>（对数生存函数）、<code>mean</code>（均值）、<code>mode</code>（众数）、<code>stddev</code>（标准差）、<code>variance</code>（方差）等，这些方法可以方便地用于各种与正态分布相关的计算和操作。具体使用方法可以参考
TensorFlow 的官方文档。</p>
<p>这些方法使得在 TensorFlow
中处理正态分布变得更加方便和高效，尤其是在涉及到概率计算、随机采样以及与其他分布进行比较和组合的任务中。例如，在一些机器学习和深度学习模型中，可能需要使用正态分布来初始化参数、生成随机噪声或者构建概率模型等。通过
<code>tf.distributions.Normal</code> 类，可以以一种与 TensorFlow
计算图兼容的方式进行这些操作，从而更好地利用 TensorFlow
的自动微分和优化功能。同时，还可以方便地处理批量数据（即同时处理多个正态分布，每个分布具有不同的参数），这在处理大规模数据或多任务学习等场景中非常有用。</p>
<h3 id="tf.clip_by_value">tf.clip_by_value</h3>
<p><code>tf.clip_by_value</code>是 TensorFlow
中的一个函数，用于将张量中的数值限制在指定的范围内。</p>
<p>它的语法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.clip_by_value(t, clip_value_min, clip_value_max, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>t</code>：要进行裁剪的张量。</li>
<li><code>clip_value_min</code>：裁剪的最小值。如果张量中的值小于这个最小值，它们将被设置为这个最小值。</li>
<li><code>clip_value_max</code>：裁剪的最大值。如果张量中的值大于这个最大值，它们将被设置为这个最大值。</li>
<li><code>name</code>（可选）：操作的名称。</li>
</ul>
<p>例如，如果有一个张量 <code>t</code>，使用
<code>tf.clip_by_value</code>可以将其值限制在 <code>2</code>和
<code>5</code>之间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">t = tf.constant(((<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>), (<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>)))</span><br><span class="line">t2 = tf.clip_by_value(t, <span class="number">2.5</span>, <span class="number">4.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(sess.run(t2)) </span><br></pre></td></tr></table></figure>
<p>输出结果将是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">((2.5, 2.5, 3.0), (4.0, 4.5, 4.5))</span><br></pre></td></tr></table></figure>
<p>在上述例子中，原始张量 <code>t</code>中的值小于
<code>2.5</code>的被替换为 <code>2.5</code>，大于
<code>4.5</code>的被替换为 <code>4.5</code>。</p>
<p>需要注意的是，<code>clip_value_min</code>需要小于或等于
<code>clip_value_max</code>，以获得正确的结果。如果输入的张量
<code>t</code>的元素类型是整数，而 <code>clip_value_min</code>或
<code>clip_value_max</code>是浮点数，需要先将整数张量转换为浮点数，否则可能会引发类型错误。此外，<code>clip_value_min</code>和
<code>clip_value_max</code>可以是标量张量，也可以是能广播到张量
<code>t</code>形状的张量。</p>
<h3 id="torch.nn.module">torch.nn.Module</h3>
<p><code>torch.nn.Module</code> 是 PyTorch
中所有神经网络模块的基类。它提供了许多用于构建和训练神经网络的基本功能。以下是一些关键点：</p>
<ol type="1">
<li><p><strong>模型定义</strong>：<code>torch.nn.Module</code>
是所有神经网络模块的基类，因此你可以通过继承它来定义自己的神经网络模型。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)  <span class="comment"># 定义一个全连接层</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></li>
<li><p><strong>参数管理</strong>：<code>torch.nn.Module</code>
自动管理模型的所有参数。当你定义一个神经网络层（如
<code>nn.Linear</code>、<code>nn.Conv2d</code>
等）时，这些层会自动添加到模型的参数列表中。例如，在上面的
<code>MyModel</code> 中，<code>self.fc</code>
是一个线性层，它的参数（权重和偏置）会被自动添加到模型中。</p></li>
<li><p><strong>前向传播</strong>：你需要定义一个 <code>forward</code>
方法来指定数据如何通过模型进行前向传播。例如，在上面的
<code>MyModel</code> 中，<code>forward</code> 方法将输入 <code>x</code>
传递给线性层 <code>self.fc</code>。</p></li>
<li><p><strong>训练和评估</strong>：<code>torch.nn.Module</code>
提供了一些方法来管理模型的训练和评估过程，例如
<code>train()</code>、<code>eval()</code>、<code>zero_grad()</code>、<code>step()</code>
等。</p></li>
<li><p><strong>保存和加载</strong>：<code>torch.nn.Module</code> 提供了
<code>save</code> 和 <code>load_state_dict</code>
方法来保存和加载模型的参数。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = MyModel()</span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model.pth&#x27;</span>)</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model.pth&#x27;</span>))</span><br></pre></td></tr></table></figure></li>
<li><p><strong>子模块</strong>：<code>torch.nn.Module</code>
可以包含其他子模块，这些子模块也可以是 <code>torch.nn.Module</code>
的实例。例如，一个复杂的神经网络可能由多个层和子网络组成。</p></li>
</ol>
<p>总的来说，<code>torch.nn.Module</code> 是 PyTorch
中构建和训练神经网络的核心类，它提供了许多方便的功能来管理模型参数、前向传播、训练和评估等。</p>
<h3 id="abc.abc">abc.ABC</h3>
<p>#abc.ABC 是 Python 的抽象基类，用于定义抽象方法。</p>
<p>在 Python 中，<code>abc.ABC</code>类是用于实现抽象基类（Abstract Base
Class）的。</p>
<p>抽象基类主要用于定义一组必须由其子类实现的方法。通过从
<code>abc.ABC</code>类继承，您可以创建一个抽象基类，并在其中定义抽象方法。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> abc</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyAbstractClass</span>(abc.ABC):</span><br><span class="line"><span class="meta">    @abc.abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">abstract_method</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConcreteClass</span>(<span class="title class_ inherited__">MyAbstractClass</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">abstract_method</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Implemented abstract method&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>在上述示例中，<code>MyAbstractClass</code>是一个抽象基类，它定义了一个抽象方法
<code>abstract_method</code>。<code>ConcreteClass</code>是
<code>MyAbstractClass</code>的子类，并且实现了这个抽象方法。</p>
<p>使用抽象基类的好处在于，它可以强制子类遵循一定的接口规范，有助于提高代码的可读性、可维护性和可扩展性。例如，在一个大型项目中，如果多个类都需要实现某些特定的方法，通过定义一个抽象基类，可以确保这些方法在所有相关的子类中都得到正确实现。</p>
<h3 id="property"><code>@property</code></h3>
<p><code>@property</code> 是 Python
的装饰器语法，用于将一个方法转换为只读属性。这意味着你可以像访问属性一样访问这个方法，而不需要调用它。这对于创建只读属性或需要计算属性值的方法非常有用。</p>
<p>下面是一个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Circle</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, radius</span>):</span><br><span class="line">        self._radius = radius</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">radius</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._radius</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">diameter</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._radius * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">c = Circle(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(c.radius)  <span class="comment"># 输出: 5</span></span><br><span class="line"><span class="built_in">print</span>(c.diameter)  <span class="comment"># 输出: 10</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>radius</code> 和 <code>diameter</code>
都是只读属性。<code>radius</code> 属性直接返回 <code>_radius</code>
属性的值，而 <code>diameter</code> 属性计算并返回直径的值。</p>
<p>使用 <code>@property</code>
装饰器的好处是，它允许你保持属性访问的一致性，同时提供额外的功能。例如，你可以使用
<code>@property</code> 装饰器来定义一个计算属性，或者使用
<code>@property</code> 和 <code>@radius.setter</code>
装饰器来定义一个可写的属性。</p>
<h3 id="import-gym">import gym</h3>
<p><code>import gym</code> 通常用于导入 OpenAI Gym 库。</p>
<p>OpenAI Gym
是一个用于开发和比较强化学习算法的工具包。它提供了各种各样的环境，例如经典控制问题（如
CartPole、MountainCar 等）、Atari 游戏等。</p>
<p>通过导入 Gym
库，您可以创建这些环境，并与它们进行交互，以测试和开发强化学习算法。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)</span><br><span class="line">observation = env.reset()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    env.render()</span><br><span class="line">    action = env.action_space.sample()  <span class="comment"># 随机选择一个动作</span></span><br><span class="line">    observation, reward, done, info = env.step(action)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> done:</span><br><span class="line">        observation = env.reset()</span><br><span class="line"></span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure>
<p>在上述代码中，首先创建了 <code>CartPole-v1</code>
环境，然后进行了一系列的交互操作，包括获取初始观察值、采取动作、获取新的观察值和奖励等。</p>
<p>不同的环境具有不同的状态、动作空间和奖励机制，您可以根据具体的问题和需求选择合适的环境进行强化学习算法的开发和实验。</p>
<h3 id="torch.utils.data.dataloader">torch.utils.data.DataLoader</h3>
<p><code>torch.utils.data.DataLoader</code>是 PyTorch
中用于加载数据的重要工具。以下是关于它的详细介绍：</p>
<p><strong>一、功能概述</strong></p>
<p><code>DataLoader</code>的主要作用是将数据集（通常是自定义的数据集类对象）进行封装，为模型训练或评估提供高效的数据迭代器。它可以自动处理数据的批处理、随机打乱、并行加载等操作，极大地简化了数据加载的过程。</p>
<p><strong>二、主要参数</strong></p>
<ol type="1">
<li><code>dataset</code>：这是必须提供的参数，代表要加载的数据集对象。这个数据集对象通常需要实现
<code>__len__</code>方法以返回数据集的大小，以及
<code>__getitem__</code>方法以支持通过索引获取数据样本。</li>
<li><code>batch_size</code>：指定每一批数据的大小。例如，如果设置为
32，那么每次迭代将返回 32 个数据样本。</li>
<li><code>shuffle</code>：布尔值，表示是否在每个 epoch
开始时随机打乱数据的顺序。对于训练集，通常设置为 True
以增加数据的多样性；对于测试集，一般设置为 False。</li>
<li><code>num_workers</code>：指定用于数据加载的子进程数量。增加这个值可以提高数据加载的速度，但也会占用更多的系统资源。</li>
<li><code>collate_fn</code>：一个可选的函数，用于将一批数据样本整理成适合模型输入的格式。如果不提供，<code>DataLoader</code>会使用默认的整理函数。</li>
<li><code>pin_memory</code>参数主要用于提高数据从 CPU 内存传输到 GPU
内存的速度。当
<code>pin_memory=True</code>时，<code>DataLoader</code>会在将数据加载到
CPU
内存后，立即将其复制到一个固定的（pinned）内存区域。这个固定内存区域可以被直接映射到
GPU 的内存地址空间，从而减少了数据从 CPU 传输到 GPU
时的内存复制开销。</li>
</ol>
<p><strong>三、使用方法</strong></p>
<ol type="1">
<li>首先，定义一个自定义的数据集类，实现 <code>__len__</code>和
<code>__getitem__</code>方法。例如：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data</span>):</span><br><span class="line">        self.data = data</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.data[index]</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>然后，创建 <code>DataLoader</code>对象：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]</span><br><span class="line">dataset = MyDataset(data)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">3</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<ol start="3" type="1">
<li>最后，可以通过迭代 <code>DataLoader</code>对象来获取批量数据：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="built_in">print</span>(batch)</span><br></pre></td></tr></table></figure>
<p><strong>四、优势和应用场景</strong></p>
<ol type="1">
<li>高效的数据加载：通过并行加载和批处理，可以大大提高数据加载的速度，减少模型训练或评估的等待时间。</li>
<li>灵活性：可以与各种不同类型的数据集配合使用，包括自定义的数据集类。同时，可以方便地调整批处理大小、随机打乱等参数，以适应不同的任务需求。</li>
<li>深度学习中的广泛应用：在图像分类、自然语言处理、语音识别等深度学习任务中，<code>DataLoader</code>是不可或缺的工具，用于为模型提供训练和测试数据。</li>
</ol>
<h3 id="np.clip">np.clip</h3>
<p>“np.clip”通常是在 Python
编程中使用的一个函数。它用于将数组中的值限制在给定的区间范围内，以达到对数值进行裁剪或限制的目的。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz">ALTNT</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/">http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/img/altnt.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5/" title="强化学习相关概念"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">强化学习相关概念</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/22/%E8%B5%84%E6%96%99/%E5%91%BD%E4%BB%A4/" title="命令"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">命令</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/altnt.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ALTNT</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">35</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ALTNT"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB"><span class="toc-number">1.</span> <span class="toc-text">机器学习分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.</span> <span class="toc-text">回归问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BB%BA%E7%AB%8B"><span class="toc-number">1.2.</span> <span class="toc-text">回归模型的建立</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%E7%A1%AE%E5%AE%9A%E7%94%A8%E4%BA%8E%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.2.1.</span> <span class="toc-text">第一步：确定用于回归任务的模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E6%9D%A5%E8%A1%A1%E9%87%8F%E8%BF%99%E4%BA%9B%E5%A4%87%E9%80%89%E5%87%BD%E6%95%B0%E7%9A%84%E5%A5%BD%E5%9D%8F%E7%A8%8B%E5%BA%A6%E7%A1%AE%E5%AE%9A%E5%8F%82%E6%95%B0"><span class="toc-number">1.2.2.</span> <span class="toc-text">第二步：使用训练数据来衡量这些备选函数的好坏程度（确定参数）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%E6%A0%B9%E6%8D%AE%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%80%89%E5%87%BA%E6%8B%9F%E5%90%88%E6%9C%80%E5%A5%BD%E7%9A%84%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%9C%80%E7%BB%88%E7%9A%84%E6%8B%9F%E5%90%88%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.3.</span> <span class="toc-text">第三步：根据训练数据选出拟合最好的函数，作为最终的拟合函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="toc-number">2.1.</span> <span class="toc-text">预测结果分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">2.2.</span> <span class="toc-text">正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%81%97%E7%95%99%E9%97%AE%E9%A2%98"><span class="toc-number">2.3.</span> <span class="toc-text">遗留问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-number">2.4.</span> <span class="toc-text">梯度下降算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%AD%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7tips"><span class="toc-number">2.4.1.</span> <span class="toc-text">梯度下降中常用技巧（Tips）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%80%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">2.4.1.1.</span> <span class="toc-text">一、调整学习率</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#adagrad%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">2.4.1.1.1.</span> <span class="toc-text">AdaGrad优化器</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BA%8C%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8Dsgd"><span class="toc-number">2.4.1.2.</span> <span class="toc-text">二、随机梯度下降（SGD）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%89%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BEfeature-scaling"><span class="toc-number">2.4.1.3.</span> <span class="toc-text">三、特征缩放（Feature Scaling）</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">3.</span> <span class="toc-text">梯度上升和梯度下降的区别是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="toc-number">4.</span> <span class="toc-text">偏差和方差</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E5%AE%9A%E4%B9%89"><span class="toc-number">4.1.</span> <span class="toc-text">数学定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-number">4.2.</span> <span class="toc-text">偏差-方差与模型复杂度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95"><span class="toc-number">4.3.</span> <span class="toc-text">调整方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#k-%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">4.4.</span> <span class="toc-text">K-折交叉验证</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax"><span class="toc-number">5.</span> <span class="toc-text">softmax</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E7%94%A8%E9%80%94"><span class="toc-number">5.1.</span> <span class="toc-text">主要用途:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82fully-connected-layer"><span class="toc-number">6.</span> <span class="toc-text">全连接层（Fully Connected
Layer）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5embedding%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3"><span class="toc-number">7.</span> <span class="toc-text">嵌入(embedding)层的理解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5%E5%B1%82%E7%9A%84%E4%B8%80%E4%B8%AA%E4%BD%9C%E7%94%A8%E9%99%8D%E7%BB%B4"><span class="toc-number">7.1.</span> <span class="toc-text">嵌入层的一个作用——降维</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5%E5%B1%82%E7%9A%84%E4%B8%80%E4%B8%AA%E4%BD%9C%E7%94%A8%E5%8D%87%E7%BB%B4"><span class="toc-number">7.2.</span> <span class="toc-text">嵌入层的一个作用——升维</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">8.</span> <span class="toc-text">池化层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">8.1.</span> <span class="toc-text">池化层的作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#max-pooling"><span class="toc-number">8.2.</span> <span class="toc-text">Max pooling:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">9.</span> <span class="toc-text">逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">9.1.</span> <span class="toc-text">1. 什么是逻辑回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFsigmoid%E5%87%BD%E6%95%B0"><span class="toc-number">9.2.</span> <span class="toc-text">2. 什么是Sigmoid函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">9.3.</span> <span class="toc-text">3. 损失函数是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E4%BB%A5%E8%BF%9B%E8%A1%8C%E5%A4%9A%E5%88%86%E7%B1%BB%E5%90%97"><span class="toc-number">9.4.</span> <span class="toc-text">4.可以进行多分类吗？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%9C%89%E4%BB%80%E4%B9%88%E4%BC%98%E7%82%B9"><span class="toc-number">9.5.</span> <span class="toc-text">5.逻辑回归有什么优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%9C%89%E5%93%AA%E4%BA%9B%E5%BA%94%E7%94%A8"><span class="toc-number">9.6.</span> <span class="toc-text">6. 逻辑回归有哪些应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%B8%B8%E7%94%A8%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="toc-number">9.7.</span> <span class="toc-text">7.
逻辑回归常用的优化方法有哪些</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E9%98%B6%E6%96%B9%E6%B3%95"><span class="toc-number">9.7.1.</span> <span class="toc-text">7.1 一阶方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E9%98%B6%E6%96%B9%E6%B3%95%E7%89%9B%E9%A1%BF%E6%B3%95%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95"><span class="toc-number">9.7.2.</span> <span class="toc-text">7.2 二阶方法：牛顿法、拟牛顿法：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E6%96%AF%E7%89%B9%E5%9B%9E%E5%BD%92%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%AF%B9%E7%89%B9%E5%BE%81%E8%BF%9B%E8%A1%8C%E7%A6%BB%E6%95%A3%E5%8C%96"><span class="toc-number">9.8.</span> <span class="toc-text">8.
逻辑斯特回归为什么要对特征进行离散化。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0%E4%B8%AD%E5%A2%9E%E5%A4%A7l1%E6%AD%A3%E5%88%99%E5%8C%96%E4%BC%9A%E6%98%AF%E4%BB%80%E4%B9%88%E7%BB%93%E6%9E%9C"><span class="toc-number">9.9.</span> <span class="toc-text">9.
逻辑回归的目标函数中增大L1正则化会是什么结果。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">9.10.</span> <span class="toc-text">10. 代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#t-snet-distributed-stochastic-neighbor-embedding"><span class="toc-number">10.</span> <span class="toc-text">t-SNE（t-distributed
Stochastic Neighbor Embedding）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86%E9%AA%8C%E8%AF%81%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86"><span class="toc-number">11.</span> <span class="toc-text">训练集、验证集和测试集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6self-attention-mechanism"><span class="toc-number">12.</span> <span class="toc-text">自注意力机制（Self-Attention
Mechanism）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E8%9E%8D%E7%A0%94%E7%A9%B6ablation-study"><span class="toc-number">13.</span> <span class="toc-text">消融研究（Ablation Study）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0contrastive-learning"><span class="toc-number">14.</span> <span class="toc-text">对比学习（Contrastive
Learning）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF"><span class="toc-number">15.</span> <span class="toc-text">可变形卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E5%87%BA%E8%83%8C%E6%99%AF"><span class="toc-number">15.1.</span> <span class="toc-text">提出背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AF%E5%8F%98%E5%BD%A2%E5%8D%B7%E7%A7%AF-1"><span class="toc-number">15.2.</span> <span class="toc-text">可变形卷积</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#dcn-v1"><span class="toc-number">15.2.1.</span> <span class="toc-text">DCN v1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#dcn-v2"><span class="toc-number">15.2.2.</span> <span class="toc-text">DCN v2</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#paddle%E4%B8%AD%E7%9A%84api"><span class="toc-number">15.2.3.</span> <span class="toc-text">paddle中的API</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">16.</span> <span class="toc-text">随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">16.1.</span> <span class="toc-text">1.什么是随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#bagging%E6%80%9D%E6%83%B3"><span class="toc-number">16.1.1.</span> <span class="toc-text">1.1 Bagging思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-1"><span class="toc-number">16.1.2.</span> <span class="toc-text">1.2 随机森林</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%88%86%E7%B1%BB%E6%95%88%E6%9E%9C%E7%9A%84%E5%BD%B1%E5%93%8D%E5%9B%A0%E7%B4%A0"><span class="toc-number">16.2.</span> <span class="toc-text">2. 随机森林分类效果的影响因素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E6%9C%89%E4%BB%80%E4%B9%88%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">16.3.</span> <span class="toc-text">随机森林有什么优缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E7%BC%BA%E5%A4%B1%E5%80%BC"><span class="toc-number">16.4.</span> <span class="toc-text">4. 随机森林如何处理缺失值？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFoob%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E4%B8%ADoob%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E7%9A%84%E5%AE%83%E6%9C%89%E4%BB%80%E4%B9%88%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">16.5.</span> <span class="toc-text">5.
什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%9A%84%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98"><span class="toc-number">16.6.</span> <span class="toc-text">6. 随机森林的过拟合问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">16.7.</span> <span class="toc-text">7. 代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#import%E5%B7%A5%E5%85%B7%E5%BA%93"><span class="toc-number">16.7.1.</span> <span class="toc-text">0.import工具库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">16.7.2.</span> <span class="toc-text">1.加载数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="toc-number">16.7.3.</span> <span class="toc-text">构建模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gbdtgradient-boosting-decision-tree%E5%85%A8%E5%90%8D%E5%8F%AB%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">17.</span> <span class="toc-text">GBDT(Gradient
Boosting Decision Tree)，全名叫梯度提升决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E4%B8%80%E4%B8%8Bgbdt%E7%AE%97%E6%B3%95%E7%9A%84%E8%BF%87%E7%A8%8B"><span class="toc-number">17.1.</span> <span class="toc-text">1. 解释一下GBDT算法的过程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#boosting%E6%80%9D%E6%83%B3"><span class="toc-number">17.1.1.</span> <span class="toc-text">1.1 Boosting思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gbdt%E5%8E%9F%E6%9D%A5%E6%98%AF%E8%BF%99%E4%B9%88%E5%9B%9E%E4%BA%8B"><span class="toc-number">17.1.2.</span> <span class="toc-text">1.2 GBDT原来是这么回事</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">17.2.</span> <span class="toc-text">2.
梯度提升和梯度下降的区别和联系是什么？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gbdt%E7%9A%84%E4%BC%98%E7%82%B9%E5%92%8C%E5%B1%80%E9%99%90%E6%80%A7%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="toc-number">17.3.</span> <span class="toc-text">3.
GBDT的优点和局限性有哪些？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-number">17.3.1.</span> <span class="toc-text">3.1 优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">17.3.2.</span> <span class="toc-text">3.2 局限性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rf%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E4%B8%8Egbdt%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB"><span class="toc-number">17.4.</span> <span class="toc-text">4.
RF(随机森林)与GBDT之间的区别与联系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-2"><span class="toc-number">17.5.</span> <span class="toc-text">5. 代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-number">17.5.1.</span> <span class="toc-text">获取训练数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE"><span class="toc-number">17.5.2.</span> <span class="toc-text">获取测试数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gbdt%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B"><span class="toc-number">17.5.3.</span> <span class="toc-text">GBDT模型建立</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#xgboost"><span class="toc-number">18.</span> <span class="toc-text">XGBoost</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFxgboost"><span class="toc-number">18.1.</span> <span class="toc-text">1. 什么是XGBoost</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#xgboost%E6%A0%91%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-number">18.1.1.</span> <span class="toc-text">1.1 XGBoost树的定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E9%A1%B9%E6%A0%91%E7%9A%84%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-number">18.1.2.</span> <span class="toc-text">1.2 正则项：树的复杂度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%91%E8%AF%A5%E6%80%8E%E4%B9%88%E9%95%BF"><span class="toc-number">18.1.3.</span> <span class="toc-text">1.3 树该怎么长</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%81%9C%E6%AD%A2%E6%A0%91%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%94%9F%E6%88%90"><span class="toc-number">18.1.4.</span> <span class="toc-text">1.4 如何停止树的循环生成</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#xgboost%E4%B8%8Egbdt%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C"><span class="toc-number">18.2.</span> <span class="toc-text">2. XGBoost与GBDT有什么不同</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88xgboost%E8%A6%81%E7%94%A8%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80%E4%BC%98%E5%8A%BF%E5%9C%A8%E5%93%AA%E9%87%8C"><span class="toc-number">18.3.</span> <span class="toc-text">3.
为什么XGBoost要用泰勒展开，优势在哪里？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-3"><span class="toc-number">18.4.</span> <span class="toc-text">4. 代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">19.</span> <span class="toc-text">损失函数:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">19.1.</span> <span class="toc-text">常见的损失函数:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0%E8%AE%A1%E7%AE%97"><span class="toc-number">19.2.</span> <span class="toc-text">损失函数的导数计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E6%8D%9F%E5%A4%B1supervised-contrastive-loss"><span class="toc-number">20.</span> <span class="toc-text">监督对比损失（Supervised
Contrastive Loss）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">21.</span> <span class="toc-text">激活函数:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">21.1.</span> <span class="toc-text">常见的激活函数：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">22.</span> <span class="toc-text">学习率</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A5%E4%B8%8B%E6%98%AF%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95"><span class="toc-number">22.1.</span> <span class="toc-text">以下是一些常见的学习率调整方法：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#optimizer%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">23.</span> <span class="toc-text">optimizer的概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2image-segmentation"><span class="toc-number">24.</span> <span class="toc-text">图像分割(Image Segmentation)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B"><span class="toc-number">24.1.</span> <span class="toc-text">1、 图像分割模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B"><span class="toc-number">24.1.1.</span> <span class="toc-text">(1)
基于全卷积网络的图像分割模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90"><span class="toc-number">25.</span> <span class="toc-text">pytorch代码分析:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.one_hot"><span class="toc-number">25.1.</span> <span class="toc-text">tf.one_hot</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self.critic_net.parameters"><span class="toc-number">25.2.</span> <span class="toc-text">self.critic_net.parameters()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self.optimizer.zero_grad"><span class="toc-number">25.3.</span> <span class="toc-text">self.optimizer.zero_grad()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loss.backwardretain_graphtrue"><span class="toc-number">25.4.</span> <span class="toc-text">loss.backward(retain_graph&#x3D;True)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6%E9%9C%80%E8%A6%81%E5%A4%9A%E6%AC%A1%E4%BD%BF%E7%94%A8%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">25.4.1.</span> <span class="toc-text">如何判断是否需要多次使用计算图？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.reduce_mean"><span class="toc-number">25.5.</span> <span class="toc-text">tf.reduce_mean</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.placeholder"><span class="toc-number">25.6.</span> <span class="toc-text">tf.placeholder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.variable_scope"><span class="toc-number">25.7.</span> <span class="toc-text">tf.variable_scope</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-tf.variable_scope"><span class="toc-number">25.7.1.</span> <span class="toc-text">1. 什么是
tf.variable_scope？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8-tf.variable_scope"><span class="toc-number">25.7.2.</span> <span class="toc-text">2. 为什么使用
tf.variable_scope？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-tf.variable_scope"><span class="toc-number">25.7.3.</span> <span class="toc-text">3. 如何使用
tf.variable_scope？</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-1%E5%88%9B%E5%BB%BA%E5%8F%98%E9%87%8F%E4%BD%9C%E7%94%A8%E5%9F%9F"><span class="toc-number">25.7.3.1.</span> <span class="toc-text">示例 1：创建变量作用域</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-2%E5%A4%8D%E7%94%A8%E5%8F%98%E9%87%8F"><span class="toc-number">25.7.3.2.</span> <span class="toc-text">示例 2：复用变量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#tf.get_variable-%E5%92%8C-tf.variable-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">25.7.3.3.</span> <span class="toc-text">4.
tf.get_variable 和 tf.Variable 的区别</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-3%E4%BD%BF%E7%94%A8-tf.get_variable-%E5%92%8C-tf.variable"><span class="toc-number">25.7.3.4.</span> <span class="toc-text">示例 3：使用
tf.get_variable 和 tf.Variable</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">25.7.4.</span> <span class="toc-text">5. 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.session"><span class="toc-number">25.8.</span> <span class="toc-text">tf.Session</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-tf.session"><span class="toc-number">25.8.1.</span> <span class="toc-text">1. 什么是 tf.Session？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-tf.session"><span class="toc-number">25.8.2.</span> <span class="toc-text">2. 为什么需要
tf.Session？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-tf.session"><span class="toc-number">25.8.3.</span> <span class="toc-text">3. 如何使用 tf.Session？</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-1%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-number">25.8.3.1.</span> <span class="toc-text">示例 1：基本使用</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-2%E4%BD%BF%E7%94%A8-tf.placeholder-%E5%92%8C-feed_dict"><span class="toc-number">25.8.3.2.</span> <span class="toc-text">示例 2：使用
tf.placeholder 和 feed_dict</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-3%E7%AE%A1%E7%90%86%E5%8F%98%E9%87%8F"><span class="toc-number">25.8.3.3.</span> <span class="toc-text">示例 3：管理变量</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tensorflow-2.x-%E4%B8%AD%E7%9A%84%E5%8F%98%E5%8C%96"><span class="toc-number">25.8.4.</span> <span class="toc-text">4. TensorFlow 2.x 中的变化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-number">25.8.5.</span> <span class="toc-text">5. 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.random_normal_initializer"><span class="toc-number">25.9.</span> <span class="toc-text">tf.random_normal_initializer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86"><span class="toc-number">25.9.1.</span> <span class="toc-text">背景知识</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%E8%AF%B4%E6%98%8E"><span class="toc-number">25.9.2.</span> <span class="toc-text">实例说明</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E8%BF%99%E4%B8%AA%E7%A4%BA%E4%BE%8B"><span class="toc-number">25.9.3.</span> <span class="toc-text">解释这个示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.compat.v1.global_variables_initializer"><span class="toc-number">25.10.</span> <span class="toc-text">tf.compat.v1.global_variables_initializer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%9C%E7%94%A8"><span class="toc-number">25.10.1.</span> <span class="toc-text">作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%98%E9%87%8F%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-number">25.10.2.</span> <span class="toc-text">变量初始化的重要性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="toc-number">25.11.</span> <span class="toc-text">使用示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E8%BF%99%E4%B8%AA%E7%A4%BA%E4%BE%8B-1"><span class="toc-number">25.12.</span> <span class="toc-text">解释这个示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensorflow-2.x-%E7%9A%84%E5%8F%98%E5%8C%96"><span class="toc-number">25.13.</span> <span class="toc-text">TensorFlow 2.x 的变化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-2"><span class="toc-number">25.14.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.constant_initializer"><span class="toc-number">25.15.</span> <span class="toc-text">tf.constant_initializer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">25.15.1.</span> <span class="toc-text">使用方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B"><span class="toc-number">25.15.2.</span> <span class="toc-text">示例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E8%BF%99%E4%B8%AA%E7%A4%BA%E4%BE%8B-2"><span class="toc-number">25.15.3.</span> <span class="toc-text">解释这个示例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">25.15.4.</span> <span class="toc-text">应用场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tensorflow-2.x-%E7%9A%84%E5%8F%98%E5%8C%96-1"><span class="toc-number">25.15.5.</span> <span class="toc-text">TensorFlow 2.x 的变化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-3"><span class="toc-number">25.15.6.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.layers.dense"><span class="toc-number">25.16.</span> <span class="toc-text">tf.layers.dense</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8A%E9%9D%A230-%E4%B8%AA%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E5%90%AB%E4%B9%89"><span class="toc-number">25.16.1.</span> <span class="toc-text">上面30 个神经元的含义</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4"><span class="toc-number">25.16.1.1.</span> <span class="toc-text">详细步骤</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E"><span class="toc-number">25.16.1.2.</span> <span class="toc-text">举例说明</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.multiply"><span class="toc-number">25.17.</span> <span class="toc-text">tf.multiply ：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.get_collection"><span class="toc-number">25.18.</span> <span class="toc-text">tf.get_collection</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E5%8E%9F%E5%9E%8B"><span class="toc-number">25.18.1.</span> <span class="toc-text">函数原型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0"><span class="toc-number">25.18.2.</span> <span class="toc-text">参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%94%E5%9B%9E%E5%80%BC"><span class="toc-number">25.18.3.</span> <span class="toc-text">返回值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E-1"><span class="toc-number">25.18.4.</span> <span class="toc-text">举例说明</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81"><span class="toc-number">25.18.4.1.</span> <span class="toc-text">示例代码</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E5%AE%9A%E4%B9%89%E9%9B%86%E5%90%88%E9%94%AE"><span class="toc-number">25.18.5.</span> <span class="toc-text">预定义集合键</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">25.18.6.</span> <span class="toc-text">在实际应用中的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%89%80%E6%9C%89%E5%8F%AF%E8%AE%AD%E7%BB%83%E5%8F%98%E9%87%8F"><span class="toc-number">25.18.6.1.</span> <span class="toc-text">获取所有可训练变量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E7%89%B9%E5%AE%9A%E4%BD%9C%E7%94%A8%E5%9F%9F%E5%86%85%E7%9A%84%E5%8F%98%E9%87%8F"><span class="toc-number">25.18.6.2.</span> <span class="toc-text">获取特定作用域内的变量</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-4"><span class="toc-number">25.18.7.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.stop_gradient"><span class="toc-number">25.19.</span> <span class="toc-text">tf.stop_gradient</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E5%8E%9F%E5%9E%8B-1"><span class="toc-number">25.19.1.</span> <span class="toc-text">函数原型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0-1"><span class="toc-number">25.19.2.</span> <span class="toc-text">参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%94%E5%9B%9E%E5%80%BC-1"><span class="toc-number">25.19.3.</span> <span class="toc-text">返回值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B-1"><span class="toc-number">25.19.4.</span> <span class="toc-text">使用示例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B1%E7%AE%80%E5%8D%95%E7%A4%BA%E4%BE%8B"><span class="toc-number">25.19.4.1.</span> <span class="toc-text">示例1：简单示例</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B2%E5%9C%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">25.19.4.2.</span> <span class="toc-text">示例2：在神经网络中的应用</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-5"><span class="toc-number">25.19.5.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AF%E6%9B%BF%E6%8D%A2%E4%BB%A3%E7%A0%81"><span class="toc-number">25.20.</span> <span class="toc-text">软替换代码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A9%E6%88%91%E4%BB%AC%E9%80%90%E6%AD%A5%E8%A7%A3%E6%9E%90%E8%BF%99%E6%AE%B5%E4%BB%A3%E7%A0%81"><span class="toc-number">25.20.1.</span> <span class="toc-text">让我们逐步解析这段代码：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BD%AF%E6%9B%BF%E6%8D%A2%E7%9A%84%E5%85%B7%E4%BD%93%E4%BD%9C%E7%94%A8"><span class="toc-number">25.20.2.</span> <span class="toc-text">软替换的具体作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-1"><span class="toc-number">25.20.3.</span> <span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A1%AC%E6%9B%BF%E6%8D%A2%E4%BB%A3%E7%A0%81"><span class="toc-number">25.21.</span> <span class="toc-text">硬替换代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.gradients"><span class="toc-number">25.22.</span> <span class="toc-text">tf.gradients</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tf.gradients%E7%9A%84%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA%E4%B9%8B%E9%97%B4%E7%9A%84%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">25.22.1.</span> <span class="toc-text">tf.gradients的输入和输出之间的形状的关系</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3-grad_ys"><span class="toc-number">25.22.2.</span> <span class="toc-text">如何理解 grad_ys</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%8A%A0%E6%9D%83"><span class="toc-number">25.22.2.1.</span> <span class="toc-text">为什么需要加权</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%B8%BA-grad_ys-%E4%B8%AD%E7%9A%84%E6%AF%8F%E4%B8%AA%E5%BC%A0%E9%87%8F%E8%AE%BE%E7%BD%AE%E5%90%88%E9%80%82%E7%9A%84%E6%9D%83%E9%87%8D"><span class="toc-number">25.22.2.2.</span> <span class="toc-text">如何为
grad_ys 中的每个张量设置合适的权重？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#np.hstack"><span class="toc-number">25.23.</span> <span class="toc-text">np.hstack</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#td-error"><span class="toc-number">25.24.</span> <span class="toc-text">td error</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.distributions.normal"><span class="toc-number">25.25.</span> <span class="toc-text">tf.distributions.Normal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.clip_by_value"><span class="toc-number">25.26.</span> <span class="toc-text">tf.clip_by_value</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch.nn.module"><span class="toc-number">25.27.</span> <span class="toc-text">torch.nn.Module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#abc.abc"><span class="toc-number">25.28.</span> <span class="toc-text">abc.ABC</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#property"><span class="toc-number">25.29.</span> <span class="toc-text">@property</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#import-gym"><span class="toc-number">25.30.</span> <span class="toc-text">import gym</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch.utils.data.dataloader"><span class="toc-number">25.31.</span> <span class="toc-text">torch.utils.data.DataLoader</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#np.clip"><span class="toc-number">25.32.</span> <span class="toc-text">np.clip</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/28/crop_classification/Crop%20classification/8%E6%9C%8828/" title="8 月 28 总结">8 月 28 总结</a><time datetime="2024-08-28T03:46:06.408Z" title="Created 2024-08-28 11:46:06">2024-08-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/27/crop_classification/rse-papers-for-crop-classification/rse-2024-Boosting-crop-classification-by-hierarchically-fusing-satellite-rotational-and-contextual-data/" title="Boosting crop classification by hierarchically fusing satellite, rotational, and contextual data">Boosting crop classification by hierarchically fusing satellite, rotational, and contextual data</a><time datetime="2024-08-27T06:34:09.562Z" title="Created 2024-08-27 14:34:09">2024-08-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/27/crop_classification/rse-papers-for-crop-classification/Enhancing-crop-classification-in-agriculture-through-dipper-throat-optimization-and-deep-learning/" title="Enhancing food crop classification in agriculture through dipper throat optimization and deep learning with remote sensing">Enhancing food crop classification in agriculture through dipper throat optimization and deep learning with remote sensing</a><time datetime="2024-08-27T03:55:26.097Z" title="Created 2024-08-27 11:55:26">2024-08-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/27/crop_classification/%E7%9B%B8%E5%85%B3%E4%BC%9A%E8%AE%AE/" title="Untitled">Untitled</a><time datetime="2024-08-26T17:11:44.292Z" title="Created 2024-08-27 01:11:44">2024-08-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/27/crop_classification/Crop%20classification/2022-RSE-Early-and-in-season-crop-type-mapping/" title="Early- and in-season crop type mapping without current-year ground truth: Generating labels from historical information via a topology-based approach">Early- and in-season crop type mapping without current-year ground truth: Generating labels from historical information via a topology-based approach</a><time datetime="2024-08-26T16:53:19.306Z" title="Created 2024-08-27 00:53:19">2024-08-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By ALTNT</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>