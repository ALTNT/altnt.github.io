<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习相关概念 | ALTNT's Hexo Blog</title><meta name="author" content="ALTNT"><meta name="copyright" content="ALTNT"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="机器学习分类 根据训练数据是否有标签，可以分为： 监督学习：所有训练数据均具有标签（典型的问题有回归：模型输出的是一个具体数值；分类：模型的输出是某一类别） 在监督学习中，常用的模型种类可以分为：线性模型 和 非线性模型。其中，非线性模型应用更加广泛，表达能力也更强，包括深度学习，支持向量机（SVM），决策树，K-NN算法等。 半监督学习：训练数据中，部分具有标签，另一部分没有标签（但是没有标签的">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习相关概念">
<meta property="og:url" content="http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/index.html">
<meta property="og:site_name" content="ALTNT&#39;s Hexo Blog">
<meta property="og:description" content="机器学习分类 根据训练数据是否有标签，可以分为： 监督学习：所有训练数据均具有标签（典型的问题有回归：模型输出的是一个具体数值；分类：模型的输出是某一类别） 在监督学习中，常用的模型种类可以分为：线性模型 和 非线性模型。其中，非线性模型应用更加广泛，表达能力也更强，包括深度学习，支持向量机（SVM），决策树，K-NN算法等。 半监督学习：训练数据中，部分具有标签，另一部分没有标签（但是没有标签的">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://blog.705553939.xyz/img/altnt.jpeg">
<meta property="article:published_time" content="2024-06-26T09:09:56.000Z">
<meta property="article:modified_time" content="2024-08-08T04:07:22.236Z">
<meta property="article:author" content="ALTNT">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.705553939.xyz/img/altnt.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习相关概念',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-08-08 12:07:22'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/altnt.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="ALTNT's Hexo Blog"><span class="site-name">ALTNT's Hexo Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习相关概念</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-06-26T09:09:56.000Z" title="Created 2024-06-26 17:09:56">2024-06-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-08-08T04:07:22.236Z" title="Updated 2024-08-08 12:07:22">2024-08-08</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习相关概念"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="机器学习分类">机器学习分类</h2>
<p>根据训练数据是否有标签，可以分为：</p>
<p>监督学习：所有训练数据均具有标签（典型的问题有回归：模型输出的是一个具体数值；分类：模型的输出是某一类别）
在监督学习中，常用的模型种类可以分为：线性模型 和
非线性模型。其中，非线性模型应用更加广泛，表达能力也更强，包括深度学习，支持向量机（SVM），决策树，K-NN算法等。</p>
<p>半监督学习：训练数据中，部分具有标签，另一部分没有标签（但是没有标签的数据，对于模型的学习也是有用处的）。
迁移学习：使用与当前任务无关的数据（可能有标签，可能没有标签）来促进当前模型的学习。
无监督学习：训练数据都没有标签。
无监督学习存在的原因是，现实世界中，为训练数据进行标注成本较高，当训练数据都没有标签时，如果我们想要为数据进行分类，只能根据数据的特征进行划分，比如聚类算法。</p>
<p>强化学习：训练数据没有标签，智能体从环境交互中进行学习，来更新自身的策略，根据最终环境的反馈（获得的奖励）来调整自身行为。</p>
<h3 id="回归问题">回归问题</h3>
<pre><code>机器学习笔记的第二篇博客，来介绍机器学习中最基础的回归任务，上一篇博客中有提到回归任务和分类任务的差别在于，回归任务中模型的输出是一个具体的数值， 而分类任务中模型的输出是某一类别。其实，许多问题我们都可以视为回归问题：![alt text](机器学习相关概念/image-4.png)</code></pre>
<p>例如：根据股票市场的历史数据预测明天的股票走势；自动驾驶中根据传感器获取的信息输出方向盘的转动角度；推荐系统中，输入用户和商品的特征，模型输出一个[0,1]之间的数值，表示购买的可能性。</p>
<h3 id="回归模型的建立">回归模型的建立</h3>
<p>机器学习模型建立的三个步骤：</p>
<ol type="1">
<li>我们准备许多备选的函数 <strong><em>f </em></strong>
，构成一个集合，也就是机器学习中的模型（Model）。</li>
<li>使用训练数据来衡量这些备选函数的好坏程度。</li>
<li>根据训练数据选出拟合最好的函数，作为最终的拟合函数。</li>
</ol>
<h4 id="第一步确定用于回归任务的模型">第一步：确定用于回归任务的模型</h4>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-5.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>这里选择一元线性函数 <strong><em>y = wx +
b</em></strong>即线性模型，一元表示我们使用的特征是一个（宝可梦的原CP值）；这样我们就构造了一个函数集合，由于参数<strong><em>w</em></strong>和<strong><em>b</em></strong>的取值是无穷的，所以函数集合中函数的个数是无穷个，接下来在函数集合中选择最好的函数的过程实际上就是为函数确定参数值的过程。</p>
<h4 id="第二步使用训练数据来衡量这些备选函数的好坏程度确定参数">第二步：使用训练数据来衡量这些备选函数的好坏程度（确定参数）</h4>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-6.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>接下来我们需要根据训练数据来从备选函数中选择一个效果最好的函数（即确定函数参数部分），这里需要使用损失函数，损失函数的输入是
“用来进行回归任务的函数” 和 “真实标签” ，输出是
进行回归任务的函数的好坏程度（Loss的值越小，认为该函数的效果越好）。如上图所示，我们使用
平方差之和 作为损失函数。</p>
<h4 id="第三步根据训练数据选出拟合最好的函数作为最终的拟合函数">第三步：根据训练数据选出拟合最好的函数，作为最终的拟合函数</h4>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-7.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>第三步，我们在训练数据上根据损失函数来评估拟合函数的好坏，找到使得损失函数最小的一组参数，作为我们最终的拟合函数。其中，寻找参数时，使用的方法为
<strong><em>梯度下降法</em></strong> 。</p>
<h2 id="梯度下降">梯度下降</h2>
<p>梯度下降算法是机器学习领域最广为人知、用途最广的优化算法，用来确定模型的参数（包括随机梯度下降SGD，Momentum，Adam等）。梯度下降算法的一个简单介绍如下：</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-8.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720165442054.png" alt="1720165442054">
<figcaption aria-hidden="true">1720165442054</figcaption>
</figure>
<p>1、在机器学习中，只要损失函数是可微分（可求导）的，就可以使用梯度下降算法进行参数的求解，那么怎么判断损失函数是否可微？（后面解释）</p>
<p>上面图片展示的是，模型当中只有一个参数（所以直接对该参数求导就可以），如果模型中存在两个及以上的参数，那么就需要分别对每个参数计算偏导数，然后根据参数更新公式进行每个参数的更新，如下图所示:
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-9.png" alt="alt text"></p>
<p>梯度下降算法的原理已经清楚，其实就是沿着损失函数降低的方向更新模型的参数，但是如果损失函数很复杂，比如下面图片所示，
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-10.png" alt="alt text"></p>
<p>我们很可能在更新参数的过程中，走到导数为0的点（第四个红点位置），这时因为不知道更新的方向，就陷入了局部最优点（其实真正的全局最优点还在右边）。</p>
<p>2、不过对于上面回归问题中，损失函数为平方差之和，该损失函数为凸函数，没有局部最优点，只有全局最优点。那么如何判断一个函数是否为凸函数？（后面解释）</p>
<h3 id="预测结果分析">预测结果分析</h3>
<p>我们上面的线性模型，经过梯度下降算法，寻得一组最优参数，其结果表现如下：
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-11.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-12.png" alt="alt text"></p>
<p>训练数据上面的损失函数值为31.9，测试数据上面损失函数值为35。在实际问题中，我们更加关注的是模型在测试集上面的性能表现，也就是模型的
<strong><em>泛化能力</em></strong>
，线性模型在测试集上面的误差较大，所以如果我们重新设计预测模型，使用更加复杂的模型，会不会得到更好的效果？
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-13.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-14.png" alt="alt text"></p>
<p>随着使用更加复杂的二次模型，三次模型，无论是在训练集还是测试集上面，效果都有提升。可是当继续增加模型的复杂度，使用四次模型的时候，虽然在训练集上面的<strong><em>loss</em></strong>更小，但是测试集上面的效果却变糟了。使用五次模型的时候，这一趋势更加明显：
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-15.png" alt="alt text">
虽然复杂的模型对于训练数据的拟合程度会更好，但是很容易出现过拟合的现象（过于严格的去拟合训练数据，当面对新数据的时候没有办法做出准确的预测，即无法泛化到其他数据）。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-16.png" alt="alt text">
在机器学习模型训练过程中，我们要尽量避免过拟合的现象，一方面要选择合适的模型，模型不是越复杂越好，可以通过交叉验证来选择合适的模型；另一方面，<strong>可以通过一些技术手段来帮助我们避免过拟合，比如正则化，early
stopping等等。</strong> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-17.png" alt="alt text">
如上图所示，<strong>蓝色线代表训练集上的损失函数，红色线代表验证集的损失函数</strong>，当训练进行到中间垂直的线段时，模型应该是最优的；如果继续训练，就会造成过拟合现象。</p>
<h3 id="正则化">正则化</h3>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-18.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>简单来说，正则化是一种为了减小<strong>测试误差</strong>的行为(有时候会增加训练误差)。我们在构造机器学习模型时，最终目的是让模型在面对新数据的时候，可以有很好的表现。当使用比较复杂的模型比如神经网络，去拟合数据时，很容易出现过拟合现象(训练集表现很好，测试集表现较差)，这会导致模型的泛化能力下降，这时候，就需要使用正则化，降低模型的复杂度。</p>
<p><strong>具体而言，正则化就是在损失函数后面增加一项惩罚项（对某些参数进行限制），使得我们的模型更加平滑。以上图为例，我们在损失函数后面增加一项关于参数w的正则项，限制参数w不要过大，这样模型会有更好的泛化能力。因为，这样对于测试数据中存在的噪声，会不那么敏感，即噪声对于预测的结果影响会降低。</strong>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720515070068.png" alt="1720515070068"></p>
<p>加入正则化技术之后，训练集和测试集上面的误差如下： <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-19.png" alt="alt text"></p>
<p>和我们的预期分析是一致的，加入正则化之后，参数<embed src="https://private.codecogs.com/gif.latex?%5Clambda"></p>
<p>越大，表示我们越关注模型的平滑程度（也就是模型的泛化能力），相对于训练误差考虑较少，所以训练集上面的loss增大，测试集上面的loss降低。但是，参数<embed src="https://private.codecogs.com/gif.latex?%5Clambda"></p>
<p>不是越大越好，我们希望得到一个比较平滑的函数，但是不能过于平滑（会丧失其预测能力）。</p>
<p><strong>机器学习中的正则化概念</strong></p>
<p>在机器学习中，正则化（Regularization）是一种用于防止过拟合（Overfitting）的技术。</p>
<p>过拟合是指模型在训练数据上表现得非常好，但在新的、未见过的数据上表现不佳。这通常是因为模型过于复杂，学习到了训练数据中的噪声和特定的细节，而不是一般性的模式。</p>
<p>正则化通过在损失函数中添加一个惩罚项来限制模型的复杂度。常见的正则化方法有
L1 正则化和 L2 正则化。</p>
<p><strong>L1 正则化</strong>：也称为 Lasso
正则化，它在损失函数中添加的惩罚项是模型参数的绝对值之和。L1
正则化具有特征选择的效果，因为它可能会将一些不重要的特征对应的参数压缩至零。</p>
<p>例如，在线性回归中，假设模型的预测函数为
<code>y = w1 * x1 + w2 * x2 +... + wn * xn + b</code> ，L1 正则化项就是
<code>λ * |w1| + λ * |w2| +... + λ * |wn|</code> ，其中 <code>λ</code>
是正则化参数，控制正则化的强度。</p>
<p><strong>L2 正则化</strong>：也称为 Ridge
正则化，它在损失函数中添加的惩罚项是模型参数的平方和。L2
正则化会使模型的参数值趋向于较小的值，但不太会将参数压缩至零。</p>
<p>同样在线性回归中，L2 正则化项就是
<code>λ * (w1^2 + w2^2 +... + wn^2)</code> 。</p>
<p><strong>正则化的作用</strong>：</p>
<ol type="1">
<li>控制模型复杂度：通过限制模型的参数大小，防止模型过于复杂。</li>
<li>提高模型泛化能力：使模型能够更好地应对新的数据，减少过拟合的风险。</li>
</ol>
<p><strong>举例说明</strong>：
假设我们正在训练一个神经网络来识别图像中的猫和狗。如果没有正则化，模型可能会过度学习训练数据中的细微特征，比如图片中的背景颜色或微小的噪声，导致在新的图像上识别准确率下降。</p>
<p>当我们应用 L2
正则化时，模型的参数会受到一定的约束，不会变得过大。这可能会导致模型在训练数据上的准确率稍微降低，但在测试数据上的表现会更好，因为它学习到了更通用的特征，而不是过度依赖于特定的训练样本。</p>
<p>总之，正则化是机器学习中非常重要的技术，有助于提高模型的性能和稳定性。</p>
<h3 id="遗留问题">遗留问题</h3>
<p>1、在机器学习中，只要损失函数是可微的，就可以使用梯度下降算法进行参数的求解，那么怎么判断损失函数是否可微？（后面解释）</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720166051610.png" alt="1720166051610">
<figcaption aria-hidden="true">1720166051610</figcaption>
</figure>
<p>2、不过对于上面回归问题中，损失函数为平方差之和，该损失函数为凸函数，没有局部最优点，只有全局最优点。那么如何判断一个函数是否为凸函数？（后面解释）</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720166067204.png" alt="1720166067204">
<figcaption aria-hidden="true">1720166067204</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-20.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>判断一个矩阵是不是半正定矩阵，方法之一是判断该矩阵的所有主子式是不是非负；对于上面的海森矩阵，其所有主子式为：</p>
<p><embed src="https://private.codecogs.com/gif.latex?2x%5E%7B2%7D">，2,
0，均为非负，所以该海森矩阵为半正定矩阵。所以该损失函数为凸函数，只有全局最优值。</p>
<p><strong>半正定矩阵</strong></p>
<p>半正定矩阵是矩阵理论中的一个重要概念。</p>
<p>一个实对称矩阵 <code>A</code>
被称为半正定矩阵，如果对于任意的非零实向量 <code>x</code> ，都有
<code>x^T Ax ≥ 0</code> 。</p>
<p><strong>性质</strong> ：</p>
<ol type="1">
<li>半正定矩阵的所有特征值都是非负的。</li>
<li>半正定矩阵的主子式都非负。</li>
<li>半正定矩阵与另一个半正定矩阵的和仍是半正定矩阵。</li>
</ol>
<p>在实际的机器学习和优化问题中，Hessian 矩阵具有重要的地位。</p>
<p><strong>计算方面</strong>： 计算 Hessian
矩阵可能是计算密集型的，特别是对于具有大量参数的模型。在深度学习中，直接计算完整的
Hessian
矩阵通常是不现实的。然而，可以使用近似方法或针对特定结构的模型进行简化计算。例如，对于一些具有简单结构的神经网络，可能通过一些技巧来估计
Hessian 矩阵的部分元素。</p>
<p><strong>应用方面</strong>：</p>
<ol type="1">
<li>优化算法：如牛顿法及其变体，利用 Hessian
矩阵来确定搜索方向和步长。相比于梯度下降法只依赖一阶导数（梯度），牛顿法考虑了二阶导数信息，能够在一些情况下更快地收敛到最优解。
<ul>
<li>举例来说，在求解一个二次函数的最小值时，牛顿法通过一次迭代就可以直接到达最小值点，因为它准确地利用了
Hessian 矩阵的信息。</li>
</ul></li>
<li>模型分析：帮助理解模型的性质和行为。通过分析 Hessian
矩阵的特征值和特征向量，可以了解模型在不同方向上的敏感度和曲率，从而洞察模型的稳定性和鲁棒性。
<ul>
<li>例如，在图像分类任务中，如果 Hessian
矩阵的某些特征值很大，说明模型在对应的特征方向上变化剧烈，可能对输入的微小变化非常敏感。</li>
</ul></li>
<li>正则化：可以用于设计一些基于二阶信息的正则化方法，以防止过拟合。
<ul>
<li>比如，通过对 Hessian
矩阵进行某种变换或约束，使得模型的复杂度得到控制。</li>
</ul></li>
</ol>
<p>总之，尽管在实际中直接处理 Hessian
矩阵存在困难，但通过巧妙的近似和应用，它仍然为解决机器学习和优化问题提供了有价值的见解和工具。</p>
<h3 id="梯度下降算法">梯度下降算法</h3>
<p>梯度下降算法是机器学习领域最广为人知、用途最广的优化算法，用来确定模型的参数（包括随机梯度下降SGD，Momentum，Adam等）。首先回顾一下梯度下降的计算过程：</p>
<h4 id="梯度下降中常用技巧tips">梯度下降中常用技巧（Tips）</h4>
<h5 id="一调整学习率">一、调整学习率</h5>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-28.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720543595377.png" alt="1720543595377">
<figcaption aria-hidden="true">1720543595377</figcaption>
</figure>
<p>所以，我们可以根据Loss曲线的变化情况，对我们的学习率进行一个合理的调整。可以绘制上图右边部分所示的曲线图，横轴代表参数更新次数，纵轴代表Loss值，学习率的大小分为四种情况：</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720543613512.png" alt="1720543613512">
<figcaption aria-hidden="true">1720543613512</figcaption>
</figure>
<p>在学习率的设置过程中，常见的做法是，进行
<strong><em>学习率的衰减</em></strong> 。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-29.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>在模型训练初期，距离全局最优点较远，可以设置一个相对大一些的学习率，随着训练的进行，距离全局最优点的距离越来越小，此时应该减小学习率；所以让学习率随着时间或者更新的次数进行衰减。</p>
<p>除了学习率的衰减之外，另外一个做法是：为不同的参数设置不同的学习率。也就是接下来要介绍的一种优化算法AdaGrad。</p>
<h6 id="adagrad优化器">AdaGrad优化器</h6>
<p>AdaGrad（Adaptive Gradient
Algorithm）是一种自适应学习率的方法，用于优化机器学习模型，特别是在处理稀疏数据和高维数据时表现出色。AdaGrad的主要特点是它为每个参数维护一个单独的学习率，并根据之前的梯度信息调整这些学习率。</p>
<p>AdaGrad根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题,即为每个参数设置不同的学习率。具体来说，对于每个参数
<strong>θ</strong>i，AdaGrad计算其所有历史梯度的平方和，并用这个和来缩放当前的梯度，从而得到更新步长。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-30.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720543759855.png" alt="1720543759855">
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-31.png" alt="alt text"></p>
<p>这里需要注意的一点为： <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-32.png" alt="alt text"></p>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-33.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-34.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-35.png" alt="alt text">
而在AdaGrad中，正是这一思想的体现，不过为了减少计算量，增加运算速度，AdaGrad中使用过去一阶偏导数的均方根作为分母项（二阶偏导数）的近似。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-36.png" alt="alt text">
过去一阶偏导数的均方根可以在一定程度上反应二阶偏导的大小情况。如上图所示，二阶偏导小的函数，其采样的一阶偏导的值也相对较小。</p>
<p><strong>优势</strong></p>
<ol type="1">
<li><strong>自适应学习率</strong>
：不同参数有不同的学习率，能够更好地处理稀疏数据，使得更新幅度较大的参数的学习率更小，而更新幅度较小的参数的学习率更大。</li>
<li><strong>简化调参</strong>
：由于学习率是自适应的，通常不需要频繁调整学习率超参数。</li>
</ol>
<p><strong>局限性</strong></p>
<ol type="1">
<li><strong>学习率过小</strong>
：随着时间的推移，累积梯度平方和不断增大，导致学习率逐渐缩小，可能会导致算法在后期学习变得非常缓慢。</li>
<li><strong>不适用于所有问题</strong>
：虽然AdaGrad在处理稀疏数据上有优势，但对于某些问题，其性能可能不如其他优化算法，如RMSprop或Adam。</li>
</ol>
<p><strong>典型应用</strong></p>
<p>AdaGrad常用于处理自然语言处理中的词嵌入、推荐系统中的用户行为数据等高维、稀疏数据场景。</p>
<p>总结来说，AdaGrad通过自适应地调整每个参数的学习率，在处理稀疏数据和高维数据时提供了显著的优势，但其逐渐减小的学习率可能在某些情况下限制其性能。</p>
<h5 id="二随机梯度下降sgd">二、随机梯度下降（SGD）</h5>
<p>随机梯度下降（SGD, Stochastic Gradient
Descent）是一种用于优化机器学习模型参数的算法，特别适用于大规模数据集的训练。SGD与传统的批量梯度下降（Batch
Gradient
Descent）不同，它在每次迭代中仅使用一个样本或一个小批量的样本（mini-batch）来计算梯度和更新参数。这种方法具有较快的更新速度和更好的内存效率，特别是在处理大数据集时。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720599100421.png" alt="1720599100421">
<figcaption aria-hidden="true">1720599100421</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-37.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-38.png" alt="alt text">
之前梯度下降算法中，Loss函数是对所有训练样本的Loss之和，而在随机梯度下降中，每次只采样一个样本，根据这一个样本进行一次梯度下降，所以随机梯度下降算法更新参数的过程更快。</p>
<p>随机梯度下降（SGD, Stochastic Gradient
Descent）是一种用于优化机器学习模型参数的算法，特别适用于大规模数据集的训练。SGD与传统的批量梯度下降（Batch
Gradient
Descent）不同，它在每次迭代中仅使用一个样本或一个小批量的样本（mini-batch）来计算梯度和更新参数。这种方法具有较快的更新速度和更好的内存效率，特别是在处理大数据集时。</p>
<p><strong>优势</strong></p>
<ol type="1">
<li><strong>快速收敛</strong>：由于每次迭代只使用一个样本或小批量样本，更新频繁，收敛速度快。</li>
<li><strong>内存效率</strong>：每次只需要加载一个样本或小批量样本，内存占用低，适合处理大规模数据集。</li>
<li><strong>逃离局部最优</strong>：由于引入了随机性，SGD有助于跳出局部最优解，更容易找到全局最优解。</li>
</ol>
<p><strong>局限性</strong></p>
<ol type="1">
<li><strong>收敛波动</strong>：由于每次更新基于单个样本或小批量样本，导致参数更新不稳定，损失函数可能会剧烈波动。</li>
<li><strong>调参困难</strong>：学习率的选择对SGD的性能影响很大，通常需要进行超参数调优。</li>
</ol>
<p><strong>改进方法</strong></p>
<p>为了解决SGD的一些局限性，提出了多种改进算法，如：</p>
<ol type="1">
<li><strong>Mini-batch
SGD</strong>：在每次迭代中使用一个小批量样本，而不是单个样本，平衡了收敛速度和稳定性。</li>
<li><strong>动量（Momentum）</strong>：在参数更新时引入动量项，利用之前梯度的指数加权平均来加速收敛。</li>
<li><strong>RMSprop</strong>：自适应调整学习率，缓解学习率逐渐减小的问题。</li>
<li><strong>Adam</strong>：结合了动量和RMSprop的优点，自适应地调整学习率。</li>
</ol>
<p><strong>应用场景</strong></p>
<p>SGD广泛应用于深度学习和机器学习的各种模型训练中，包括神经网络、线性回归、逻辑回归等。它特别适用于大规模数据集和在线学习场景。</p>
<p>总结来说，SGD通过随机选择样本来进行参数更新，提供了快速且内存高效的优化方法，但其波动性和学习率调优是需要注意的问题。改进的变种算法如Mini-batch
SGD、动量、RMSprop和Adam在实践中被广泛采用，以提高SGD的性能和稳定性。</p>
<h5 id="三特征缩放feature-scaling">三、特征缩放（Feature Scaling）</h5>
<p>特征缩放是用来标准化数据特征的范围，减少特征中特异值的影响。</p>
<p>例如： <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-39.png" alt="alt text"></p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720544009279.png" alt="1720544009279">
<figcaption aria-hidden="true">1720544009279</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720544026157.png" alt="1720544026157">
<figcaption aria-hidden="true">1720544026157</figcaption>
</figure>
<h2 id="梯度上升和梯度下降的区别是什么">梯度上升和梯度下降的区别是什么？</h2>
<p><strong>!!!!!!!
简单来说就是正常的机器学习算法是需要计算loss函数的梯度,往梯度下降的方向走,需要loss函数的最小值</strong></p>
<pre><code>**但是梯度上升时,这个函数不是loss函数,而是一个真正的func函数,需要找这个func函数的最大值**</code></pre>
<p>梯度上升和梯度下降是两种相反的优化方法，主要区别如下：</p>
<p><strong>目标方向</strong>：</p>
<p>梯度下降的目标是找到函数的最小值，因此沿着函数梯度的负方向更新参数。</p>
<p>梯度上升的目标是找到函数的最大值，所以沿着函数梯度的正方向更新参数。</p>
<p><strong>更新参数的方式</strong>：</p>
<p>假设函数 <code>f</code> 关于参数 <code>w</code> 的梯度为
<code>∇f(w)</code> ，学习率为 <code>α</code> 。</p>
<p>在梯度下降中，参数的更新公式为：<code>w = w - α * ∇f(w)</code> 。</p>
<p>在梯度上升中，参数的更新公式为：<code>w = w + α * ∇f(w)</code> 。</p>
<p><strong>应用场景</strong>：</p>
<p>梯度下降常用于损失函数的最小化，比如在机器学习中，通过最小化预测值与真实值之间的差异来优化模型的参数。</p>
<p>梯度上升则常用于需要最大化某个目标函数的情况，比如强化学习中最大化奖励，或者在某些特定的优化问题中找到使某个函数达到最大值的参数配置。</p>
<p><strong>示例</strong>：</p>
<p>假设有一个简单的二次函数 <code>f(w) = w^2</code> ，其梯度为
<code>∇f(w) = 2w</code> 。</p>
<p>如果使用梯度下降，学习率为 <code>0.1</code> ，初始参数
<code>w = 2</code> ，则第一次更新为
<code>w = 2 - 0.1 * 2 * 2 = 1.6</code> 。</p>
<p>如果使用梯度上升，同样学习率为 <code>0.1</code> ，初始参数
<code>w = 2</code> ，则第一次更新为
<code>w = 2 + 0.1 * 2 * 2 = 2.4</code> 。</p>
<p>总的来说，梯度上升和梯度下降的核心区别在于更新参数的方向，一个朝着梯度正方向，一个朝着梯度负方向，以分别实现最大化和最小化的目标。</p>
<h2 id="偏差和方差">偏差和方差</h2>
<p>这篇博客介绍机器学习中误差（error）的来源，知道我们的模型中产生的误差来自于哪一部分，才能更好地进行模型的调整。一般来说，误差的来源有两部分：偏差（bias）和方差（variance）。偏差和方差——用来衡量模型泛化能力的工具，所以我的理解是在测试集上面根据偏差和方差来对模型进行一个评估。</p>
<p>回顾之前回归问题中的例子，简单模型对于数据的拟合能力比较差，在训练集和测试集上面效果均不好；但同时不是越复杂的模型越好，因为有可能产生过拟合的现象，所以需要选择合适的模型。偏差-方差分析可以帮我们诊断模型中存在的问题（过于复杂或者过于简单）。</p>
<p><strong>偏差就是：预测输出的期望值 -
真实值，（描述模型的拟合能力）</strong></p>
<p><strong>方差就是：（每个模型实例的预测输出 - 模型预测输出的期望值）^
2 （描述模型的稳定性，即受数据扰动的影响程度）</strong></p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-21.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>还是以宝可梦进化之后的CP值预测为例，如果我们有一些不同的训练数据（也就是李宏毅老师PPT中所说从若干个平行世界中收集的不同的宝可梦），</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-22.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<p>实质上是指有几个不同的训练集（TrainData_1，TrainData_2，TrainData_3），模型分别在不同的训练集上面训练，然后在同样的测试集（TestData）上面测试。对于不同的训练集，我们会得到一个模型的实例，比如有一次模型和五次模型，训练结果：(这里，“模型”表示具体的模型类别（比如一次模型，二次模型）；“模型实例”表示一个模型在不同训练集上面训练得到的最终模型，有几个训练集就会有几个模型实例。)
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-23.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-24.png" alt="alt text"></p>
<p>图片中，红色线表示在100个不同的训练集上面得到的模型的实例，蓝色线表示模型的预测输出的期望，黑色线是真实值。</p>
<p>可以看出，一次模型的偏差较大，方差较小；五次模型的偏差较小，方差较大。</p>
<h3 id="数学定义">数学定义</h3>
<p>上面是偏差和方差一个比较直观的理解，接下来给出数学形式的定义：</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720518028206.png" alt="1720518028206">
<figcaption aria-hidden="true">1720518028206</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720518369317.png" alt="1720518369317">
<figcaption aria-hidden="true">1720518369317</figcaption>
</figure>
<h3 id="偏差-方差与模型复杂度">偏差-方差与模型复杂度</h3>
<p>偏差和方差的几种情况： <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-25.png" alt="alt text"></p>
<p>偏差小，方差小：追求的目标，理想的模型。
偏差小，方差大：模型比较复杂，在训练集上面过拟合，导致在测试集上面泛化效果不好。
偏差大，方差小：模型比较简单，拟合能力较差，在训练集上面欠拟合，导致在测试集上面泛化效果不好。
偏差大，方差大：最糟的情况，模型需要重新进行设计，不适合于现有数据集。</p>
<p>很直观的一个解释，因为偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力，所以偏差大的模型拟合能力差，模型简单，容易欠拟合；方差度量数据扰动所造成的影响（在不同的训练集上面训练得到的模型在测试集上面效果表现相差很大），说明模型过于拟合训练集，模型复杂。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-26.png" alt="alt text"></p>
<h3 id="调整方法">调整方法</h3>
<p>综上，根据模型在测试集上的表现，可以得出结论：</p>
<blockquote>
<p>偏差大，方差小：模型欠拟合</p>
<p>偏差小，方差大：模型过拟合</p>
</blockquote>
<p>对于偏差大（欠拟合）的情况，常用的解决方法：</p>
<p>• 重新设计模型，使用更复杂的模型结构</p>
<p>• 输入中使用更多的特征</p>
<p>对于方差大（过拟合）的情况，常用的解决方法：</p>
<p>• 参数正则化（减小模型的复杂程度）</p>
<p>• 使用更多的训练数据</p>
<h3 id="k-折交叉验证">K-折交叉验证</h3>
<p>模型的设计选择需要在偏差和方差之间进行平衡，在选择合适的模型（比如一次模型还是二次模型）时，常用的方法是进行K-折交叉验证。将训练集的数据等分成K份，每次使用（K-1）份数据进行训练，余下的1份数据进行验证，进行K次，保证每份数据均做过验证集，统计K次验证集上面的loss，取loss均值最小的模型作为使用的模型。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-27.png" alt="alt text">
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<h2 id="softmax">softmax</h2>
<h3 id="主要用途">主要用途:</h3>
<p><strong>softmax是深度学习任务中常用于计算最终输出类别的函数</strong>。</p>
<p>Softmax
函数主要用于多分类问题中，将多个神经元的输出值转换为概率分布。</p>
<p>Softmax
函数在很多机器学习任务中都有广泛的应用，比如图像分类、文本分类等，它有助于将模型的输出转化为可解释的类别概率。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719916089305.png" alt="1719916089305">
<figcaption aria-hidden="true">1719916089305</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719916111218.png" alt="1719916111218">
<figcaption aria-hidden="true">1719916111218</figcaption>
</figure>
<ul>
<li>softmax和我们普通意义上的max函数不同，每一个元素都有一个概率，而不是其中一个元素为1，其余为0。它的含义是对于输入向量，有多大的概率去选择元素1、元素2、元素3等，主要的目的是使得概率计算过程可导。</li>
</ul>
<p>以下是对 Softmax 函数的一些关键理解点：</p>
<ol type="1">
<li>输出值在 0 到 1 之间：Softmax 函数确保每个输出值都在 0 和 1
之间。</li>
<li>输出值总和为 1：所有输出值的总和为
1，这使得它们可以被解释为概率。</li>
<li>强调相对大小：Softmax 函数会放大输入值之间的差异。较大的输入值在经过
Softmax
计算后会得到更大的概率值，较小的输入值则会得到较小的概率值。</li>
</ol>
<p>例如，假设有一个神经网络的输出为 <code>[1, 2, 0]</code>，经过 Softmax
函数计算后，得到的概率分布可能是
<code>[0.269, 0.731, 0.0]</code>。这意味着模型预测第二个类别是最有可能的，第一个类别有一定的可能性，而第三个类别几乎不可能。</p>
<h2 id="池化层">池化层</h2>
<p>池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合</p>
<p>简而言之，如果输入是图像的话，那么池化层的最主要作用就是压缩图像</p>
<h3 id="池化层的作用">池化层的作用</h3>
<p>主要是两个作用：</p>
<ol type="1">
<li>invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)</li>
<li>保留主要的特征同时减少参数(降维，效果类似PCA)和计算量，防止过拟合，提高模型泛化能力</li>
</ol>
<p>A:
特征不变性，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。</p>
<p>B.
特征降维，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来，这也是池化操作的一大作用</p>
<ol type="1">
<li><p>translation invariance：
这里举一个直观的例子(数字识别)，假设有一个16x16的图片，里面有个数字1，我们需要识别出来，这个数字1可能写的偏左一点(图1)，这个数字1可能偏右一点(图2)，图1到图2相当于向右平移了一个单位，但是图1和图2经过max
pooling之后它们都变成了相同的8x8特征矩阵，主要的特征我们捕获到了，同时又将问题的规模从16x16降到了8x8，而且具有平移不变性的特点。图中的a（或b）表示，在原始图片中的这些a（或b）位置，最终都会映射到相同的位置。</p></li>
<li><p>rotation invariance：
下图表示汉字“一”的识别，第一张相对于x轴有倾斜角，第二张是平行于x轴，两张图片相当于做了旋转，经过多次max
pooling后具有相同的特征 ————————————————</p>
<p>版权声明：本文为博主原创文章，遵循 CC 4.0 BY-SA
版权协议，转载请附上原文出处链接和本声明。</p></li>
</ol>
<p>原文链接：https://blog.csdn.net/weixin_38145317/article/details/89310404</p>
<p><strong>池化层用的方法有Max pooling 和 average pooling</strong></p>
<h3 id="max-pooling">Max pooling:</h3>
<p><img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-40.png" alt="alt text"> <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-41.png" alt="alt text"></p>
<p>对于每个2<em>2的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个2</em>2窗口中最大的数是6，那么输出矩阵的第一个元素就是6，如此类推。</p>
<h2 id="损失函数">损失函数:</h2>
<p>在机器学习中，损失函数（Loss
Function）是用于衡量模型预测结果与真实结果之间差异的函数。</p>
<p>损失函数的主要作用是评估模型在给定数据上的性能表现。它为模型的优化提供了一个明确的目标，通过最小化损失函数的值，来调整模型的参数，使得模型的预测结果越来越接近真实结果。</p>
<p>不同的机器学习任务通常会使用不同的损失函数。</p>
<h3 id="常见的损失函数">常见的损失函数:</h3>
<ol type="1">
<li><p>回归问题：</p>
<ul>
<li>均方误差（Mean Squared
Error，MSE）：计算预测值与真实值之差的平方的平均值，即 <span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n}(y_i -
\hat{y_i})^2\)</span>，其中 <span class="math inline">\(y_i\)</span>
是真实值，<span class="math inline">\(\hat{y_i}\)</span> 是预测值，<span class="math inline">\(n\)</span> 是样本数量。</li>
<li>平均绝对误差（Mean Absolute
Error，MAE）：计算预测值与真实值之差的绝对值的平均值，即 <span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n}|y_i -
\hat{y_i}|\)</span> 。</li>
</ul></li>
<li><p>分类问题：</p>
<ul>
<li>二分类交叉熵损失（Binary Cross Entropy
Loss）：常用于二分类问题，如逻辑回归。对于单个样本，其公式为 <span class="math inline">\(- [y \log(\hat{y}) + (1 - y) \log(1 -
\hat{y})]\)</span>，其中 <span class="math inline">\(y\)</span>
是真实标签（0 或 1），<span class="math inline">\(\hat{y}\)</span>
是预测的概率。</li>
<li>多分类交叉熵损失（Categorical Cross Entropy
Loss）：用于多分类问题，公式为 <span class="math inline">\(-\sum_{i=1}^{C} y_i \log(\hat{y_i})\)</span>，其中
<span class="math inline">\(C\)</span> 是类别数量，<span class="math inline">\(y_i\)</span> 是第 <span class="math inline">\(i\)</span> 类的真实标签（如果样本属于该类为
1，否则为 0），<span class="math inline">\(\hat{y_i}\)</span>
是模型预测样本属于第 <span class="math inline">\(i\)</span>
类的概率。</li>
</ul></li>
</ol>
<p>选择合适的损失函数对于模型的训练和性能至关重要。它会影响模型的学习速度、收敛性以及最终的泛化能力。</p>
<p>以下为您介绍几种常见的损失函数：</p>
<ol type="1">
<li><p><strong>均方误差（Mean Squared Error，MSE）</strong>：</p>
<ul>
<li>公式：<span class="math inline">\(MSE = \frac{1}{n}
\sum_{i=1}^{n}(y_i - \hat{y_i})^2\)</span></li>
<li>其中，<span class="math inline">\(y_i\)</span> 是真实值，<span class="math inline">\(\hat{y_i}\)</span> 是预测值，<span class="math inline">\(n\)</span> 是样本数量。</li>
<li>常用于回归问题，对较大的误差给予更高的惩罚。</li>
<li>例如，预测房价时，如果预测值与真实房价相差较大，MSE 会较大。</li>
</ul></li>
<li><p><strong>平均绝对误差（Mean Absolute Error，MAE）</strong>：</p>
<ul>
<li>公式：<span class="math inline">\(MAE = \frac{1}{n}
\sum_{i=1}^{n}|y_i - \hat{y_i}|\)</span></li>
<li>同样常用于回归问题，相比 MSE 对异常值更鲁棒。</li>
<li>比如，在预测股票价格时，MAE
可能更能承受个别极端价格波动的影响。</li>
</ul></li>
<li><p><strong>交叉熵损失（Cross Entropy Loss）</strong>：</p>
<ul>
<li>二分类交叉熵：<span class="math inline">\(L = - [y \log(\hat{y}) +
(1 - y) \log(1 - \hat{y})]\)</span> ，其中 <span class="math inline">\(y\)</span> 是真实标签（0 或 1），<span class="math inline">\(\hat{y}\)</span> 是预测的概率。</li>
<li>多分类交叉熵：<span class="math inline">\(L = -\sum_{i=1}^{C} y_i
\log(\hat{y_i})\)</span> ，其中 <span class="math inline">\(C\)</span>
是类别数量，<span class="math inline">\(y_i\)</span> 是第 <span class="math inline">\(i\)</span> 类的真实标签（如果样本属于该类为
1，否则为 0），<span class="math inline">\(\hat{y_i}\)</span>
是模型预测样本属于第 <span class="math inline">\(i\)</span>
类的概率。</li>
<li>广泛应用于分类问题，衡量预测概率分布与真实分布之间的差异。</li>
</ul>
<p>交叉熵损失函数（Cross-Entropy
Loss）主要用于衡量模型预测的概率分布与真实的概率分布之间的差异，它在机器学习，特别是深度学习的分类问题中被广泛使用。</p>
<p>其作用主要体现在以下方面：</p>
<ul>
<li><strong>评估模型性能</strong>：通过计算预测分布和真实分布之间的差距，来确定模型在分类任务中的表现优劣。交叉熵越小，表示两个概率分布越接近，说明模型的预测结果越接近真实标签，模型的性能也就越好。</li>
<li><strong>指导模型优化</strong>：在模型训练过程中，交叉熵损失函数的值被用于反向传播，以调整模型的参数，使得损失不断减小，从而使模型的预测逐渐逼近真实标签。</li>
<li><strong>处理多分类问题</strong>：适用于多类别分类任务，可以方便地处理具有多个类别的情况。对于每个样本，模型输出每个类别的预测概率，而交叉熵损失函数会综合考虑所有类别的预测概率和真实标签来计算损失。</li>
</ul>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720089363560.png" alt="1720089363560">
<figcaption aria-hidden="true">1720089363560</figcaption>
</figure>
<p>例如，在一个图像分类任务中，模型需要判断图片属于多个类别中的哪一个。模型的输出是每个类别的预测概率，而真实标签则是图片实际所属的类别。通过计算交叉熵损失，可以衡量模型的预测结果与真实类别之间的差异，并利用这个差异来调整模型的参数，以提高模型的分类准确性。</p>
<p>相比其他一些损失函数，交叉熵损失函数具有一些优点，例如易于理解和计算，对噪声数据具有一定的鲁棒性等。然而，它也存在一些缺点，比如对类别不平衡的数据可能较为敏感，在类别不平衡的数据集上，可能会过于关注多数类别而导致模型性能下降；并且它对输出概率分布的平滑性要求较高，如果输出概率分布过于离散，可能会导致损失值较大。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720089568706.png" alt="1720089568706">
<figcaption aria-hidden="true">1720089568706</figcaption>
</figure></li>
<li><p><strong>Hinge 损失</strong>：</p>
<ul>
<li>常用于支持向量机（SVM）中，特别是在二分类问题中。</li>
<li>对于二分类，公式为：<span class="math inline">\(L = \max(0, 1 - y
\cdot \hat{y})\)</span> ，其中 <span class="math inline">\(y\)</span>
是真实标签（1 或 -1），<span class="math inline">\(\hat{y}\)</span>
是预测值。</li>
</ul></li>
<li><p><strong>KL 散度（Kullback-Leibler Divergence）</strong>：</p>
<ul>
<li>用于衡量两个概率分布之间的差异。</li>
<li>公式：<span class="math inline">\(KL(P || Q) = \sum_{x} P(x) \log
\frac{P(x)}{Q(x)}\)</span></li>
</ul></li>
</ol>
<p>这些损失函数在不同的机器学习任务和场景中各有优缺点，选择合适的损失函数对于模型的性能和训练效果至关重要。</p>
<h3 id="损失函数的导数计算">损失函数的导数计算</h3>
<p>以下是几种常见损失函数及其导数的计算方法：</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720880004976.png" alt="1720880004976">
<figcaption aria-hidden="true">1720880004976</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720880028751.png" alt="1720880028751">
<figcaption aria-hidden="true">1720880028751</figcaption>
</figure>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720880055767.png" alt="1720880055767">
<figcaption aria-hidden="true">1720880055767</figcaption>
</figure>
<h2 id="激活函数">激活函数:</h2>
<p>在神经网络中，激活函数是一种对神经元的输入进行非线性变换的函数。</p>
<p>激活函数的主要作用包括：</p>
<ol type="1">
<li>引入非线性：如果没有激活函数，神经网络仅仅是对输入进行线性组合，其表达能力非常有限，无法处理复杂的非线性问题。通过引入非线性的激活函数，可以使神经网络能够拟合各种复杂的函数和模式。</li>
<li>控制神经元的输出范围：不同的激活函数会将输入映射到不同的输出范围，例如
<code>Sigmoid</code> 函数将输出限制在 <code>(0, 1)</code>
之间，<code>Tanh</code> 函数将输出限制在 <code>(-1, 1)</code>
之间。</li>
<li>增加网络的稀疏性：某些激活函数，如 <code>ReLU</code> （Rectified
Linear Unit），当输入为负数时输出为
0，这有助于在网络中引入稀疏性，减少计算量，并可能有助于防止过拟合。</li>
</ol>
<p>常见的激活函数有：</p>
<ol type="1">
<li><p><code>Sigmoid</code>
函数：<code>f(x) = 1 / (1 + e^(-x))</code>，输出范围在
<code>(0, 1)</code> 之间，常用于二分类问题的输出层。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image.png" alt="alt text"></p>
<p>可以看出，sigmoid函数连续，光滑，严格单调，以(0,0.5)中心对称，是一个非常良好的阈值函数。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719936221442.png" alt="1719936221442">
<figcaption aria-hidden="true">1719936221442</figcaption>
</figure></li>
<li><p><code>Tanh</code>
函数：<code>f(x) = (e^x - e^(-x)) / (e^x + e^(-x))</code>，输出范围在
<code>(-1, 1)</code> 之间。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-1.png" alt="alt text"></p></li>
<li><p><code>ReLU</code>
函数：<code>f(x) = max(0, x)</code>，计算简单，在很多深度神经网络中广泛使用。
线性整流函数，又称修正线性单元ReLU，是一种人工神经网络中常用的激活函数，通常指代以斜坡函数及其变种为代表的非线性函数。</p>
<p>线性整流函数（ReLU函数）的特点：</p>
<p>当输入为正时，不存在梯度饱和问题。 计算速度快得多。ReLU
函数中只存在线性关系，因此它的计算速度比Sigmoid函数和tanh函数更快。 Dead
ReLU问题。当输入为负时，ReLU完全失效，在正向传播过程中，这不是问题。有些区域很敏感，有些则不敏感。但是在反向传播过程中，如果输入负数，则梯度将完全为零，Sigmoid函数和tanh函数也具有相同的问题
ReLU函数的输出为0或正数，这意味着ReLU函数不是以0为中心的函数。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-2.png" alt="alt text"></p></li>
<li><p><code>Leaky ReLU</code>
函数：<code>f(x) = max(0.01x, x)</code>，是 <code>ReLU</code>
的改进版，解决了 <code>ReLU</code> 中神经元可能“死亡”的问题。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719937549074.png" alt="1719937549074">
<figcaption aria-hidden="true">1719937549074</figcaption>
</figure></li>
</ol>
<p>Leaky ReLU函数的特点：</p>
<p>Leaky ReLU函数通过把x xx的非常小的线性分量给予负输入0.01 x
0.01x0.01x来调整负值的零梯度问题。 Leaky有助于扩大ReLU函数的范围，通常α
0.01左右。 Leaky ReLU的函数范围是负无穷到正无穷。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-3.png" alt="alt text"></p>
<p>下面是一个使用 <code>Sigmoid</code> 激活函数的简单示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line">x = np.array([-<span class="number">2</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(sigmoid(x))</span><br></pre></td></tr></table></figure>
<p>激活函数的选择对神经网络的性能有很大影响，需要根据具体问题和网络结构进行合适的选择。</p>
<h3 id="常见的激活函数">常见的激活函数：</h3>
<ol type="1">
<li><p><strong>Sigmoid 函数</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = 1 / (1 + e^(-x))</code></li>
<li>特点：将输入值压缩到 0 到 1
之间，具有平滑的曲线。但在输入值较大或较小时，梯度接近
0，可能导致梯度消失问题。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image.png" alt="alt text"></li>
</ul></li>
<li><p><strong>Tanh 函数</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = (e^x - e^(-x)) / (e^x + e^(-x))</code></li>
<li>特点：将输入值压缩到 -1 到 1 之间，相比 Sigmoid 函数，以 0 为中心。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-1.png" alt="alt text"></li>
</ul></li>
<li><p><strong>ReLU 函数（Rectified Linear Unit）</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = max(0, x)</code></li>
<li>特点：计算简单，在正半轴上梯度恒为
1，有效缓解了梯度消失问题。但存在神经元“死亡”的可能，即输入为负时永远不被激活。
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/image-2.png" alt="alt text"></li>
</ul></li>
<li><p><strong>Leaky ReLU 函数</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = max(ax, x)</code> ，其中 <code>a</code>
是一个较小的正数（如 0.01）</li>
<li>特点：对 ReLU 进行改进，解决了神经元“死亡”的问题。 <img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719937549074.png" alt="1719937549074"></li>
</ul></li>
<li><p><strong>ELU 函数（Exponential Linear Unit）</strong>：</p>
<ul>
<li>数学表达式：<code>f(x) = x if x &gt; 0 else a(e^x - 1)</code> ，其中
<code>a</code> 是一个常数</li>
<li>特点：具有 ReLU 的优点，同时在输入为负时输出不为
0，使得平均输出更接近 0。</li>
<li><figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1719937783555.png" alt="1719937783555">
<figcaption aria-hidden="true">1719937783555</figcaption>
</figure></li>
</ul></li>
<li><p><strong>Softmax 函数</strong>：</p>
<ul>
<li>常用于多分类问题的输出层，将多个神经元的输出值映射为概率分布。</li>
</ul></li>
</ol>
<p>以下是一个使用 Python 绘制部分常见激活函数图像的示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tanh</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.tanh(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(<span class="number">0</span>, x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">leaky_relu</span>(<span class="params">x, a=<span class="number">0.01</span></span>):</span><br><span class="line">    <span class="keyword">return</span> np.maximum(a * x, x)</span><br><span class="line"></span><br><span class="line">x = np.linspace(-<span class="number">5</span>, <span class="number">5</span>, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">plt.plot(x, sigmoid(x), label=<span class="string">&#x27;Sigmoid&#x27;</span>)</span><br><span class="line">plt.plot(x, tanh(x), label=<span class="string">&#x27;Tanh&#x27;</span>)</span><br><span class="line">plt.plot(x, relu(x), label=<span class="string">&#x27;ReLU&#x27;</span>)</span><br><span class="line">plt.plot(x, leaky_relu(x), label=<span class="string">&#x27;Leaky ReLU&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Input&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Output&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Common Activation Functions&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>不同的激活函数在不同的场景和网络结构中表现各异，需要根据具体问题进行选择和调整。</p>
<h2 id="学习率">学习率</h2>
<p>学习率（Learning Rate）在机器学习中是一个非常关键的超参数。</p>
<p>学习率决定了模型在训练过程中参数更新的步长大小。</p>
<p>较小的学习率意味着模型在参数更新时采取较小的步幅，可能会使训练过程更加稳定，但收敛速度较慢，需要更多的训练迭代次数才能达到较好的效果。</p>
<p>例如，如果学习率过小，如 0.0001
，模型可能会在训练中进展缓慢，需要大量的训练轮数才能逐渐优化参数。</p>
<p>较大的学习率会使模型在参数更新时采取较大的步幅，可能会加快收敛速度，但也可能导致模型跳过最优解，甚至无法收敛。</p>
<p>比如，学习率过大，如 10
，模型可能会在训练中出现剧烈的波动，无法稳定地优化参数。</p>
<p>选择合适的学习率通常需要通过试验和错误来确定。常见的方法包括使用固定的学习率、学习率衰减（随着训练的进行逐渐减小学习率）、自适应学习率算法（如
Adam 优化器中的自适应学习率调整）等。</p>
<p>例如，在深度学习中，训练开始时可以使用较大的学习率，如 0.1
，然后随着训练的进行，逐渐将学习率减小到 0.001
，以实现更精细的参数调整和更好的收敛效果。</p>
<h3 id="以下是一些常见的学习率调整方法">以下是一些常见的学习率调整方法：</h3>
<ol type="1">
<li><p><strong>固定学习率</strong>：在整个训练过程中保持学习率不变。这种方法简单，但可能不是最优的，因为在训练的不同阶段可能需要不同的学习率。</p></li>
<li><p><strong>分段常数学习率</strong>：将训练过程分为几个阶段，每个阶段使用不同的固定学习率。例如，在前几个
epoch 使用较大的学习率，然后在后续阶段使用较小的学习率。</p></li>
<li><p><strong>学习率衰减</strong>：</p>
<ul>
<li><strong>按步长衰减</strong>：每隔一定的训练步数或
epoch，将学习率乘以一个小于 1 的衰减因子。</li>
<li><strong>指数衰减</strong>：学习率按照指数形式衰减，例如
<code>learning_rate = initial_learning_rate * decay_rate ^ (epoch / decay_steps)</code>
。</li>
<li><strong>多项式衰减</strong>：学习率按照多项式的形式逐渐减小。</li>
</ul></li>
<li><p><strong>自适应学习率算法</strong>：</p>
<ul>
<li><strong>Adagrad</strong>：根据每个参数之前的梯度历史来调整学习率，对于不常更新的参数给予较大的学习率，对于频繁更新的参数给予较小的学习率。</li>
<li><strong>Adadelta</strong>：是对 Adagrad
的改进，避免了学习率单调递减的问题。</li>
<li><strong>RMSProp</strong>：类似于
Adadelta，对梯度的二阶矩进行指数加权平均来调整学习率。</li>
<li><strong>Adam</strong>：结合了动量和 RMSProp
的优点，能够自适应地调整学习率。</li>
</ul></li>
<li><p><strong>余弦退火</strong>：学习率按照余弦函数的形式进行周期性的变化，在每个周期内从较高的值逐渐降低到较低的值，然后再上升。</p></li>
<li><p><strong>基于验证集性能调整</strong>：根据模型在验证集上的性能来动态调整学习率。如果验证集性能在一段时间内没有改善，就降低学习率。</p></li>
</ol>
<p>这些方法各有优缺点，具体选择哪种方法取决于数据集、模型架构和训练需求等因素。通常需要通过实验来找到最适合特定问题的学习率调整策略。</p>
<h2 id="optimizer的概念">optimizer的概念</h2>
<p>在机器学习中，<code>optimizer</code>（优化器）是用于调整模型参数以最小化损失函数或最大化目标函数的算法或策略。</p>
<p>优化器的主要作用是根据模型的当前状态（包括参数值和计算得到的梯度）来决定如何更新模型的参数，以使得模型在训练数据上的性能逐渐提高。</p>
<p>常见的优化器有随机梯度下降（Stochastic Gradient
Descent，SGD）、Adagrad、Adadelta、RMSprop、Adam 等。</p>
<p>以随机梯度下降（SGD）为例，它的基本思想是沿着梯度的反方向，以一定的学习率来更新参数。假设参数为
<code>w</code> ，梯度为 <code>g</code> ，学习率为 <code>lr</code>
，则更新公式为 <code>w = w - lr * g</code> 。</p>
<p>Adagrad
则根据每个参数之前的梯度历史来自适应地调整学习率。对于频繁更新的参数，学习率会逐渐减小，而对于很少更新的参数，学习率会相对较大。</p>
<p>RMSprop 类似于
Adagrad，但它不是累积所有的梯度平方，而是使用指数加权平均来计算梯度平方的估计值。</p>
<p>Adam 结合了动量和 RMSprop
的优点，同时考虑了梯度的一阶矩和二阶矩来动态调整学习率。</p>
<p>选择合适的优化器对于模型的训练效率和最终性能至关重要。例如，在数据量较大且模型较复杂时，Adam
通常能取得较好的效果；而在一些简单的模型和小数据集上，SGD
可能表现不错，并且通过适当的调整学习率等超参数，也能获得较好的结果。</p>
<p>再比如，在处理具有稀疏特征的问题时，Adagrad
可能更合适。总之，优化器的选择需要根据具体的问题和数据特点来决定，并可能需要通过实验来找到最优的选择。</p>
<h2 id="pytorch代码分析">pytorch代码分析:</h2>
<p>如何理解下面的代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span><br><span class="line">		<span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">		self.layer = nn.Linear(n_feature, n_hidden)</span><br><span class="line">		self.all_act = nn.Linear(n_hidden, n_output)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">		x = self.layer(x)</span><br><span class="line">		x = torch.tanh(x)</span><br><span class="line">		x = self.all_act(x)</span><br><span class="line">		<span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这段代码是一个简单的神经网络模型的定义，使用 PyTorch
框架来构建。让我解释一下这段代码的含义：</p>
<ol type="1">
<li><strong>class Net(nn.Module):</strong>：定义了一个名为
<strong>Net</strong> 的类，继承自 <strong>nn.Module</strong>
类，表示这个类是一个神经网络模型。</li>
</ol>
<p><strong>2. </strong>def <strong>init</strong>(self, n_feature,
n_hidden, n_output):<strong>：定义了 </strong>Net** 类的构造函数
<strong><strong>init</strong></strong>，该函数接受三个参数
<strong>n_feature</strong>、<strong>n_hidden</strong> 和
<strong>n_output</strong>，分别表示输入特征的数量、隐藏层神经元的数量和输出的数量。**</p>
<p><strong>3. </strong>super(Net, self).__init__()<strong>：调用父类
</strong>nn.Module** 的构造函数，初始化神经网络模型。**</p>
<p><strong>4. </strong>self.layer = nn.Linear(n_feature,
n_hidden)<strong>：定义了一个线性层
</strong>layer<strong>，输入特征数量为
</strong>n_feature<strong>，输出特征数量为
</strong>n_hidden<strong>。</strong></p>
<p><strong>5. </strong>self.all_act = nn.Linear(n_hidden,
n_output)<strong>：定义了另一个线性层
</strong>all_act<strong>，输入特征数量为
</strong>n_hidden<strong>，输出特征数量为
</strong>n_output<strong>。</strong></p>
<ol start="6" type="1">
<li><strong>def forward(self, x):</strong>：定义了前向传播函数
<strong>forward</strong>，接受输入
<strong>x</strong>，表示对输入数据进行前向传播计算。</li>
<li><strong>x = self.layer(x)</strong>：将输入 <strong>x</strong> 经过
<strong>layer</strong> 线性层的计算，得到隐藏层的输出。</li>
<li><strong>x =
torch.tanh(x)</strong>：对隐藏层的输出应用双曲正切函数（tanh）作为激活函数，增加模型的非线性能力。</li>
<li><strong>x =
self.all_act(x)</strong>：将经过激活函数后的隐藏层输出再经过
<strong>all_act</strong> 线性层的计算，得到最终的输出。</li>
<li><strong>return x</strong>：返回神经网络模型的输出。</li>
</ol>
<p>这段代码定义了一个简单的神经网络模型，包括一个输入层到隐藏层的线性变换和隐藏层到输出层的线性变换，中间使用了双曲正切函数作为激活函数。在神经网络的训练过程中，可以通过调用
<strong>forward</strong> 函数来进行前向传播计算。</p>
<h3 id="tf.one_hot">tf.one_hot</h3>
<p><code>tf.one_hot</code> 是 TensorFlow
中的一个函数，用于将输入的索引值转换为独热编码（One-Hot
Encoding）的张量。</p>
<p>独热编码是一种将类别变量转换为二进制向量的编码方式，其中只有一个元素为
1 ，其余元素为 0 。</p>
<p>以下是 <code>tf.one_hot</code> 函数的一般用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">indices = [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]</span><br><span class="line">depth = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">one_hot_encoded = tf.one_hot(indices, depth)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(one_hot_encoded.numpy())</span><br></pre></td></tr></table></figure>
<p>在上述示例中，<code>indices</code>
是要编码的索引值列表，<code>depth</code>
表示编码的维度（即类别数量）。</p>
<p>例如，对于索引值 <code>0</code> ，在维度为 4
的编码中，得到的独热编码为 <code>[1, 0, 0, 0]</code> ；对于索引值
<code>2</code> ，得到的编码为 <code>[0, 0, 1, 0]</code> 。</p>
<p><code>tf.one_hot</code>
常用于将分类标签转换为适合神经网络输入的形式，方便模型进行处理和计算。</p>
<p>上述代码中，<code>indices = [0, 2, 1, 3]</code> 且
<code>depth = 4</code> ，使用 <code>tf.one_hot</code>
函数进行独热编码后的输出结果应该是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[1 0 0 0]</span><br><span class="line"> [0 0 1 0]</span><br><span class="line"> [0 1 0 0]</span><br><span class="line"> [0 0 0 1]]</span><br></pre></td></tr></table></figure>
<p>即索引为 0 的位置编码为 <code>[1, 0, 0, 0]</code> ，索引为 2
的位置编码为 <code>[0, 0, 1, 0]</code> ，索引为 1 的位置编码为
<code>[0, 1, 0, 0]</code> ，索引为 3 的位置编码为
<code>[0, 0, 0, 1]</code> 。</p>
<h3 id="self.critic_net.parameters">self.critic_net.parameters()</h3>
<p><code>self.critic_net.parameters()</code> 通常是在 Python
的深度学习框架（如 PyTorch）中使用的代码。</p>
<p><code>critic_net</code>
可能是您定义的一个神经网络模型（例如，用于评估或批评某些数据的模型）。</p>
<p><code>parameters()</code>
方法返回模型中的可学习参数，这些参数通常是权重（weights）和偏置（biases）。</p>
<p>例如，如果 <code>critic_net</code>
是一个简单的全连接神经网络，那么通过
<code>self.critic_net.parameters()</code>
您将获取到该网络中所有层的权重和偏置的迭代器。</p>
<p>您可以使用这个返回值来进行一些操作，比如：</p>
<ol type="1">
<li><p>在优化器中使用它来进行参数的更新，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(self.critic_net.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>对参数进行一些统计或分析，比如计算参数的数量、查看参数的值等。</p></li>
</ol>
<p>总之，<code>self.critic_net.parameters()</code>
为您提供了对模型可学习参数的访问，以便进行各种与模型训练和优化相关的操作。</p>
<h3 id="self.optimizer.zero_grad">self.optimizer.zero_grad()</h3>
<p><code>self.optimizer.zero_grad()</code>
通常用于将模型参数的梯度清零。</p>
<p>当进行反向传播计算梯度后，如果不将梯度清零，那么在后续的迭代中，新计算的梯度会与之前的梯度累加。这会导致梯度计算的错误，影响模型的训练效果</p>
<h3 id="loss.backwardretain_graphtrue">loss.backward(retain_graph=True)</h3>
<p>在深度学习中，<code>loss.backward(retain_graph=True)</code>
用于计算损失函数 <code>loss</code> 关于模型参数的梯度，同时通过设置
<code>retain_graph=True</code> 来保留计算图。</p>
<p>通常，在进行一次反向传播计算梯度后，计算图会被释放以节省内存。但当您需要多次对同一计算图执行反向传播时，就需要设置
<code>retain_graph=True</code> 。</p>
<p>例如，如果您在一个循环中多次计算梯度并更新参数，像下面这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">    <span class="comment"># 前向传播，计算损失</span></span><br><span class="line">    loss = some_operation()</span><br><span class="line">    <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">    loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 根据梯度更新参数</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>在上述示例中，由于在循环内多次执行反向传播，所以需要保留计算图，以便下一次迭代能够再次基于相同的计算图计算梯度。</p>
<p>如果不设置 <code>retain_graph=True</code> ，在第二次执行
<code>loss.backward()</code> 时会报错，因为计算图已经被释放。</p>
<p>需要注意的是，一直保留计算图会占用较多内存，所以在不需要多次使用计算图时，应避免设置
<code>retain_graph=True</code> ，以节省内存资源。</p>
<p>在 PyTorch 中，<code>retain_graph=True</code>
的作用是在执行反向传播（<code>backward</code>）后保留计算图。</p>
<p>PyTorch
采用动态图机制，默认情况下，每次反向传播之后都会释放计算图以节省内存。然而，当需要对同一个计算图进行多次反向传播时，就需要设置
<code>retain_graph=True</code>。</p>
<p>例如，假设有一个输入 <code>x</code>，经过一些计算得到
<code>y</code>，然后有两个输出 <code>output_1</code> 和
<code>output_2</code>，都需要对其执行反向传播。如果在第一次对
<code>output_1</code> 执行 <code>backward</code> 时没有设置
<code>retain_graph=True</code>，那么在执行完后计算图就会被释放。当再对
<code>output_2</code> 执行 <code>backward</code>
时，由于计算图已经不存在，就会引发错误。</p>
<p>通过设置 <code>retain_graph=True</code>，可以保留中间参数，使得多次
<code>backward</code> 之间不会相互影响。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss1.backward(retain_graph=<span class="literal">True</span>)  <span class="comment"># 第一次 backward，保留计算图</span></span><br><span class="line">loss2.backward()  <span class="comment"># 第二次 backward，此时计算图仍然存在</span></span><br></pre></td></tr></table></figure>
<p>需要注意的是，一直保留计算图会占用较多内存，所以在不需要多次使用计算图时，应避免设置
<code>retain_graph=True</code>，以节省内存资源。</p>
<p>另外，<code>create_graph</code>
参数用于创建导数计算图，允许进行高阶导数的计算；而
<code>grad_tensors</code>
可用于设置多梯度的权重，当有多个损失函数需要计算梯度时，可以设置各个损失之间权重的比例。</p>
<h4 id="如何判断是否需要多次使用计算图">如何判断是否需要多次使用计算图？</h4>
<p>判断是否需要多次使用计算图，主要考虑以下几个方面：</p>
<ol type="1">
<li>模型的结构和需求：如果你的模型在一次前向传播后，需要对不同的输出或中间结果进行多次反向传播来计算梯度，以实现不同的目的，例如优化不同的部分或计算多个相关的梯度，那么就可能需要多次使用计算图。</li>
<li>是否涉及多个相关的损失函数：当存在多个损失函数，且需要分别或同时根据这些损失函数计算梯度并更新参数时，通常需要多次使用计算图。</li>
<li>循环或迭代的操作：在某些情况下，可能会在一个循环或迭代中多次执行类似的计算和反向传播操作。例如，在强化学习中，可能需要在每个时间步都进行前向传播和反向传播。</li>
<li>中间结果的重用：如果在计算过程中需要多次访问或使用某些中间结果的梯度，那么就需要保留计算图以进行多次反向传播。</li>
</ol>
<p>例如，如果你有一个神经网络模型，其中包含多个子模块，每个子模块都有自己的损失，并且你希望分别根据这些子模块的损失来更新它们的参数，那么就需要多次使用计算图，对每个子模块的损失进行单独的反向传播。</p>
<p>又或者在训练过程中，你可能想要尝试不同的优化策略或超参数，需要在同一次前向传播后，多次计算梯度并更新参数，来比较不同策略或参数的效果，这也需要多次使用计算图。</p>
<p>另外，一些复杂的模型结构或自定义的计算流程可能会导致需要多次使用计算图的情况。但需注意，一直保留计算图会占用较多内存，所以在不需要多次使用计算图时，应避免设置
<code>retain_graph=True</code> ，以节省内存资源。</p>
<p>如果你不确定是否需要多次使用计算图，可以先尝试在不保留计算图的情况下（即不设置
<code>retain_graph=True</code> ）运行代码，观察是否会出现报错“trying to
backward through the graph a second time, but the buffers have already
been freed”。如果出现该报错，且确实需要进行多次反向传播，那么就需要设置
<code>retain_graph=True</code>
。同时，为了避免内存过度占用，在完成所有需要的反向传播后，应及时释放不再使用的计算图和相关数据。</p>
<h3 id="tf.reduce_mean">tf.reduce_mean</h3>
<p><code>tf.reduce_mean</code> 是 TensorFlow
中的一个函数，用于计算张量在指定维度上的平均值。</p>
<p>以下是一个示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个张量</span></span><br><span class="line">tensor = tf.constant([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算所有元素的平均值</span></span><br><span class="line">average = tf.reduce_mean(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每一行的平均值</span></span><br><span class="line">row_averages = tf.reduce_mean(tensor, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算每一列的平均值</span></span><br><span class="line">column_averages = tf.reduce_mean(tensor, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;所有元素的平均值：&quot;</span>, sess.run(average))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;每一行的平均值：&quot;</span>, sess.run(row_averages))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;每一列的平均值：&quot;</span>, sess.run(column_averages))</span><br></pre></td></tr></table></figure>
<p>在上述示例中：</p>
<ul>
<li>当不指定 <code>axis</code>
参数时，<code>tf.reduce_mean(tensor)</code> 计算张量 <code>tensor</code>
中所有元素的平均值。</li>
<li><code>tf.reduce_mean(tensor, axis=1)</code>
计算每一行的平均值，得到一个长度为行数的张量。</li>
<li><code>tf.reduce_mean(tensor, axis=0)</code>
计算每一列的平均值，得到一个长度为列数的张量。</li>
</ul>
<p>例如，对于上述示例中的张量 <code>[[1, 2, 3], [4, 5, 6]]</code> ：</p>
<ul>
<li>所有元素的平均值为 <code>(1 + 2 + 3 + 4 + 5 + 6) / 6 = 3.5</code>
。</li>
<li>每一行的平均值分别为 <code>(1 + 2 + 3) / 3 = 2</code> 和
<code>(4 + 5 + 6) / 3 = 5</code> 。</li>
<li>每一列的平均值分别为 <code>(1 + 4) / 2 = 2.5</code> 、
<code>(2 + 5) / 2 = 3.5</code> 和 <code>(3 + 6) / 2 = 4.5</code> 。</li>
</ul>
<h3 id="tf.placeholder">tf.placeholder</h3>
<p>用于表示强化学习或神经网络中的状态输入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with tf.name_scope(&#x27;S&#x27;):#状态</span><br><span class="line">    S = tf.placeholder(tf.float32, shape=[None, state_dim], name=&#x27;s&#x27;)</span><br></pre></td></tr></table></figure>
<ol type="1">
<li>tf.name_scope('S')</li>
</ol>
<p>这行代码使用 TensorFlow 的 tf.name_scope 创建一个命名空间
S。命名空间在 TensorFlow
中用于组织图中的节点，使其更具可读性和结构化。在 TensorBoard
中查看图时，可以更清晰地看到节点的组织结构。</p>
<ol start="2" type="1">
<li>tf.placeholder(tf.float32, shape=[None, state_dim], name='s')</li>
</ol>
<p>这行代码定义了一个占位符 S。占位符在 TensorFlow
中用于在执行图时提供输入数据。这里的占位符有以下几个参数：</p>
<p>tf.float32：占位符的数据类型是 32 位浮点数。</p>
<p><strong>shape=[None, state_dim]：占位符的形状。shape
参数定义了输入数据的维度</strong>：</p>
<p>None
表示这个维度可以是任意长度，通常用于<strong>批次（batch）的大小</strong>。使用
None 是因为在实际运行时，批次的大小可能会有所不同。</p>
<p>state_dim
表示状态的维度。这是一个整数，定义了每个状态的特征数量。在强化学习中，状态通常是一个向量，其长度由具体的环境或问题决定。</p>
<p>name='s'：给占位符命名为
's'。这个名字在构建和调试图时有助于识别。</p>
<figure>
<img src="/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/1720887133485.png" alt="1720887133485">
<figcaption aria-hidden="true">1720887133485</figcaption>
</figure>
<h3 id="tf.variable_scope">tf.variable_scope</h3>
<p><code>tf.variable_scope</code> 是 TensorFlow
中用于管理变量作用域的一个重要工具。它帮助组织和复用变量，特别是在构建复杂的神经网络时</p>
<h4 id="什么是-tf.variable_scope">1. 什么是
<code>tf.variable_scope</code>？</h4>
<p><code>tf.variable_scope</code>
提供了一种机制来创建和管理变量范围（scope），这对变量进行命名和复用非常有帮助。通过使用变量范围，可以确保变量命名的一致性和避免命名冲突。</p>
<h4 id="为什么使用-tf.variable_scope">2. 为什么使用
<code>tf.variable_scope</code>？</h4>
<ul>
<li><strong>组织变量</strong>：可以将相关的变量组织在一起，使代码更具可读性和结构性。</li>
<li><strong>复用变量</strong>：在共享参数的模型（如共享权重的神经网络层）中，复用变量非常重要。</li>
<li><strong>命名管理</strong>：自动处理变量命名，避免命名冲突。</li>
</ul>
<h4 id="如何使用-tf.variable_scope">3. 如何使用
<code>tf.variable_scope</code>？</h4>
<p>使用 <code>tf.variable_scope</code>
创建一个新的变量作用域，可以在该作用域内定义和复用变量。</p>
<h5 id="示例-1创建变量作用域">示例 1：创建变量作用域</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope1&#x27;</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope2&#x27;</span>):</span><br><span class="line">    var2 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">2.0</span>))</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>var1</code> 和 <code>var2</code>
是在不同的变量作用域中创建的，尽管它们的名字相同，但在图中它们是不同的变量，分别命名为
<code>scope1/var</code> 和 <code>scope2/var</code>。</p>
<h5 id="示例-2复用变量">示例 2：复用变量</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope&#x27;</span>, reuse=tf.AUTO_REUSE):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope&#x27;</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">    var2 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>var2</code> 复用了 <code>var1</code>
的变量。通过设置
<code>reuse=True</code>，确保在相同的作用域内复用已经存在的变量。</p>
<h5 id="tf.get_variable-和-tf.variable-的区别">4.
<code>tf.get_variable</code> 和 <code>tf.Variable</code> 的区别</h5>
<ul>
<li><strong><code>tf.get_variable</code></strong>：用于创建或获取变量，通常与
<code>tf.variable_scope</code> 一起使用。它支持变量复用机制。</li>
<li><strong><code>tf.Variable</code></strong>：直接创建新变量，不支持复用机制。</li>
</ul>
<h5 id="示例-3使用-tf.get_variable-和-tf.variable">示例 3：使用
<code>tf.get_variable</code> 和 <code>tf.Variable</code></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;scope1&#x27;</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">&#x27;var&#x27;</span>, shape=[<span class="number">1</span>], initializer=tf.constant_initializer(<span class="number">1.0</span>))</span><br><span class="line">    var2 = tf.Variable([<span class="number">1.0</span>], name=<span class="string">&#x27;var2&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>var1</code> 是通过 <code>tf.get_variable</code>
创建的，支持复用机制，而 <code>var2</code> 是通过
<code>tf.Variable</code> 创建的，不支持复用。</p>
<h4 id="总结">5. 总结</h4>
<p><code>tf.variable_scope</code> 是 TensorFlow
中用于管理变量作用域的工具，帮助组织和复用变量，提供了一种结构化和高效的变量管理方式。通过合理使用变量作用域，可以避免命名冲突，实现变量复用，特别是在构建共享参数的复杂神经网络时非常有用。</p>
<h3 id="tf.session">tf.Session</h3>
<p><code>tf.Session</code> 是 TensorFlow 1.x
中一个重要的类，用于执行定义好的计算图（computation graph）。在
TensorFlow 2.x 中，<code>tf.Session</code> 被逐渐弃用，更多地采用了
Eager Execution 模式，但了解 <code>tf.Session</code>
对于维护和理解旧代码还是非常重要的。以下是对 <code>tf.Session</code>
的详细解释及其使用方法。</p>
<h4 id="什么是-tf.session">1. 什么是 <code>tf.Session</code>？</h4>
<p><code>tf.Session</code> 提供了一个运行 TensorFlow 操作的环境。它管理
TensorFlow 运行时的所有资源，包括变量的内存分配、执行设备的选择等。</p>
<h4 id="为什么需要-tf.session">2. 为什么需要
<code>tf.Session</code>？</h4>
<p>在 TensorFlow 1.x
中，计算图的定义和执行是分开的。你首先定义一个计算图，然后在
<code>tf.Session</code> 中执行它。<code>tf.Session</code>
提供了以下功能：</p>
<ul>
<li><strong>控制和管理资源</strong>：例如内存、线程等。</li>
<li><strong>分配计算设备</strong>：决定在 CPU 还是 GPU 上执行操作。</li>
<li><strong>执行计算图</strong>：运行图中的操作，获取结果。</li>
</ul>
<h4 id="如何使用-tf.session">3. 如何使用 <code>tf.Session</code>？</h4>
<h5 id="示例-1基本使用">示例 1：基本使用</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义计算图</span></span><br><span class="line">a = tf.constant(<span class="number">2</span>)</span><br><span class="line">b = tf.constant(<span class="number">3</span>)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话并运行计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result = sess.run(c)</span><br><span class="line">    <span class="built_in">print</span>(result)  <span class="comment"># 输出 5</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，首先定义了一个简单的计算图，然后创建一个
<code>tf.Session</code> 来运行这个图，并获取结果。</p>
<h5 id="示例-2使用-tf.placeholder-和-feed_dict">示例 2：使用
<code>tf.placeholder</code> 和 <code>feed_dict</code></h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义占位符和操作</span></span><br><span class="line">a = tf.placeholder(tf.int32)</span><br><span class="line">b = tf.placeholder(tf.int32)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话并运行计算图</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result = sess.run(c, feed_dict=&#123;a: <span class="number">2</span>, b: <span class="number">3</span>&#125;)</span><br><span class="line">    <span class="built_in">print</span>(result)  <span class="comment"># 输出 5</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，使用 <code>tf.placeholder</code>
定义了占位符，可以在运行时通过 <code>feed_dict</code> 提供输入数据。</p>
<h5 id="示例-3管理变量">示例 3：管理变量</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义变量和初始化操作</span></span><br><span class="line">var = tf.Variable([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建会话并初始化变量</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    result = sess.run(var)</span><br><span class="line">    <span class="built_in">print</span>(result)  <span class="comment"># 输出 [1. 2.]</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，定义了一个变量，并在会话中运行初始化操作来分配和初始化变量。</p>
<h4 id="tensorflow-2.x-中的变化">4. TensorFlow 2.x 中的变化</h4>
<p>在 TensorFlow 2.x 中，引入了 Eager
Execution，默认情况下无需显式创建会话，可以直接执行操作。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorFlow 2.x 默认启用 Eager Execution</span></span><br><span class="line">a = tf.constant(<span class="number">2</span>)</span><br><span class="line">b = tf.constant(<span class="number">3</span>)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(c)  <span class="comment"># 直接输出 &lt;tf.Tensor: shape=(), dtype=int32, numpy=5&gt;</span></span><br></pre></td></tr></table></figure>
<h4 id="总结-1">5. 总结</h4>
<p><code>tf.Session</code> 是 TensorFlow 1.x
中用于执行计算图的核心工具，帮助管理和控制资源，执行计算操作。在
TensorFlow 2.x 中，虽然 <code>tf.Session</code>
被弃用，但理解它的工作机制对于维护旧代码和理解 TensorFlow
的运行原理仍然非常重要。</p>
<h3 id="tf.random_normal_initializer">tf.random_normal_initializer</h3>
<p>这是 TensorFlow
提供的一个初始化器，用于从正态分布（高斯分布）中随机抽取样本来初始化变量。</p>
<h4 id="背景知识">背景知识</h4>
<p>在神经网络中，权重和偏置的初始化对模型的收敛速度和效果有很大影响。TensorFlow
提供了多种初始化器来帮助我们初始化这些变量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init_w = tf.random_normal_initializer(<span class="number">0.</span>, <span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>
<p><strong>参数解释</strong> ：</p>
<ul>
<li>第一个参数
<code>0.</code>：表示正态分布的均值（mean）。在这里，均值被设置为
<code>0</code>。</li>
<li>第二个参数 <code>0.3</code>：表示正态分布的标准差（standard
deviation）。在这里，标准差被设置为 <code>0.3</code>。
因此，<code>tf.random_normal_initializer(0., 0.3)</code> 表示使用均值为
<code>0</code>、标准差为 <code>0.3</code> 的正态分布来初始化变量。</li>
</ul>
<h4 id="实例说明">实例说明</h4>
<p>以下是一个简单的示例，展示如何使用这个初始化器来初始化一个 TensorFlow
变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个变量并使用 tf.random_normal_initializer 初始化</span></span><br><span class="line">init_w = tf.random_normal_initializer(<span class="number">0.</span>, <span class="number">0.3</span>)</span><br><span class="line">var = tf.Variable(initial_value=init_w(shape=[<span class="number">2</span>, <span class="number">3</span>], dtype=tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化全局变量</span></span><br><span class="line">tf.compat.v1.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建并运行一个会话以查看变量的值</span></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.compat.v1.global_variables_initializer())</span><br><span class="line">    <span class="built_in">print</span>(sess.run(var))</span><br></pre></td></tr></table></figure>
<h4 id="解释这个示例">解释这个示例</h4>
<ol type="1">
<li><strong>定义初始化器</strong>
：<code>init_w = tf.random_normal_initializer(0., 0.3)</code>
定义了一个初始化器，它会生成均值为 <code>0</code>、标准差为
<code>0.3</code> 的随机数。</li>
<li><strong>初始化变量</strong>
：<code>var = tf.Variable(initial_value=init_w(shape=[2, 3], dtype=tf.float32))</code>
使用这个初始化器来初始化一个形状为 <code>[2, 3]</code> 的变量。</li>
<li><strong>初始化全局变量</strong>
：<code>tf.compat.v1.global_variables_initializer()</code>
用于初始化所有的 TensorFlow 变量。</li>
<li><strong>运行会话</strong> ：创建并运行一个 TensorFlow
会话以查看变量的值。</li>
</ol>
<h3 id="tf.compat.v1.global_variables_initializer">tf.compat.v1.global_variables_initializer</h3>
<p><code>tf.compat.v1.global_variables_initializer</code> 是 TensorFlow
1.x 中的一个函数，用于初始化所有的全局变量。在 TensorFlow 2.x
中，这个函数被包括在 <code>compat.v1</code> 模块中，以便兼容旧代码。</p>
<h4 id="作用">作用</h4>
<p><code>tf.compat.v1.global_variables_initializer()</code>
函数会创建一个操作（operation），这个操作会初始化 TensorFlow
图中的所有全局变量。执行这个操作能够将所有变量赋值为它们的初始值。</p>
<h4 id="变量初始化的重要性">变量初始化的重要性</h4>
<p>在使用 TensorFlow
构建和训练模型时，变量（如权重和偏置）需要先被初始化，然后才能在计算图中使用。未初始化的变量在使用时会导致错误。</p>
<h3 id="使用示例">使用示例</h3>
<p>下面是一个简单的示例，展示了如何使用
<code>tf.compat.v1.global_variables_initializer()</code>
来初始化变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建变量</span></span><br><span class="line">var1 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">&quot;var1&quot;</span>)</span><br><span class="line">var2 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">&quot;var2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建初始化操作</span></span><br><span class="line">init_op = tf.compat.v1.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建并运行会话以初始化变量</span></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init_op)  <span class="comment"># 初始化所有变量</span></span><br><span class="line">    <span class="built_in">print</span>(sess.run(var1))</span><br><span class="line">    <span class="built_in">print</span>(sess.run(var2))</span><br></pre></td></tr></table></figure>
<h3 id="解释这个示例-1">解释这个示例</h3>
<ol type="1">
<li><p><strong>创建变量</strong>：我们创建了两个变量 <code>var1</code>
和 <code>var2</code>，它们的初始值是从正态分布中随机抽取的。</p></li>
<li><p><strong>创建初始化操作</strong>：使用
<code>tf.compat.v1.global_variables_initializer()</code>
创建一个初始化操作 <code>init_op</code>。</p></li>
<li><p><strong>运行会话</strong>：</p>
<ul>
<li>使用 <code>with tf.compat.v1.Session() as sess:</code> 创建一个
TensorFlow 会话。</li>
<li>使用 <code>sess.run(init_op)</code>
来运行初始化操作，初始化所有变量。</li>
<li>之后，通过 <code>sess.run(var1)</code> 和
<code>sess.run(var2)</code> 来查看变量的值。</li>
</ul></li>
</ol>
<h3 id="tensorflow-2.x-的变化">TensorFlow 2.x 的变化</h3>
<p>在 TensorFlow 2.x 中，变量的初始化通常由更高级别的 API（如
Keras）自动处理。直接使用
<code>tf.compat.v1.global_variables_initializer()</code>
的情况较少见，除非是在兼容旧代码或使用低级别 API 时。</p>
<p>以下是 TensorFlow 2.x 的一个简单示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建变量</span></span><br><span class="line">var1 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">&quot;var1&quot;</span>)</span><br><span class="line">var2 = tf.Variable(tf.random.normal([<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">&quot;var2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化所有变量</span></span><br><span class="line">tf.compat.v1.global_variables_initializer().run(session=tf.compat.v1.Session())</span><br></pre></td></tr></table></figure>
<h3 id="总结-2">总结</h3>
<p><code>tf.compat.v1.global_variables_initializer()</code>
是用于初始化所有全局变量的函数。在 TensorFlow 2.x 中，通常通过更高级别的
API 处理变量的初始化，但在需要兼容 TensorFlow 1.x
代码时，<code>tf.compat.v1.global_variables_initializer()</code>
仍然非常有用。通过初始化变量，确保在计算图中使用这些变量时不会遇到未初始化变量的错误。</p>
<h3 id="tf.constant_initializer">tf.constant_initializer</h3>
<p><code>tf.constant_initializer</code> 是 TensorFlow
中的一个初始化器，用于创建一个常量初始化器对象，该对象用于将变量初始化为指定的常量值。</p>
<h4 id="使用方法">使用方法</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">initializer = tf.constant_initializer(value)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>value</code> 是要初始化的常量值。</li>
</ul>
<h4 id="示例">示例</h4>
<p>以下是一个简单的示例，展示如何使用
<code>tf.constant_initializer</code> 来初始化一个 TensorFlow
变量为常量值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个常量初始化器，将变量初始化为常量值 42.0</span></span><br><span class="line">initializer = tf.constant_initializer(<span class="number">42.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个变量，并使用常量初始化器初始化</span></span><br><span class="line">var = tf.Variable(initializer(shape=[<span class="number">2</span>, <span class="number">3</span>]), name=<span class="string">&quot;constant_var&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化所有变量</span></span><br><span class="line">tf.compat.v1.global_variables_initializer().run(session=tf.compat.v1.Session())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印变量的值</span></span><br><span class="line"><span class="keyword">with</span> tf.compat.v1.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(sess.run(var))</span><br></pre></td></tr></table></figure>
<h4 id="解释这个示例-2">解释这个示例</h4>
<ol type="1">
<li><p><strong>创建初始化器</strong>：使用
<code>tf.constant_initializer(42.0)</code>
创建一个常量初始化器，它会将变量初始化为常量值
<code>42.0</code>。</p></li>
<li><p><strong>初始化变量</strong>：使用
<code>initializer(shape=[2, 3])</code> 创建一个形状为
<code>[2, 3]</code> 的变量，并使用常量初始化器初始化它。</p></li>
<li><p><strong>运行会话</strong>：</p>
<ul>
<li>使用 <code>tf.compat.v1.global_variables_initializer()</code>
初始化所有变量。</li>
<li>使用 <code>sess.run(var)</code>
在会话中运行并打印变量的值，输出将会是一个形状为 <code>[2, 3]</code>
的数组，其所有元素都是 <code>42.0</code>。</li>
</ul></li>
</ol>
<h4 id="应用场景">应用场景</h4>
<ul>
<li><strong>固定初始化值</strong>：当你希望将所有权重或偏置初始化为固定的常量值时，可以使用
<code>tf.constant_initializer</code>。</li>
<li><strong>保证初始化的一致性</strong>：有时在调试和开发过程中，需要确保变量被初始化为固定的常量，以便进行稳定性检查和测试。</li>
</ul>
<h4 id="tensorflow-2.x-的变化-1">TensorFlow 2.x 的变化</h4>
<p>在 TensorFlow 2.x 中，变量的初始化通常由更高级别的 API（如
Keras）自动处理。直接使用 <code>tf.constant_initializer</code>
的情况较少见，除非是在兼容旧代码或使用低级别 API 时。</p>
<h4 id="总结-3">总结</h4>
<p><code>tf.constant_initializer</code> 是 TensorFlow
中的一个初始化器，用于将变量初始化为指定的常量值。通过设置常量值，可以确保在构建和训练神经网络时，某些变量始终具有固定的初始值。</p>
<h3 id="tf.layers.dense">tf.layers.dense</h3>
<p><code>tf.layers.dense</code> 是 TensorFlow
提供的一个高级API，用于创建全连接层。它自动创建并管理权重变量（kernel）和偏置变量（bias），并且可以通过参数来控制激活函数、初始化器、层名称等。这样可以简化神经网络的构建过程，并提供了一致性的接口。</p>
<p><strong>示例解释</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 s 是输入张量，init_w 和 init_b 是初始化器</span></span><br><span class="line">s = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, input_dim])  <span class="comment"># input_dim 是输入的特征数</span></span><br><span class="line"></span><br><span class="line">init_w = tf.random_normal_initializer(<span class="number">0.</span>, <span class="number">0.1</span>)</span><br><span class="line">init_b = tf.constant_initializer(<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个全连接层</span></span><br><span class="line">net = tf.layers.dense(s, <span class="number">30</span>, activation=tf.nn.relu,</span><br><span class="line">                      kernel_initializer=init_w, bias_initializer=init_b, name=<span class="string">&#x27;l1&#x27;</span>,</span><br><span class="line">                      trainable=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>这段代码的作用是在输入张量 <code>s</code> 上创建一个具有 30
个神经元、ReLU
激活函数的全连接层，使用指定的权重和偏置初始化器，并指定该层为可训练的。这样，TensorFlow
将自动处理该层的变量初始化和管理，使得神经网络的搭建更加高效和便捷。</p>
<h4 id="上面30-个神经元的含义">上面30 个神经元的含义</h4>
<p>神经网络中的“神经元”是指一个计算单元，它接收输入、应用权重和偏置，并通过激活函数产生输出。在全连接层（dense
layer）中，神经元的数量决定了该层的输出维度。</p>
<p>在代码
<code>net = tf.layers.dense(s, 30, activation=tf.nn.relu, kernel_initializer=init_w, bias_initializer=init_b, name='l1', trainable=trainable)</code>
中，指定了这一层有 30 个神经元。具体来说：</p>
<ol type="1">
<li><strong>输入张量</strong> ：假设输入张量 <code>s</code> 的形状为
<code>[batch_size, input_dim]</code>，其中 <code>batch_size</code>
是批次大小（每次训练或推理使用的样本数量），<code>input_dim</code>
是输入特征的数量。</li>
<li><strong>神经元的作用</strong> ：每个神经元都接收输入张量
<code>s</code>
的所有特征，将这些特征与它自身的权重（weights）相乘，然后加上一个偏置（bias），再通过激活函数（如
ReLU）计算输出。</li>
<li><strong>输出维度</strong> ：具有 30 个神经元意味着输出张量的形状将是
<code>[batch_size, 30]</code>。也就是说，每个输入样本将被转换为一个 30
维的输出向量。</li>
</ol>
<h5 id="详细步骤">详细步骤</h5>
<ol type="1">
<li><strong>权重矩阵</strong> ：如果输入张量 <code>s</code> 的形状为
<code>[batch_size, input_dim]</code>，则权重矩阵 <code>W</code> 的形状为
<code>[input_dim, 30]</code>。</li>
<li><strong>计算加权和</strong> ：每个神经元计算加权和
<code>z = W*x + b</code>，其中 <code>x</code> 是输入特征，<code>W</code>
是权重，<code>b</code> 是偏置。</li>
<li><strong>应用激活函数</strong> ：计算出加权和 <code>z</code>
后，应用激活函数
<code>a = activation(z)</code>。在这个例子中，激活函数是 ReLU，即
<code>a = max(0, z)</code>。</li>
<li><strong>输出张量</strong> ：最终，每个输入样本被转换为一个 30
维的输出向量，所有样本组成的输出张量形状为
<code>[batch_size, 30]</code>。</li>
</ol>
<h5 id="举例说明">举例说明</h5>
<p>假设我们有一个输入张量 <code>s</code>，形状为
<code>[batch_size=2, input_dim=5]</code>：</p>
<p>我们定义一个具有 30 个神经元的全连接层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = tf.layers.dense(s, <span class="number">30</span>, activation=tf.nn.relu)</span><br></pre></td></tr></table></figure>
<p>输入张量 <code>s</code> 的形状是 <code>[2, 5]</code>，经过具有 30
个神经元的全连接层后，输出张量 <code>net</code> 的形状将是
<code>[2, 30]</code>。每个输入样本（2 个样本）被转换为一个 30
维的输出向量。</p>
<p>总之，具有 30 个神经元意味着这一层的输出维度是
30，每个输入样本将被转化为一个 30
维的向量。这有助于神经网络在不同层之间传递和处理信息，提取和学习更复杂的特征。</p>
<h3 id="tf.multiply">tf.multiply ：</h3>
<ul>
<li><code>tf.multiply</code> 是 TensorFlow
的一个操作，用于逐元素相乘。它的输入是两个张量，输出是这两个张量对应位置元素相乘的结果。</li>
</ul>
<h3 id="tf.get_collection">tf.get_collection</h3>
<p><code>tf.get_collection</code> 是 TensorFlow
中用于获取一个集合中的所有元素的函数。集合在 TensorFlow
中是一种组织和管理变量、操作等对象的方式，可以通过集合来方便地访问和操作这些对象。</p>
<h4 id="函数原型">函数原型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.get_collection(key, scope=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<h4 id="参数">参数</h4>
<ol type="1">
<li><p><strong><code>key</code></strong>：</p>
<ul>
<li>表示要获取的集合的键（名称），是一个字符串。</li>
<li>TensorFlow 中有一些预定义的集合键，例如
<code>tf.GraphKeys.GLOBAL_VARIABLES</code>、<code>tf.GraphKeys.TRAINABLE_VARIABLES</code>
等。</li>
</ul></li>
<li><p><strong><code>scope</code></strong>（可选）：</p>
<ul>
<li>表示要获取的集合中元素的作用域（scope）。如果指定了
<code>scope</code>，则只返回该作用域内的元素。</li>
</ul></li>
</ol>
<h4 id="返回值">返回值</h4>
<p>返回与 <code>key</code>
对应的集合中的所有元素，通常是一个列表。如果集合不存在，返回空列表。</p>
<h4 id="举例说明-1">举例说明</h4>
<p>假设我们在构建神经网络时，将一些变量添加到集合中，然后在训练或评估时需要获取这些变量，可以使用
<code>tf.get_collection</code>。</p>
<h5 id="示例代码">示例代码</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建变量并将其添加到集合中</span></span><br><span class="line">var1 = tf.Variable(<span class="number">1.0</span>, name=<span class="string">&#x27;var1&#x27;</span>)</span><br><span class="line">var2 = tf.Variable(<span class="number">2.0</span>, name=<span class="string">&#x27;var2&#x27;</span>)</span><br><span class="line">tf.add_to_collection(<span class="string">&#x27;my_collection&#x27;</span>, var1)</span><br><span class="line">tf.add_to_collection(<span class="string">&#x27;my_collection&#x27;</span>, var2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取集合中的所有变量</span></span><br><span class="line">collection_vars = tf.get_collection(<span class="string">&#x27;my_collection&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(collection_vars)</span><br></pre></td></tr></table></figure>
<h4 id="预定义集合键">预定义集合键</h4>
<p>TensorFlow 提供了一些常用的集合键，方便用户管理变量和操作：</p>
<ul>
<li><code>tf.GraphKeys.GLOBAL_VARIABLES</code>：所有全局变量。</li>
<li><code>tf.GraphKeys.TRAINABLE_VARIABLES</code>：所有可训练的变量。</li>
<li><code>tf.GraphKeys.SUMMARIES</code>：所有摘要操作。</li>
<li><code>tf.GraphKeys.UPDATE_OPS</code>：所有需要在训练时执行的更新操作。</li>
</ul>
<h4 id="在实际应用中的使用">在实际应用中的使用</h4>
<h5 id="获取所有可训练变量">获取所有可训练变量</h5>
<p>在训练神经网络时，我们通常需要获取所有可训练的变量，例如在优化器中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.01</span>)</span><br><span class="line">train_op = optimizer.minimize(loss, var_list=trainable_vars)</span><br></pre></td></tr></table></figure>
<h5 id="获取特定作用域内的变量">获取特定作用域内的变量</h5>
<p>我们还可以使用 <code>scope</code> 参数来获取特定作用域内的变量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;layer1&#x27;</span>):</span><br><span class="line">    var1 = tf.get_variable(<span class="string">&#x27;var1&#x27;</span>, shape=[<span class="number">10</span>])</span><br><span class="line">  </span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;layer2&#x27;</span>):</span><br><span class="line">    var2 = tf.get_variable(<span class="string">&#x27;var2&#x27;</span>, shape=[<span class="number">20</span>])</span><br><span class="line"></span><br><span class="line">layer1_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="string">&#x27;layer1&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(layer1_vars)  <span class="comment"># 只会打印 layer1 作用域内的变量</span></span><br></pre></td></tr></table></figure>
<h4 id="总结-4">总结</h4>
<p><code>tf.get_collection</code>
是一个强大的函数，允许用户根据集合键和作用域来获取 TensorFlow
图中的变量和操作。它在组织和管理复杂模型中的变量和操作时非常有用，尤其是在训练、保存和恢复模型时。通过合理使用集合和
<code>tf.get_collection</code>，可以使代码更加清晰和易于维护。</p>
<h3 id="tf.stop_gradient">tf.stop_gradient</h3>
<p><code>tf.stop_gradient</code> 是 TensorFlow
中用于阻止梯度反向传播的操作。它返回一个与输入张量具有相同内容的新张量，但在计算梯度时会被视为常量。换句话说，<code>tf.stop_gradient</code>
可以用来“截断”梯度的传播，这在某些情况下（例如实现某些特殊的梯度更新规则或避免某些变量的梯度更新）非常有用。</p>
<h4 id="函数原型-1">函数原型</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.stop_gradient(</span><br><span class="line">    <span class="built_in">input</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="参数-1">参数</h4>
<ul>
<li><strong><code>input</code></strong>：输入张量。</li>
<li><strong><code>name</code></strong>（可选）：操作名。</li>
</ul>
<h4 id="返回值-1">返回值</h4>
<p>返回一个与输入张量具有相同内容的新张量，但在反向传播过程中，该张量的梯度将被视为零。</p>
<h4 id="使用示例-1">使用示例</h4>
<h5 id="示例1简单示例">示例1：简单示例</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个变量和一个操作</span></span><br><span class="line">x = tf.Variable(<span class="number">2.0</span>, name=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 tf.stop_gradient 阻止梯度传播</span></span><br><span class="line">y_no_grad = tf.stop_gradient(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建损失函数</span></span><br><span class="line">loss = y_no_grad + x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算梯度</span></span><br><span class="line">gradients = tf.gradients(loss, [x])</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>y_no_grad</code> 是通过
<code>tf.stop_gradient</code> 创建的，因此在计算 <code>loss</code>
的梯度时，<code>y_no_grad</code> 不会对 <code>x</code>
的梯度产生影响。最终结果是 <code>gradients</code> 只包含 <code>x</code>
的梯度，即 1，而不是 <code>y = x * 2</code> 导致的 2。</p>
<h5 id="示例2在神经网络中的应用">示例2：在神经网络中的应用</h5>
<p>在一些强化学习算法中，我们可能希望通过停止梯度来实现某些特殊的策略。例如，在
DDPG 算法中，actor 网络的输出动作传递给 critic 网络，但我们不希望通过
critic 网络更新 actor 网络的参数，可以使用 <code>tf.stop_gradient</code>
实现这一点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一个 actor 网络输出动作 a</span></span><br><span class="line">a = actor_network(state)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们希望通过 critic 网络计算梯度，但不希望更新 actor 网络</span></span><br><span class="line">q_value = critic_network(tf.stop_gradient(a))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失并更新 critic 网络</span></span><br><span class="line">critic_loss = -tf.reduce_mean(q_value)</span><br><span class="line">critic_optimizer = tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">critic_train_op = critic_optimizer.minimize(critic_loss)</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>tf.stop_gradient(a)</code> 确保了
<code>critic_network</code> 的梯度不会影响 <code>actor_network</code>
的参数更新。</p>
<h4 id="总结-5">总结</h4>
<p><code>tf.stop_gradient</code>
是一个非常有用的操作，特别是在需要控制梯度传播路径或实现自定义梯度更新规则时。通过合理使用
<code>tf.stop_gradient</code>，可以更灵活地设计和训练复杂的模型。</p>
<h3 id="软替换代码">软替换代码</h3>
<p>代码理解:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.soft_replace = [tf.assign(t, (<span class="number">1</span> - self.replacement[<span class="string">&#x27;tau&#x27;</span>]) * t + self.replacement[<span class="string">&#x27;tau&#x27;</span>] * e)</span><br><span class="line">                     <span class="keyword">for</span> t, e <span class="keyword">in</span> <span class="built_in">zip</span>(self.t_params, self.e_params)]</span><br></pre></td></tr></table></figure>
<p>这一行代码的作用是实现软替换（soft
replacement），用来逐步更新目标网络的参数，使其接近评估网络的参数。在强化学习中，软替换是一种常见的参数更新策略，常用于深度确定性策略梯度（DDPG）等算法。</p>
<h4 id="让我们逐步解析这段代码">让我们逐步解析这段代码：</h4>
<ol type="1">
<li><p><strong><code>zip(self.t_params, self.e_params)</code></strong>：</p>
<ul>
<li><code>self.t_params</code> 包含目标网络的所有参数。</li>
<li><code>self.e_params</code> 包含评估网络的所有参数。</li>
<li><code>zip</code>
函数将这两个参数列表配对，使得每对包含一个目标网络的参数和一个评估网络的参数。</li>
</ul></li>
<li><p><strong>列表推导式</strong>：</p>
<ul>
<li>列表推导式用于生成一个包含多个 TensorFlow
操作的列表。在这里，每个操作都是一个 <code>tf.assign</code>
操作，用于更新目标网络的参数。</li>
</ul></li>
<li><p><strong><code>tf.assign</code></strong>：</p>
<ul>
<li><code>tf.assign</code> 是 TensorFlow
中用于更新变量值的操作。它接受两个参数：要更新的变量和新的值。</li>
<li><code>t</code> 是目标网络的参数，<code>e</code>
是评估网络的参数。</li>
</ul></li>
<li><p><strong>软更新公式</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">1</span> - self.replacement[<span class="string">&#x27;tau&#x27;</span>]) * t + self.replacement[<span class="string">&#x27;tau&#x27;</span>] * e</span><br></pre></td></tr></table></figure>
<ul>
<li><code>self.replacement['tau']</code>
是软更新的速率，通常是一个小值（如
0.01）。它控制了目标网络参数向评估网络参数靠拢的速度。</li>
<li><code>(1 - self.replacement['tau']) * t</code>
保留了目标网络参数的大部分值。</li>
<li><code>self.replacement['tau'] * e</code>
引入了一小部分评估网络参数的值。</li>
<li>通过这种加权平均的方式，目标网络参数逐步向评估网络参数靠拢，但不会立即完全相同。</li>
</ul></li>
<li><p><strong>生成软替换操作列表</strong>：</p>
<ul>
<li>对于每一对目标网络参数 <code>t</code> 和评估网络参数
<code>e</code>，生成一个 <code>tf.assign</code>
操作，将目标网络参数更新为加权平均后的值。</li>
<li><code>self.soft_replace</code> 最终是一个包含所有这些
<code>tf.assign</code> 操作的列表。</li>
</ul></li>
</ol>
<h4 id="软替换的具体作用">软替换的具体作用</h4>
<p>在强化学习算法中，目标网络和评估网络的更新策略对算法的稳定性至关重要。软替换通过逐步更新目标网络的参数，可以减少训练过程中的振荡和不稳定性。相对于硬替换（每隔一段时间直接将评估网络的参数复制到目标网络），软替换更为平滑和稳定，有助于提高算法的训练效果。</p>
<h4 id="示例-1">示例</h4>
<p>假设评估网络和目标网络的参数分别为 <code>e_params = [1, 2, 3]</code>
和 <code>t_params = [4, 5, 6]</code>，且
<code>tau = 0.1</code>，则软替换后的目标网络参数计算如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">new_t_params = [(<span class="number">1</span> - <span class="number">0.1</span>) * <span class="number">4</span> + <span class="number">0.1</span> * <span class="number">1</span>, (<span class="number">1</span> - <span class="number">0.1</span>) * <span class="number">5</span> + <span class="number">0.1</span> * <span class="number">2</span>, (<span class="number">1</span> - <span class="number">0.1</span>) * <span class="number">6</span> + <span class="number">0.1</span> * <span class="number">3</span>]</span><br><span class="line">              = [<span class="number">3.7</span>, <span class="number">4.7</span>, <span class="number">5.7</span>]</span><br></pre></td></tr></table></figure>
<p>每次更新后，目标网络的参数都会逐渐向评估网络的参数靠拢，但不会立即相同，从而实现平滑的参数更新。</p>
<h3 id="硬替换代码">硬替换代码</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">self.t_replace_counter = <span class="number">0</span></span><br><span class="line">            self.hard_replace = [tf.assign(t, e) <span class="keyword">for</span> t, e <span class="keyword">in</span> <span class="built_in">zip</span>(self.t_params, self.e_params)]</span><br></pre></td></tr></table></figure>
<h3 id="tf.gradients">tf.gradients</h3>
<p><code>tf.gradients</code> 是 TensorFlow
中用于计算梯度的函数。它的主要功能是计算某个或某些张量（<code>ys</code>）相对于另一些张量（<code>xs</code>）的导数。</p>
<p>其函数签名如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.gradients(ys, xs, grad_ys=<span class="literal">None</span>, name=<span class="string">&#x27;gradients&#x27;</span>, colocate_gradients_with_ops=<span class="literal">False</span>, gate_gradients=<span class="literal">False</span>, aggregation_method=<span class="literal">None</span>, stop_gradients=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>参数说明如下：</p>
<ul>
<li><code>ys</code>：表示要计算梯度的张量或张量列表。</li>
<li><code>xs</code>：表示要对哪些张量计算梯度的张量或张量列表。</li>
<li><code>grad_ys</code>（可选）：也是一个张量或张量列表，用于对
<code>ys</code> 中的每个元素进行加权。如果未提供，则默认为每个元素都是
1。</li>
<li><code>name</code>（可选）：操作的名称。</li>
<li><code>colocate_gradients_with_ops</code>（可选）：通常不需要设置。</li>
<li><code>gate_gradients</code>（可选）：通常不需要设置。</li>
<li><code>aggregation_method</code>（可选）：聚合方法，一般使用默认值即可。</li>
<li><code>stop_gradients</code>（可选）：是一个张量或张量列表，用于指定在反向传播中梯度停止的节点。这意味着这些节点的梯度不会再向后传播，就好像它们被明确地断开了一样。</li>
</ul>
<p>例如，如果 <code>ys</code> 是一个张量，<code>xs</code>
是一个张量列表，那么返回的是一个与 <code>xs</code>
长度相同的列表，其中每个元素是 <code>ys</code> 相对于 <code>xs</code>
中对应张量的导数。</p>
<p>如果 <code>ys</code> 和 <code>xs</code>
都是张量列表，那么它会计算每个 <code>ys</code> 中的张量相对于每个
<code>xs</code> 中的张量的导数，并将结果以适当的方式组合成一个列表。</p>
<p>下面是一个简单的示例，假设有两个变量 <code>w1</code> 和
<code>w2</code>，通过矩阵乘法等操作得到结果 <code>res</code>，然后使用
<code>tf.gradients</code> 计算 <code>res</code> 相对于 <code>w1</code>
和 <code>w2</code> 的梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(((<span class="number">1</span>, <span class="number">2</span>)))  </span><br><span class="line">w2 = tf.Variable(((<span class="number">3</span>, <span class="number">4</span>)))  </span><br><span class="line">res = tf.matmul(w1, ((<span class="number">2</span>), (<span class="number">1</span>))) + tf.matmul(w2, ((<span class="number">3</span>), (<span class="number">5</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 res 相对于 w1 和 w2 的梯度</span></span><br><span class="line">grads = tf.gradients((res,), (w1, w2))  </span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()  </span><br><span class="line">    re = sess.run(grads)  </span><br><span class="line">    <span class="built_in">print</span>(re) </span><br></pre></td></tr></table></figure>
<p>在上述示例中，<code>tf.gradients</code>
返回了一个包含两个梯度张量的列表，分别对应 <code>res</code> 相对于
<code>w1</code> 和 <code>w2</code> 的梯度。</p>
<p><code>stop_gradients</code> 参数的使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant((<span class="number">0.0</span>))  </span><br><span class="line">b = a * <span class="number">2</span>  </span><br><span class="line"><span class="comment"># 计算 a+b 相对于 a 和 b 的梯度，stop_gradients 指定为 (a, b)，表示 a 和 b 的梯度不会再向后传播</span></span><br><span class="line">pg = tf.gradients(a+b, (a, b), stop_gradients=(a, b))  </span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    res = sess.run(pg)  </span><br><span class="line">    <span class="built_in">print</span>(res) </span><br></pre></td></tr></table></figure>
<p>在这个例子中，由于使用了 <code>stop_gradients=(a, b)</code>，所以
<code>a</code> 和 <code>b</code>
的梯度在计算后不会再向后传播，得到的梯度结果为
<code>(1.0, 1.0)</code>，而不是通常情况下的
<code>(3.0, 1.0)</code>。</p>
<p>如何理解stop_gradients</p>
<p>具体来说，对于表达式
<code>pg = tf.gradients(a+b, (a, b), stop_gradients=(a, b))</code>，意味着在计算
<code>a+b</code> 相对于 <code>a</code> 和 <code>b</code>
的梯度时，<code>a</code> 和 <code>b</code>
本身的梯度不会再进一步向后传播。也就是说，<code>a</code>
的梯度不会影响到依赖于 <code>a</code> 的其他操作，<code>b</code>
的梯度也不会影响到依赖于 <code>b</code> 的其他操作。</p>
<p>这在一些情况下是有用的，例如当你不希望某些变量的梯度继续传播，或者想要模拟某些部分的梯度为固定值的情况。通过设置
<code>stop_gradients</code>，可以更灵活地控制梯度的计算和传播。</p>
<p>例如，如果 <code>a</code> 和 <code>b</code>
是神经网络中的某些中间节点，并且你不希望它们的梯度对网络的其他部分产生影响，就可以将它们放入
<code>stop_gradients</code> 中。这样，在反向传播时，就只会计算到
<code>a+b</code> 这一步，而不会将梯度继续传播到 <code>a</code> 和
<code>b</code> 所依赖的其他变量上。</p>
<p>举个简单的例子，假设有如下计算图：<code>c = a + b</code>，<code>d = c * x</code>，如果计算
<code>d</code> 相对于 <code>a</code> 的梯度，正常情况下梯度会从
<code>d</code> 传播到 <code>c</code>，再传播到
<code>a</code>。但如果使用
<code>stop_gradients=(a, b)</code>，那么梯度只会传播到
<code>c</code>，而不会再传播到 <code>a</code> 和 <code>b</code>，即
<code>a</code> 和 <code>b</code>
的梯度不会再向后传播。这样可以在一定程度上实现对梯度传播的控制和干预。</p>
<p>需要注意的是，<code>tf.stop_gradient</code>
是在构建图的过程中使用，用来指定停止链式法则的节点；而
<code>stop_gradients</code> 是在构建图之后使用，当程序运行碰到在
<code>stop_gradients</code>
中定义的节点时，均会停止链式法则，进而求得部分偏导。它们的作用都是阻止梯度的进一步传播，但使用的时机略有不同。</p>
<p><code>grad_ys</code> 参数用于对梯度进行加权，例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, shape=(<span class="number">3</span>))  </span><br><span class="line">w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, shape=(<span class="number">3</span>))  </span><br><span class="line">w3 = tf.get_variable(<span class="string">&#x27;w3&#x27;</span>, shape=(<span class="number">3</span>))  </span><br><span class="line">w4 = tf.get_variable(<span class="string">&#x27;w4&#x27;</span>, shape=(<span class="number">3</span>))  </span><br><span class="line">z1 = <span class="number">3</span>*w1 + <span class="number">2</span>*w2 + w3  </span><br><span class="line">z2 = -<span class="number">1</span>*w3 + w4</span><br><span class="line"></span><br><span class="line">grad_ys = (tf.convert_to_tensor((<span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>)), tf.convert_to_tensor((<span class="number">3.0</span>, <span class="number">2.0</span>, <span class="number">4.0</span>)))</span><br><span class="line">grads = tf.gradients((z1, z2), (w1, w2, w3, w4), grad_ys=grad_ys)  </span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()  </span><br><span class="line">    <span class="built_in">print</span>(sess.run(grads)) </span><br></pre></td></tr></table></figure>
<p>这里的 <code>grad_ys</code> 对 <code>z1</code> 和 <code>z2</code>
的梯度进行了加权，最终得到的梯度结果会相应地受到加权的影响。</p>
<p>在 TensorFlow 2.x 中也可以使用
<code>tf.gradients</code>，但通常更推荐使用 <code>tf.GradientTape</code>
来进行自动求导，它在使用上更加灵活和方便。不过，在某些情况下，仍然可能需要使用
<code>tf.gradients</code>，例如在一些特定的模型架构或代码结构中。但要注意将其放在计算图中使用。如果不在计算图中使用，就需要使用
<code>tf.GradientTape</code> 来替代。</p>
<h4 id="tf.gradients的输入和输出之间的形状的关系"><code>tf.gradients</code>的输入和输出之间的形状的关系</h4>
<p>在 TensorFlow 中，<code>tf.gradients()</code>
用于计算梯度。它接受求导的目标值 <code>ys</code> 和自变量
<code>xs</code>，返回的梯度值与 <code>ys</code> 和 <code>xs</code>
张量之间的形状关系如下：</p>
<hr>
<p><strong><code>tf.gradients()</code> 返回的结果是一个长度为
<code>len(xs)</code>
的张量列表</strong>。<strong>--------------这是重点</strong></p>
<p><strong>一般情况下，<code>tf.gradients(self.q, self.a)</code>
的返回值形状应该是与 <code>self.a</code> 的形状相同</strong></p>
<hr>
<p>如果 <code>ys</code> 是单个张量，那么每个返回的张量表示相应的
<code>xs</code> 元素对 <code>ys</code> 的梯度。如果 <code>ys</code>
是张量列表，那么返回的张量列表中的每个张量是所有 <code>ys</code>
张量对相应 <code>xs</code> 元素的梯度之和。</p>
<p>具体来说，假设 <code>ys = (y1, y2,..., ym)</code>
是一个张量列表，<code>xs = (x1, x2,..., xn)</code>
也是一个张量列表。那么返回的梯度列表中第 <code>i</code>
个张量表示的是所有 <code>y</code> 张量对 <code>xi</code>
的梯度之和，即：</p>
<p><span class="math inline">\(\frac{d(y1 + y2 +... +
ym)}{dx_i}\)</span></p>
<p>如果 <code>ys</code>
是单个张量，那么返回的列表中只有一个张量，其形状与 <code>xs</code>
中的元素形状相对应，表示该 <code>ys</code> 张量对每个 <code>xs</code>
元素的梯度。</p>
<p>例如，当 <code>ys = (y1, y2)</code>，<code>xs = (x1, x2, x3)</code>
时，返回的梯度结果为 <code>(grad1, grad2, grad3)</code>，其中：</p>
<ul>
<li><code>grad1</code> 表示的是 <span class="math inline">\(\frac{y1}{x1} + \frac{y2}{x1}\)</span></li>
<li><code>grad2</code> 表示的是 <span class="math inline">\(\frac{y1}{x2} + \frac{y2}{x2}\)</span></li>
<li><code>grad3</code> 表示的是 <span class="math inline">\(\frac{y1}{x3} + \frac{y2}{x3}\)</span></li>
</ul>
<p>另外，<code>tf.gradients()</code> 还有一个参数
<code>grad_ys</code>，它也是一个张量列表，用于对 <code>ys</code>
中的每个张量进行加权。如果提供了
<code>grad_ys</code>，则计算梯度时会将每个 <code>y</code> 张量乘以对应的
<code>grad_ys</code> 张量后再进行求导。</p>
<p>例如，在上述同样的 <code>ys</code> 和 <code>xs</code> 的情况下，如果
<code>grad_ys = (g1, g2)</code>，那么计算的梯度将变为：</p>
<ul>
<li><code>grad1</code> 表示的是 <span class="math inline">\(\frac{g1 *
y1}{x1} + \frac{g2 * y2}{x1}\)</span></li>
<li><code>grad2</code> 表示的是 <span class="math inline">\(\frac{g1 *
y1}{x2} + \frac{g2 * y2}{x2}\)</span></li>
<li><code>grad3</code> 表示的是 <span class="math inline">\(\frac{g1 *
y1}{x3} + \frac{g2 * y2}{x3}\)</span></li>
</ul>
<h4 id="如何理解-grad_ys">如何理解 <code>grad_ys</code></h4>
<p><code>grad_ys</code>（可选）是一个张量或张量列表，用于对
<code>ys</code> 中的每个元素进行加权。</p>
<p>当使用 <code>tf.gradients(ys, xs, grad_ys)</code> 时，如果
<code>grad_ys</code> 不为空，就需要使用梯度求导的链式法则来计算相对于
<code>xs</code> 的导数。</p>
<p>假设 <code>grad_ys = (grad_ys1, grad_ys2,..., grad_ysn)</code>，其中
<code>n</code> 是 <code>ys</code>
中元素的个数，<code>xs = (x1, x2,..., xm)</code>。那么对于
<code>xs</code> 中的每个元素 <code>xi</code>，其梯度计算方式为：</p>
<p>新的梯度
<code>new_grad(i) = ∂/∂xi = ∂/∂z1 * ∂z1/∂xi + ∂/∂z2 * ∂z2/∂xi +... + ∂/∂zn * ∂zn/∂xi</code>，其中
<code>z1, z2,..., zn</code> 是 <code>ys</code> 中的元素。</p>
<p>也就是说，<code>grad_ys</code> 中的每个张量会分别与对应的
<code>ys</code> 元素相乘，然后在计算相对于 <code>xs</code>
的梯度时，这些加权后的结果会参与到链式法则的计算中。</p>
<p>例如，在代码示例中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w1 = tf.get_variable(<span class="string">&#x27;w1&#x27;</span>, shape=(<span class="number">3</span>))</span><br><span class="line">w2 = tf.get_variable(<span class="string">&#x27;w2&#x27;</span>, shape=(<span class="number">3</span>))</span><br><span class="line">w3 = tf.get_variable(<span class="string">&#x27;w3&#x27;</span>, shape=(<span class="number">3</span>))</span><br><span class="line">w4 = tf.get_variable(<span class="string">&#x27;w4&#x27;</span>, shape=(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">z1 = <span class="number">3</span>*w1 + <span class="number">2</span>*w2 + w3</span><br><span class="line">z2 = -<span class="number">1</span>*w3 + w4</span><br><span class="line"></span><br><span class="line">grads = tf.gradients((z1, z2), (w1, w2, w3, w4), grad_ys=((-<span class="number">2.0</span>,-<span class="number">3.0</span>,-<span class="number">4.0</span>), (-<span class="number">2.0</span>,-<span class="number">3.0</span>,-<span class="number">4.0</span>)))</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    <span class="built_in">print</span>(sess.run(grads))</span><br></pre></td></tr></table></figure>
<p>如果不考虑 <code>grad_ys</code>，输出应该是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(array((3., 3., 3.), dtype=float32),</span><br><span class="line"> array((2., 2., 2.), dtype=float32),</span><br><span class="line"> array((1., 1., 1.), dtype=float32),</span><br><span class="line"> array((1., 1., 1.), dtype=float32))</span><br></pre></td></tr></table></figure>
<p>现在在权重参数
<code>grad_ys = ((-2.0,-3.0,-4.0), (-2.0,-3.0,-4.0))</code>
的加权下，输出实际为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(array((-6., -9., -12.), dtype=float32),</span><br><span class="line"> array((-4., -6., -8.), dtype=float32),</span><br><span class="line"> array((0., 0., 0.), dtype=float32),</span><br><span class="line"> array((1., 1., 1.), dtype=float32))</span><br></pre></td></tr></table></figure>
<p>具体计算过程为，对于 <code>w1</code> 的梯度，原来的梯度是
<code>3</code>（来自 <code>z1</code> 对 <code>w1</code>
的偏导），现在加权后为 <code>-2.0 * 3 = -6.0</code>；对于
<code>w2</code> 的梯度，原来的梯度是 <code>2</code>（来自
<code>z1</code> 对 <code>w2</code> 的偏导），加权后为
<code>-3.0 * 2 = -6.0</code>；对于 <code>w3</code> 的梯度，来自
<code>z1</code> 的部分为 <code>-2.0 * 1 = -2.0</code>，来自
<code>z2</code> 的部分为 <code>-2.0 * (-1) = 2.0</code>，相加得到
<code>0.0</code>。</p>
<p>这样，通过设置 <code>grad_ys</code>，可以对 <code>ys</code>
中每个元素的梯度进行不同的加权，从而更灵活地控制梯度的计算。在实际应用中，这可以根据具体的需求和模型架构来调整梯度的传播和计算方式。</p>
<h5 id="为什么需要加权">为什么需要加权</h5>
<p>在某些情况下，需要对梯度进行加权是为了在计算梯度时对不同的部分进行不同程度的强调或重视。</p>
<p>例如，在深度学习中，不同的输出或误差项可能对最终的结果有不同的重要性。通过为
<code>grad_ys</code>中的每个张量设置合适的权重，可以根据这些重要性的差异来调整梯度的计算。</p>
<p>具体来说，加权可以用于以下几种情况：</p>
<ol type="1">
<li><strong>强调重要的输出或误差</strong>：如果某些输出或误差对于模型的性能或目标更关键，那么可以给它们的梯度分配较大的权重，以便在反向传播过程中更显著地影响参数的调整。</li>
<li><strong>平衡不同规模或量级的因素</strong>：当不同的输出或误差具有不同的量级时，加权可以帮助平衡它们对梯度的贡献，避免较大量级的部分主导了梯度的计算。</li>
<li><strong>实现特定的优化目标</strong>：根据具体的优化目标或任务需求，设置特定的权重来引导模型的学习方向。</li>
<li><strong>处理多任务学习</strong>：在多任务学习场景中，不同的任务可能具有不同的重要性，通过加权梯度可以更好地协调不同任务之间的学习。</li>
</ol>
<p>例如，假设有两个输出 <code>y1</code>和
<code>y2</code>，对应的梯度分别为 <code>dy1/dx</code>和
<code>dy2/dx</code>。如果直接计算梯度之和，那么它们对最终梯度的贡献是相等的。但如果
<code>y1</code>相对更重要，就可以为
<code>dy1/dx</code>设置较大的权重，使得它在决定参数调整时起到更大的作用。</p>
<p>在 TensorFlow 中，通过设置
<code>grad_ys</code>参数来实现加权梯度计算。<code>grad_ys</code>是一个与
<code>ys</code>长度相同的张量列表，其中每个张量用于对相应的
<code>ys</code>元素的梯度进行加权。这样，在计算梯度时，就会考虑这些加权值，从而实现对不同部分梯度的不同程度的重视。</p>
<p>例如，在上述代码示例中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grad_ys = (tf.variable(((<span class="number">1</span>))), tf.variable(((<span class="number">2</span>))))</span><br><span class="line">grads = tf.gradients((res2a, res2b), (w1, w2), grad_ys=grad_ys)</span><br></pre></td></tr></table></figure>
<p>这里 <code>grad_ys</code>被设置为
<code>(tf.variable(((1))), tf.variable(((2))))</code>，表示在计算梯度时，<code>res2a</code>的梯度将乘以
1，而 <code>res2b</code>的梯度将乘以 2，从而实现了对
<code>res2b</code>的梯度相对更重视的效果。这样，在后续的反向传播和参数更新中，<code>res2b</code>的梯度将对参数调整产生更大的影响。</p>
<p>总的来说，加权梯度提供了一种灵活的方式，根据具体问题和需求，对不同部分的梯度进行定制化的处理，以更好地实现模型的学习和优化目标。但确定合适的权重通常需要对问题有深入的理解，并可能需要通过试验和调优来找到最优的设置。</p>
<h5 id="如何为-grad_ys-中的每个张量设置合适的权重">如何为
<code>grad_ys</code> 中的每个张量设置合适的权重？</h5>
<p>为
<code>grad_ys</code>中的每个张量设置合适的权重，需要根据具体的问题和模型需求来决定。设置权重的目的是为了在计算梯度时对不同的部分进行不同程度的加权。</p>
<p>一种常见的方法是根据对每个元素的重要性或关注度来分配权重。例如，如果某个元素对于最终结果的影响较大，或者你希望在梯度计算中更强调该元素的变化，那么可以为其分配较大的权重；反之，如果某个元素相对不那么重要，可以分配较小的权重。</p>
<p>另外，也可以基于一些先验知识或经验来设置权重。例如，在某些情况下，可能知道某些部分的梯度对于整体优化的重要性程度。</p>
<p>然而，确定最合适的权重通常需要通过试验和调优来找到。这可能涉及尝试不同的权重组合，并观察模型在训练或优化过程中的性能表现，以找到能够达到最佳效果的权重设置。</p>
<p>例如，在上述代码示例中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grad_ys = (tf.variable(((<span class="number">1</span>))), tf.variable(((<span class="number">2</span>))))</span><br><span class="line">grads = tf.gradients((res2a, res2b), (w1, w2), grad_ys=grad_ys)</span><br></pre></td></tr></table></figure>
<p>这里 <code>grad_ys</code>被设置为
<code>(tf.variable(((1))), tf.variable(((2))))</code>，表示在计算梯度时，<code>res2a</code>的梯度将乘以
1，而 <code>res2b</code>的梯度将乘以
2，从而实现了对不同部分梯度的不同加权。</p>
<p>具体到你的问题中，如何设置权重取决于你的具体任务和数据特点。你可能需要对问题有深入的理解，并结合一些实验和观察来找到合适的权重设置。这可能需要一些尝试和错误，以及对模型输出和性能的仔细分析。同时，也可以考虑使用一些自动调参的技术或算法来帮助找到最优的权重配置。</p>
<h3 id="np.hstack">np.hstack</h3>
<p><code>np.hstack</code> 是 NumPy
库中的一个函数，用于水平（按列）堆叠数组。</p>
<p>其函数签名通常为 <code>np.hstack(tup)</code> ，其中 <code>tup</code>
是一个元组，包含要堆叠的数组。</p>
<p>例如，如果有两个数组 <code>a = np.array([1, 2, 3])</code> 和
<code>b = np.array([4, 5, 6])</code> ，使用
<code>np.hstack((a, b))</code> 将会得到一个新的数组
<code>[1, 2, 3, 4, 5, 6]</code> 。</p>
<p>再比如，如果 <code>a = np.array([[1, 2], [3, 4]])</code>
，<code>b = np.array([[5, 6], [7, 8]])</code> ，那么
<code>np.hstack((a, b))</code> 的结果是
<code>[[1, 2, 5, 6], [3, 4, 7, 8]]</code> 。</p>
<p><code>np.hstack</code>
主要用于将多个数组在水平方向上拼接成一个新的数组，前提是参与拼接的数组在除了要拼接的维度（这里是列维度）之外的其他维度上形状要匹配。</p>
<h3 id="td-error">td error</h3>
<p>TD error 即时间差分误差（Temporal Difference
error），它是强化学习中一个重要的概念，用于衡量当前估计值与目标值之间的差异。</p>
<p>其定义为：当前时刻的状态值或动作值的估计与基于即时回报和下一时刻状态值或动作值的估计所构成的目标值之间的差值。</p>
<p>例如，在某个时刻 t，TD error 可以表示为：<span class="math inline">\(\delta_t = V_t(s_t) - (r_{t+1} + \gamma
V_t(s_{t+1}))\)</span> 或者 <span class="math inline">\(\delta_t =
Q_t(s_t, a_t) - (r_{t+1} + \gamma Q_t(s_{t+1}, a_{t+1}))\)</span> ，其中
<span class="math inline">\(V_t(s_t)\)</span> 表示 t 时刻对状态 <span class="math inline">\(s_t\)</span> 的状态值估计，<span class="math inline">\(Q_t(s_t, a_t)\)</span> 表示 t 时刻对状态 <span class="math inline">\(s_t\)</span> 采取动作 <span class="math inline">\(a_t\)</span> 的动作值估计，<span class="math inline">\(r_{t+1}\)</span> 是即时回报，<span class="math inline">\(\gamma\)</span> 是折扣系数，<span class="math inline">\(V_t(s_{t+1})\)</span> 和 <span class="math inline">\(Q_t(s_{t+1}, a_{t+1})\)</span>
分别是下一时刻的状态值估计和动作值估计。</p>
<p>TD error 的主要作用是指导强化学习算法的学习和更新过程。通过计算 TD
error，算法可以了解当前的估计与实际情况之间的差距，并据此调整模型的参数，以使得估计值更接近真实值。较小的
TD error 表示当前的估计较为准确，而较大的 TD error
则意味着模型需要进行更大的调整。</p>
<p>在强化学习中，一些算法（如 Q-learning、SARSA 等）会使用 TD error
来更新价值函数或策略，以逐步优化智能体的行为，使其能够在环境中获得更好的累积回报。</p>
<p>TD error
具有一些优点，例如它可以在没有完整状态序列的情况下进行学习，能够更快速灵活地更新状态或动作的价值估计。然而，它得到的价值是一种有偏估计，但其方差通常比其他方法（如蒙特卡洛方法）得到的方差要低。</p>
<p>不同的强化学习场景和算法中，TD error
的具体形式和应用可能会有所不同，但总体上都是用于衡量估计值与目标值的差异，以推动学习和优化的进行。如果在强化学习训练中遇到
TD error
值很大的情况，可能的原因包括网络架构不够复杂、训练样本数量不足、学习率过大或没有使用足够多的经验回放等。可以尝试使用更复杂的网络架构、增加训练数据、调低学习率、增加经验回放或使用更多的超参数调整方法来解决。</p>
<h3 id="tf.distributions.normal">tf.distributions.Normal</h3>
<p><code>tf.distributions.Normal</code> 是 TensorFlow
中用于定义正态分布（高斯分布）的类。</p>
<p>它的初始化函数 <code>__init__</code> 的参数如下：</p>
<ul>
<li><code>loc</code>：表示正态分布的均值（μ），是一个浮点型张量。</li>
<li><code>scale</code>：表示正态分布的标准差（σ），也是一个浮点型张量，且必须包含只正数值。</li>
<li><code>validate_args</code>：一个 Python 布尔值，默认为
<code>False</code>。当为 <code>True</code> 时，会进行参数验证。</li>
<li><code>allow_nan_stats</code>：一个 Python 布尔值，默认为
<code>True</code>，用于描述在统计量未定义时的行为。</li>
</ul>
<p>通过创建 <code>tf.distributions.Normal</code>
对象，可以进行与正态分布相关的操作，例如采样、计算概率、计算熵、交叉熵、KL
散度等。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow_probability <span class="keyword">as</span> tfp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个均值为 0，标准差为 1 的正态分布</span></span><br><span class="line">dist = tfp.distributions.Normal(loc=<span class="number">0.</span>, scale=<span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从该正态分布中采样</span></span><br><span class="line">samples = dist.sample((<span class="number">3</span>,)) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算某个值的概率密度函数（pdf）值</span></span><br><span class="line">pdf_value = dist.prob(<span class="number">0.5</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算累积分布函数（cdf）值</span></span><br><span class="line">cdf_value = dist.cdf(<span class="number">0.5</span>) </span><br></pre></td></tr></table></figure>
<p>在正态分布中，概率密度函数（pdf）的公式为：</p>
<p><span class="math display">\[
pdf(x; \mu, \sigma) = \frac{exp(-0.5(x - \mu)^2 / \sigma^2)}{\sqrt{(2\pi
\sigma^2)}}
\]</span></p>
<p>其中，<code>loc = mu</code> 是均值，<code>scale = sigma</code>
是标准差，<code>z</code> 是归一化常数。</p>
<p>正态分布是位置-尺度族的一员，也就是说，可以通过以下方式构建：</p>
<p><span class="math display">\[
x \sim Normal(loc=0, scale=1)
\]</span></p>
<p><span class="math display">\[
y = loc + scale * x
\]</span></p>
<p><code>tf.distributions.Normal</code> 类中还提供了其他一些方法，如
<code>covariance</code>（协方差）、<code>cross_entropy</code>（交叉熵）、<code>entropy</code>（熵）、<code>log_cdf</code>（对数累积分布函数）、<code>log_prob</code>（对数概率）、<code>log_survival_function</code>（对数生存函数）、<code>mean</code>（均值）、<code>mode</code>（众数）、<code>stddev</code>（标准差）、<code>variance</code>（方差）等，这些方法可以方便地用于各种与正态分布相关的计算和操作。具体使用方法可以参考
TensorFlow 的官方文档。</p>
<p>这些方法使得在 TensorFlow
中处理正态分布变得更加方便和高效，尤其是在涉及到概率计算、随机采样以及与其他分布进行比较和组合的任务中。例如，在一些机器学习和深度学习模型中，可能需要使用正态分布来初始化参数、生成随机噪声或者构建概率模型等。通过
<code>tf.distributions.Normal</code> 类，可以以一种与 TensorFlow
计算图兼容的方式进行这些操作，从而更好地利用 TensorFlow
的自动微分和优化功能。同时，还可以方便地处理批量数据（即同时处理多个正态分布，每个分布具有不同的参数），这在处理大规模数据或多任务学习等场景中非常有用。</p>
<h3 id="tf.clip_by_value">tf.clip_by_value</h3>
<p><code>tf.clip_by_value</code>是 TensorFlow
中的一个函数，用于将张量中的数值限制在指定的范围内。</p>
<p>它的语法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.clip_by_value(t, clip_value_min, clip_value_max, name=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>其中：</p>
<ul>
<li><code>t</code>：要进行裁剪的张量。</li>
<li><code>clip_value_min</code>：裁剪的最小值。如果张量中的值小于这个最小值，它们将被设置为这个最小值。</li>
<li><code>clip_value_max</code>：裁剪的最大值。如果张量中的值大于这个最大值，它们将被设置为这个最大值。</li>
<li><code>name</code>（可选）：操作的名称。</li>
</ul>
<p>例如，如果有一个张量 <code>t</code>，使用
<code>tf.clip_by_value</code>可以将其值限制在 <code>2</code>和
<code>5</code>之间：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">t = tf.constant(((<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>), (<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>)))</span><br><span class="line">t2 = tf.clip_by_value(t, <span class="number">2.5</span>, <span class="number">4.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="built_in">print</span>(sess.run(t2)) </span><br></pre></td></tr></table></figure>
<p>输出结果将是：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">((2.5, 2.5, 3.0), (4.0, 4.5, 4.5))</span><br></pre></td></tr></table></figure>
<p>在上述例子中，原始张量 <code>t</code>中的值小于
<code>2.5</code>的被替换为 <code>2.5</code>，大于
<code>4.5</code>的被替换为 <code>4.5</code>。</p>
<p>需要注意的是，<code>clip_value_min</code>需要小于或等于
<code>clip_value_max</code>，以获得正确的结果。如果输入的张量
<code>t</code>的元素类型是整数，而 <code>clip_value_min</code>或
<code>clip_value_max</code>是浮点数，需要先将整数张量转换为浮点数，否则可能会引发类型错误。此外，<code>clip_value_min</code>和
<code>clip_value_max</code>可以是标量张量，也可以是能广播到张量
<code>t</code>形状的张量。</p>
<h3 id="torch.nn.module">torch.nn.Module</h3>
<p><code>torch.nn.Module</code> 是 PyTorch
中所有神经网络模块的基类。它提供了许多用于构建和训练神经网络的基本功能。以下是一些关键点：</p>
<ol type="1">
<li><p><strong>模型定义</strong>：<code>torch.nn.Module</code>
是所有神经网络模块的基类，因此你可以通过继承它来定义自己的神经网络模型。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(<span class="number">10</span>, <span class="number">1</span>)  <span class="comment"># 定义一个全连接层</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></li>
<li><p><strong>参数管理</strong>：<code>torch.nn.Module</code>
自动管理模型的所有参数。当你定义一个神经网络层（如
<code>nn.Linear</code>、<code>nn.Conv2d</code>
等）时，这些层会自动添加到模型的参数列表中。例如，在上面的
<code>MyModel</code> 中，<code>self.fc</code>
是一个线性层，它的参数（权重和偏置）会被自动添加到模型中。</p></li>
<li><p><strong>前向传播</strong>：你需要定义一个 <code>forward</code>
方法来指定数据如何通过模型进行前向传播。例如，在上面的
<code>MyModel</code> 中，<code>forward</code> 方法将输入 <code>x</code>
传递给线性层 <code>self.fc</code>。</p></li>
<li><p><strong>训练和评估</strong>：<code>torch.nn.Module</code>
提供了一些方法来管理模型的训练和评估过程，例如
<code>train()</code>、<code>eval()</code>、<code>zero_grad()</code>、<code>step()</code>
等。</p></li>
<li><p><strong>保存和加载</strong>：<code>torch.nn.Module</code> 提供了
<code>save</code> 和 <code>load_state_dict</code>
方法来保存和加载模型的参数。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = MyModel()</span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model.pth&#x27;</span>)</span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;model.pth&#x27;</span>))</span><br></pre></td></tr></table></figure></li>
<li><p><strong>子模块</strong>：<code>torch.nn.Module</code>
可以包含其他子模块，这些子模块也可以是 <code>torch.nn.Module</code>
的实例。例如，一个复杂的神经网络可能由多个层和子网络组成。</p></li>
</ol>
<p>总的来说，<code>torch.nn.Module</code> 是 PyTorch
中构建和训练神经网络的核心类，它提供了许多方便的功能来管理模型参数、前向传播、训练和评估等。</p>
<h3 id="abc.abc">abc.ABC</h3>
<p>#abc.ABC 是 Python 的抽象基类，用于定义抽象方法。</p>
<p>在 Python 中，<code>abc.ABC</code>类是用于实现抽象基类（Abstract Base
Class）的。</p>
<p>抽象基类主要用于定义一组必须由其子类实现的方法。通过从
<code>abc.ABC</code>类继承，您可以创建一个抽象基类，并在其中定义抽象方法。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> abc</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyAbstractClass</span>(abc.ABC):</span><br><span class="line"><span class="meta">    @abc.abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">abstract_method</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ConcreteClass</span>(<span class="title class_ inherited__">MyAbstractClass</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">abstract_method</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Implemented abstract method&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>在上述示例中，<code>MyAbstractClass</code>是一个抽象基类，它定义了一个抽象方法
<code>abstract_method</code>。<code>ConcreteClass</code>是
<code>MyAbstractClass</code>的子类，并且实现了这个抽象方法。</p>
<p>使用抽象基类的好处在于，它可以强制子类遵循一定的接口规范，有助于提高代码的可读性、可维护性和可扩展性。例如，在一个大型项目中，如果多个类都需要实现某些特定的方法，通过定义一个抽象基类，可以确保这些方法在所有相关的子类中都得到正确实现。</p>
<h3 id="property"><code>@property</code></h3>
<p><code>@property</code> 是 Python
的装饰器语法，用于将一个方法转换为只读属性。这意味着你可以像访问属性一样访问这个方法，而不需要调用它。这对于创建只读属性或需要计算属性值的方法非常有用。</p>
<p>下面是一个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Circle</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, radius</span>):</span><br><span class="line">        self._radius = radius</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">radius</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._radius</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">diameter</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self._radius * <span class="number">2</span></span><br><span class="line"></span><br><span class="line">c = Circle(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(c.radius)  <span class="comment"># 输出: 5</span></span><br><span class="line"><span class="built_in">print</span>(c.diameter)  <span class="comment"># 输出: 10</span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>radius</code> 和 <code>diameter</code>
都是只读属性。<code>radius</code> 属性直接返回 <code>_radius</code>
属性的值，而 <code>diameter</code> 属性计算并返回直径的值。</p>
<p>使用 <code>@property</code>
装饰器的好处是，它允许你保持属性访问的一致性，同时提供额外的功能。例如，你可以使用
<code>@property</code> 装饰器来定义一个计算属性，或者使用
<code>@property</code> 和 <code>@radius.setter</code>
装饰器来定义一个可写的属性。</p>
<h3 id="import-gym">import gym</h3>
<p><code>import gym</code> 通常用于导入 OpenAI Gym 库。</p>
<p>OpenAI Gym
是一个用于开发和比较强化学习算法的工具包。它提供了各种各样的环境，例如经典控制问题（如
CartPole、MountainCar 等）、Atari 游戏等。</p>
<p>通过导入 Gym
库，您可以创建这些环境，并与它们进行交互，以测试和开发强化学习算法。</p>
<p>例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)</span><br><span class="line">observation = env.reset()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    env.render()</span><br><span class="line">    action = env.action_space.sample()  <span class="comment"># 随机选择一个动作</span></span><br><span class="line">    observation, reward, done, info = env.step(action)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> done:</span><br><span class="line">        observation = env.reset()</span><br><span class="line"></span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure>
<p>在上述代码中，首先创建了 <code>CartPole-v1</code>
环境，然后进行了一系列的交互操作，包括获取初始观察值、采取动作、获取新的观察值和奖励等。</p>
<p>不同的环境具有不同的状态、动作空间和奖励机制，您可以根据具体的问题和需求选择合适的环境进行强化学习算法的开发和实验。</p>
<h3 id="np.clip">np.clip</h3>
<p>“np.clip”通常是在 Python
编程中使用的一个函数。它用于将数组中的值限制在给定的区间范围内，以达到对数值进行裁剪或限制的目的。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz">ALTNT</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/">http://blog.705553939.xyz/2024/06/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/img/altnt.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/06/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5/" title="强化学习相关概念"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">强化学习相关概念</div></div></a></div><div class="next-post pull-right"><a href="/2024/06/22/%E8%B5%84%E6%96%99/%E5%91%BD%E4%BB%A4/" title=""><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info"></div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/altnt.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ALTNT</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">20</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">9</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ALTNT"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%86%E7%B1%BB"><span class="toc-number">1.</span> <span class="toc-text">机器学习分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.</span> <span class="toc-text">回归问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BB%BA%E7%AB%8B"><span class="toc-number">1.2.</span> <span class="toc-text">回归模型的建立</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%E7%A1%AE%E5%AE%9A%E7%94%A8%E4%BA%8E%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.2.1.</span> <span class="toc-text">第一步：确定用于回归任务的模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%E4%BD%BF%E7%94%A8%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E6%9D%A5%E8%A1%A1%E9%87%8F%E8%BF%99%E4%BA%9B%E5%A4%87%E9%80%89%E5%87%BD%E6%95%B0%E7%9A%84%E5%A5%BD%E5%9D%8F%E7%A8%8B%E5%BA%A6%E7%A1%AE%E5%AE%9A%E5%8F%82%E6%95%B0"><span class="toc-number">1.2.2.</span> <span class="toc-text">第二步：使用训练数据来衡量这些备选函数的好坏程度（确定参数）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%E6%A0%B9%E6%8D%AE%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%80%89%E5%87%BA%E6%8B%9F%E5%90%88%E6%9C%80%E5%A5%BD%E7%9A%84%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%9C%80%E7%BB%88%E7%9A%84%E6%8B%9F%E5%90%88%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.3.</span> <span class="toc-text">第三步：根据训练数据选出拟合最好的函数，作为最终的拟合函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="toc-number">2.1.</span> <span class="toc-text">预测结果分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">2.2.</span> <span class="toc-text">正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%81%97%E7%95%99%E9%97%AE%E9%A2%98"><span class="toc-number">2.3.</span> <span class="toc-text">遗留问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="toc-number">2.4.</span> <span class="toc-text">梯度下降算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%AD%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7tips"><span class="toc-number">2.4.1.</span> <span class="toc-text">梯度下降中常用技巧（Tips）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%80%E8%B0%83%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">2.4.1.1.</span> <span class="toc-text">一、调整学习率</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#adagrad%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">2.4.1.1.1.</span> <span class="toc-text">AdaGrad优化器</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BA%8C%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8Dsgd"><span class="toc-number">2.4.1.2.</span> <span class="toc-text">二、随机梯度下降（SGD）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%89%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BEfeature-scaling"><span class="toc-number">2.4.1.3.</span> <span class="toc-text">三、特征缩放（Feature Scaling）</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%8C%BA%E5%88%AB%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">3.</span> <span class="toc-text">梯度上升和梯度下降的区别是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="toc-number">4.</span> <span class="toc-text">偏差和方差</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E5%AE%9A%E4%B9%89"><span class="toc-number">4.1.</span> <span class="toc-text">数学定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E4%B8%8E%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-number">4.2.</span> <span class="toc-text">偏差-方差与模型复杂度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95"><span class="toc-number">4.3.</span> <span class="toc-text">调整方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#k-%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">4.4.</span> <span class="toc-text">K-折交叉验证</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#softmax"><span class="toc-number">5.</span> <span class="toc-text">softmax</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E7%94%A8%E9%80%94"><span class="toc-number">5.1.</span> <span class="toc-text">主要用途:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="toc-number">6.</span> <span class="toc-text">池化层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">6.1.</span> <span class="toc-text">池化层的作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#max-pooling"><span class="toc-number">6.2.</span> <span class="toc-text">Max pooling:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">7.</span> <span class="toc-text">损失函数:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">7.1.</span> <span class="toc-text">常见的损失函数:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0%E8%AE%A1%E7%AE%97"><span class="toc-number">7.2.</span> <span class="toc-text">损失函数的导数计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">8.</span> <span class="toc-text">激活函数:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">8.1.</span> <span class="toc-text">常见的激活函数：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">9.</span> <span class="toc-text">学习率</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A5%E4%B8%8B%E6%98%AF%E4%B8%80%E4%BA%9B%E5%B8%B8%E8%A7%81%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%B0%83%E6%95%B4%E6%96%B9%E6%B3%95"><span class="toc-number">9.1.</span> <span class="toc-text">以下是一些常见的学习率调整方法：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#optimizer%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">10.</span> <span class="toc-text">optimizer的概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90"><span class="toc-number">11.</span> <span class="toc-text">pytorch代码分析:</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.one_hot"><span class="toc-number">11.1.</span> <span class="toc-text">tf.one_hot</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self.critic_net.parameters"><span class="toc-number">11.2.</span> <span class="toc-text">self.critic_net.parameters()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self.optimizer.zero_grad"><span class="toc-number">11.3.</span> <span class="toc-text">self.optimizer.zero_grad()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loss.backwardretain_graphtrue"><span class="toc-number">11.4.</span> <span class="toc-text">loss.backward(retain_graph&#x3D;True)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6%E9%9C%80%E8%A6%81%E5%A4%9A%E6%AC%A1%E4%BD%BF%E7%94%A8%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">11.4.1.</span> <span class="toc-text">如何判断是否需要多次使用计算图？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.reduce_mean"><span class="toc-number">11.5.</span> <span class="toc-text">tf.reduce_mean</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.placeholder"><span class="toc-number">11.6.</span> <span class="toc-text">tf.placeholder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.variable_scope"><span class="toc-number">11.7.</span> <span class="toc-text">tf.variable_scope</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-tf.variable_scope"><span class="toc-number">11.7.1.</span> <span class="toc-text">1. 什么是
tf.variable_scope？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8-tf.variable_scope"><span class="toc-number">11.7.2.</span> <span class="toc-text">2. 为什么使用
tf.variable_scope？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-tf.variable_scope"><span class="toc-number">11.7.3.</span> <span class="toc-text">3. 如何使用
tf.variable_scope？</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-1%E5%88%9B%E5%BB%BA%E5%8F%98%E9%87%8F%E4%BD%9C%E7%94%A8%E5%9F%9F"><span class="toc-number">11.7.3.1.</span> <span class="toc-text">示例 1：创建变量作用域</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-2%E5%A4%8D%E7%94%A8%E5%8F%98%E9%87%8F"><span class="toc-number">11.7.3.2.</span> <span class="toc-text">示例 2：复用变量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#tf.get_variable-%E5%92%8C-tf.variable-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">11.7.3.3.</span> <span class="toc-text">4.
tf.get_variable 和 tf.Variable 的区别</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-3%E4%BD%BF%E7%94%A8-tf.get_variable-%E5%92%8C-tf.variable"><span class="toc-number">11.7.3.4.</span> <span class="toc-text">示例 3：使用
tf.get_variable 和 tf.Variable</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">11.7.4.</span> <span class="toc-text">5. 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.session"><span class="toc-number">11.8.</span> <span class="toc-text">tf.Session</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-tf.session"><span class="toc-number">11.8.1.</span> <span class="toc-text">1. 什么是 tf.Session？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-tf.session"><span class="toc-number">11.8.2.</span> <span class="toc-text">2. 为什么需要
tf.Session？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-tf.session"><span class="toc-number">11.8.3.</span> <span class="toc-text">3. 如何使用 tf.Session？</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-1%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-number">11.8.3.1.</span> <span class="toc-text">示例 1：基本使用</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-2%E4%BD%BF%E7%94%A8-tf.placeholder-%E5%92%8C-feed_dict"><span class="toc-number">11.8.3.2.</span> <span class="toc-text">示例 2：使用
tf.placeholder 和 feed_dict</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-3%E7%AE%A1%E7%90%86%E5%8F%98%E9%87%8F"><span class="toc-number">11.8.3.3.</span> <span class="toc-text">示例 3：管理变量</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tensorflow-2.x-%E4%B8%AD%E7%9A%84%E5%8F%98%E5%8C%96"><span class="toc-number">11.8.4.</span> <span class="toc-text">4. TensorFlow 2.x 中的变化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-number">11.8.5.</span> <span class="toc-text">5. 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.random_normal_initializer"><span class="toc-number">11.9.</span> <span class="toc-text">tf.random_normal_initializer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86"><span class="toc-number">11.9.1.</span> <span class="toc-text">背景知识</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%E8%AF%B4%E6%98%8E"><span class="toc-number">11.9.2.</span> <span class="toc-text">实例说明</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E8%BF%99%E4%B8%AA%E7%A4%BA%E4%BE%8B"><span class="toc-number">11.9.3.</span> <span class="toc-text">解释这个示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.compat.v1.global_variables_initializer"><span class="toc-number">11.10.</span> <span class="toc-text">tf.compat.v1.global_variables_initializer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%9C%E7%94%A8"><span class="toc-number">11.10.1.</span> <span class="toc-text">作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%98%E9%87%8F%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-number">11.10.2.</span> <span class="toc-text">变量初始化的重要性</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="toc-number">11.11.</span> <span class="toc-text">使用示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E8%BF%99%E4%B8%AA%E7%A4%BA%E4%BE%8B-1"><span class="toc-number">11.12.</span> <span class="toc-text">解释这个示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tensorflow-2.x-%E7%9A%84%E5%8F%98%E5%8C%96"><span class="toc-number">11.13.</span> <span class="toc-text">TensorFlow 2.x 的变化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-2"><span class="toc-number">11.14.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.constant_initializer"><span class="toc-number">11.15.</span> <span class="toc-text">tf.constant_initializer</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">11.15.1.</span> <span class="toc-text">使用方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B"><span class="toc-number">11.15.2.</span> <span class="toc-text">示例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E9%87%8A%E8%BF%99%E4%B8%AA%E7%A4%BA%E4%BE%8B-2"><span class="toc-number">11.15.3.</span> <span class="toc-text">解释这个示例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">11.15.4.</span> <span class="toc-text">应用场景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tensorflow-2.x-%E7%9A%84%E5%8F%98%E5%8C%96-1"><span class="toc-number">11.15.5.</span> <span class="toc-text">TensorFlow 2.x 的变化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-3"><span class="toc-number">11.15.6.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.layers.dense"><span class="toc-number">11.16.</span> <span class="toc-text">tf.layers.dense</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8A%E9%9D%A230-%E4%B8%AA%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E5%90%AB%E4%B9%89"><span class="toc-number">11.16.1.</span> <span class="toc-text">上面30 个神经元的含义</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4"><span class="toc-number">11.16.1.1.</span> <span class="toc-text">详细步骤</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E"><span class="toc-number">11.16.1.2.</span> <span class="toc-text">举例说明</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.multiply"><span class="toc-number">11.17.</span> <span class="toc-text">tf.multiply ：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.get_collection"><span class="toc-number">11.18.</span> <span class="toc-text">tf.get_collection</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E5%8E%9F%E5%9E%8B"><span class="toc-number">11.18.1.</span> <span class="toc-text">函数原型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0"><span class="toc-number">11.18.2.</span> <span class="toc-text">参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%94%E5%9B%9E%E5%80%BC"><span class="toc-number">11.18.3.</span> <span class="toc-text">返回值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E-1"><span class="toc-number">11.18.4.</span> <span class="toc-text">举例说明</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81"><span class="toc-number">11.18.4.1.</span> <span class="toc-text">示例代码</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E5%AE%9A%E4%B9%89%E9%9B%86%E5%90%88%E9%94%AE"><span class="toc-number">11.18.5.</span> <span class="toc-text">预定义集合键</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9C%A8%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">11.18.6.</span> <span class="toc-text">在实际应用中的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%89%80%E6%9C%89%E5%8F%AF%E8%AE%AD%E7%BB%83%E5%8F%98%E9%87%8F"><span class="toc-number">11.18.6.1.</span> <span class="toc-text">获取所有可训练变量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E7%89%B9%E5%AE%9A%E4%BD%9C%E7%94%A8%E5%9F%9F%E5%86%85%E7%9A%84%E5%8F%98%E9%87%8F"><span class="toc-number">11.18.6.2.</span> <span class="toc-text">获取特定作用域内的变量</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-4"><span class="toc-number">11.18.7.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.stop_gradient"><span class="toc-number">11.19.</span> <span class="toc-text">tf.stop_gradient</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E5%8E%9F%E5%9E%8B-1"><span class="toc-number">11.19.1.</span> <span class="toc-text">函数原型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%82%E6%95%B0-1"><span class="toc-number">11.19.2.</span> <span class="toc-text">参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%94%E5%9B%9E%E5%80%BC-1"><span class="toc-number">11.19.3.</span> <span class="toc-text">返回值</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B-1"><span class="toc-number">11.19.4.</span> <span class="toc-text">使用示例</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B1%E7%AE%80%E5%8D%95%E7%A4%BA%E4%BE%8B"><span class="toc-number">11.19.4.1.</span> <span class="toc-text">示例1：简单示例</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B2%E5%9C%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">11.19.4.2.</span> <span class="toc-text">示例2：在神经网络中的应用</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-5"><span class="toc-number">11.19.5.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BD%AF%E6%9B%BF%E6%8D%A2%E4%BB%A3%E7%A0%81"><span class="toc-number">11.20.</span> <span class="toc-text">软替换代码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A9%E6%88%91%E4%BB%AC%E9%80%90%E6%AD%A5%E8%A7%A3%E6%9E%90%E8%BF%99%E6%AE%B5%E4%BB%A3%E7%A0%81"><span class="toc-number">11.20.1.</span> <span class="toc-text">让我们逐步解析这段代码：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BD%AF%E6%9B%BF%E6%8D%A2%E7%9A%84%E5%85%B7%E4%BD%93%E4%BD%9C%E7%94%A8"><span class="toc-number">11.20.2.</span> <span class="toc-text">软替换的具体作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B-1"><span class="toc-number">11.20.3.</span> <span class="toc-text">示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A1%AC%E6%9B%BF%E6%8D%A2%E4%BB%A3%E7%A0%81"><span class="toc-number">11.21.</span> <span class="toc-text">硬替换代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.gradients"><span class="toc-number">11.22.</span> <span class="toc-text">tf.gradients</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tf.gradients%E7%9A%84%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA%E4%B9%8B%E9%97%B4%E7%9A%84%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">11.22.1.</span> <span class="toc-text">tf.gradients的输入和输出之间的形状的关系</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3-grad_ys"><span class="toc-number">11.22.2.</span> <span class="toc-text">如何理解 grad_ys</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%8A%A0%E6%9D%83"><span class="toc-number">11.22.2.1.</span> <span class="toc-text">为什么需要加权</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%B8%BA-grad_ys-%E4%B8%AD%E7%9A%84%E6%AF%8F%E4%B8%AA%E5%BC%A0%E9%87%8F%E8%AE%BE%E7%BD%AE%E5%90%88%E9%80%82%E7%9A%84%E6%9D%83%E9%87%8D"><span class="toc-number">11.22.2.2.</span> <span class="toc-text">如何为
grad_ys 中的每个张量设置合适的权重？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#np.hstack"><span class="toc-number">11.23.</span> <span class="toc-text">np.hstack</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#td-error"><span class="toc-number">11.24.</span> <span class="toc-text">td error</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.distributions.normal"><span class="toc-number">11.25.</span> <span class="toc-text">tf.distributions.Normal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tf.clip_by_value"><span class="toc-number">11.26.</span> <span class="toc-text">tf.clip_by_value</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch.nn.module"><span class="toc-number">11.27.</span> <span class="toc-text">torch.nn.Module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#abc.abc"><span class="toc-number">11.28.</span> <span class="toc-text">abc.ABC</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#property"><span class="toc-number">11.29.</span> <span class="toc-text">@property</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#import-gym"><span class="toc-number">11.30.</span> <span class="toc-text">import gym</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#np.clip"><span class="toc-number">11.31.</span> <span class="toc-text">np.clip</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/08/Ros%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3/%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87/STAP%20Sequencing%20Task-Agnostic%20Policies/" title="STAP: Sequencing Task-Agnostic Policies">STAP: Sequencing Task-Agnostic Policies</a><time datetime="2024-08-08T10:13:08.353Z" title="Created 2024-08-08 18:13:08">2024-08-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/27/%E8%B5%84%E6%96%99/%E5%91%BD%E4%BB%A42/" title="命令2">命令2</a><time datetime="2024-07-26T16:12:09.942Z" title="Created 2024-07-27 00:12:09">2024-07-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/07/22/mac%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/mac%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/" title="mac常见问题汇总">mac常见问题汇总</a><time datetime="2024-07-22T05:06:51.380Z" title="Created 2024-07-22 13:06:51">2024-07-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/28/vscode%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/vscode%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/" title="vscode插件开发">vscode插件开发</a><time datetime="2024-06-28T06:47:30.251Z" title="Created 2024-06-28 14:47:30">2024-06-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/06/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E5%BF%B5/" title="强化学习相关概念">强化学习相关概念</a><time datetime="2024-06-28T04:03:18.846Z" title="Created 2024-06-28 12:03:18">2024-06-28</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By ALTNT</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>