<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>ALTNT's Hexo Blog | ALTNT's Hexo Blog</title><meta name="author" content="ALTNT"><meta name="copyright" content="ALTNT"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="最近在看深度学习的东西，一开始看的吴恩达的UFLDL教程，有中文版就直接看了，后来发现有些地方总是不是很明确，又去看英文版，然后又找了些资料看，才发现，中文版的译者在翻译的时候会对省略的公式推导过程进行补充，但是补充的又是错的，难怪觉得有问题。反向传播法其实是神经网络的基础了，但是很多人在学的时候总是会遇到一些问题，或者看到大篇的公式觉得好像很难就退缩了，其实不难，就是一个链式求导法则反复用。">
<meta property="og:type" content="article">
<meta property="og:title" content="ALTNT&#39;s Hexo Blog">
<meta property="og:url" content="http://blog.705553939.xyz/2024/12/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%B8%80%E6%96%87%E5%BC%84%E6%87%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95%E2%80%94%E2%80%94BackPropagation/index.html">
<meta property="og:site_name" content="ALTNT&#39;s Hexo Blog">
<meta property="og:description" content="最近在看深度学习的东西，一开始看的吴恩达的UFLDL教程，有中文版就直接看了，后来发现有些地方总是不是很明确，又去看英文版，然后又找了些资料看，才发现，中文版的译者在翻译的时候会对省略的公式推导过程进行补充，但是补充的又是错的，难怪觉得有问题。反向传播法其实是神经网络的基础了，但是很多人在学的时候总是会遇到一些问题，或者看到大篇的公式觉得好像很难就退缩了，其实不难，就是一个链式求导法则反复用。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://blog.705553939.xyz/img/altnt.jpeg">
<meta property="article:published_time" content="2024-12-02T13:31:08.348Z">
<meta property="article:modified_time" content="2024-12-02T13:31:08.348Z">
<meta property="article:author" content="ALTNT">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://blog.705553939.xyz/img/altnt.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://blog.705553939.xyz/2024/12/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%B8%80%E6%96%87%E5%BC%84%E6%87%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95%E2%80%94%E2%80%94BackPropagation/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ALTNT\'s Hexo Blog',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2024-12-02 21:31:08'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/altnt.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">114</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="ALTNT's Hexo Blog"><span class="site-name">ALTNT's Hexo Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Untitled</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-12-02T13:31:08.348Z" title="Created 2024-12-02 21:31:08">2024-12-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-12-02T13:31:08.348Z" title="Updated 2024-12-02 21:31:08">2024-12-02</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>　　最近在看深度学习的东西，一开始看的吴恩达的UFLDL教程，有中文版就直接看了，后来发现有些地方总是不是很明确，又去看英文版，然后又找了些资料看，才发现，中文版的译者在翻译的时候会对省略的公式推导过程进行补充，但是补充的又是错的，难怪觉得有问题。反向传播法其实是神经网络的基础了，但是很多人在学的时候总是会遇到一些问题，或者看到大篇的公式觉得好像很难就退缩了，其实不难，就是一个链式求导法则反复用。如果不想看公式，可以直接把数值带进去，实际的计算一下，体会一下这个过程之后再来推导公式，这样就会觉得很容易了。</p>
<p>　　说到神经网络，大家看到这个图应该不陌生：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630140644406-409859737.png"></p>
<p>　　这是典型的三层神经网络的基本构成，Layer L1是输入层，Layer
L2是隐含层，Layer
L3是隐含层，我们现在手里有一堆数据{x1,x2,x3,...,xn},输出也是一堆数据{y1,y2,y3,...,yn},现在要他们在隐含层做某种变换，让你把数据灌进去后得到你期望的输出。如果你希望你的输出和原始输入一样，那么就是最常见的自编码模型（Auto-Encoder）。可能有人会问，为什么要输入输出都一样呢？有什么用啊？其实应用挺广的，在图像识别，文本分类等等都会用到，我会专门再写一篇Auto-Encoder的文章来说明，包括一些变种之类的。如果你的输出和原始输入不一样，那么就是很常见的人工神经网络了，相当于让原始数据通过一个映射来得到我们想要的输出数据，也就是我们今天要讲的话题。</p>
<p>　　本文直接举一个例子，带入数值演示反向传播法的过程，公式的推导等到下次写Auto-Encoder的时候再写，其实也很简单，感兴趣的同学可以自己推导下试试：）（注：本文假设你已经懂得基本的神经网络构成，如果完全不懂，可以参考Poll写的笔记：<a target="_blank" rel="noopener" href="http://www.cnblogs.com/maybe2030/p/5597716.html">[Mechine Learning
&amp; Algorithm] 神经网络基础</a>）</p>
<p>　　假设，你有这样一个网络层：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630141449671-1058672778.png"></p>
<p>　　第一层是输入层，包含两个神经元i1，i2，和截距项b1；第二层是隐含层，包含两个神经元h1,h2和截距项b2，第三层是输出o1,o2，每条线上标的wi是层与层之间连接的权重，激活函数我们默认为sigmoid函数。</p>
<p>　　现在对他们赋上初值，如下图：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630142019140-402363317.png"></p>
<p>　　其中，输入数据  i1=0.05，i2=0.10;</p>
<p>　　　　　输出数据 o1=0.01,o2=0.99;</p>
<p>　　　　　初始权重  w1=0.15,w2=0.20,w3=0.25,w4=0.30;</p>
<p>　　　　　　　　　  w5=0.40,w6=0.45,w7=0.50,w8=0.55</p>
<p>　　目标：给出输入数据i1,i2(0.05和0.10)，使输出尽可能与原始输出o1,o2(0.01和0.99)接近。</p>
<p>　　<strong>Step 1 前向传播</strong></p>
<p>　　1.输入层----&gt;隐含层：</p>
<p>　　计算神经元h1的输入加权和：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630142915359-294460310.png"></p>
<p>神经元h1的输出o1:(此处用到激活函数为sigmoid函数)：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630150115390-1035378028.png"></p>
<p>　　同理，可计算出神经元h2的输出o2：</p>
<p>　　<img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630150244265-1128303244.png"></p>
<p>　　2.隐含层----&gt;输出层：</p>
<p>　　计算输出层神经元o1和o2的值：</p>
<p>　　<img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630150517109-389457135.png"></p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630150638390-1210364296.png"></p>
<p>这样前向传播的过程就结束了，我们得到输出值为[0.75136079 ,
0.772928465]，与实际值[0.01 ,
0.99]相差还很远，现在我们对误差进行反向传播，更新权值，重新计算输出。</p>
<p><strong>Step 2 反向传播</strong></p>
<p>1.计算总误差</p>
<p>总误差：(square error)</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630151201812-1014280864.png"></p>
<p>但是有两个输出，所以分别计算o1和o2的误差，总误差为两者之和：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630151457593-1250510503.png"></p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630151508999-1967746600.png"></p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630151516093-1257166735.png"></p>
<p>2.隐含层----&gt;输出层的权值更新：</p>
<p>以权重参数w5为例，如果我们想知道w5对整体误差产生了多少影响，可以用整体误差对w5求偏导求出：（链式法则）</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630151916796-1001638091.png"></p>
<p>下面的图可以更直观的看清楚误差是怎样反向传播的：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630152018906-1524325812.png"></p>
<p>现在我们来分别计算每个式子的值：</p>
<p>计算<img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630152206781-7976168.png">：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630152258437-1960839452.png"></p>
<p>计算<img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630152417109-711077078.png">：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630152511937-1667481051.png"></p>
<p>（这一步实际上就是对sigmoid函数求导，比较简单，可以自己推导一下）</p>
<p>计算<img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630152625593-2083321635.png">：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630152658109-214239362.png"></p>
<p>最后三者相乘：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630152811640-888140287.png"></p>
<p>这样我们就计算出整体误差E(total)对w5的偏导值。</p>
<p>回过头来再看看上面的公式，我们发现：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630153103187-515052589.png"></p>
<p>为了表达方便，用<img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630153202812-585186566.png">来表示输出层的误差：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630153251234-1144531293.png"></p>
<p>因此，整体误差E(total)对w5的偏导公式可以写成：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630153405296-436656179.png"></p>
<p>如果输出层误差计为负的话，也可以写成：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630153514734-1544628024.png"></p>
<p>最后我们来更新w5的值：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630153614374-1624035276.png"></p>
<p>（其中，<img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630153700093-743859667.png">是学习速率，这里我们取0.5）</p>
<p>同理，可更新w6,w7,w8:</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630153807624-1231975059.png"></p>
<p>3.隐含层----&gt;隐含层的权值更新：</p>
<p>　方法其实与上面说的差不多，但是有个地方需要变一下，在上文计算总误差对w5的偏导时，是从out(o1)----&gt;net(o1)----&gt;w5,但是在隐含层之间的权值更新时，是out(h1)----&gt;net(h1)----&gt;w1,而out(h1)会接受E(o1)和E(o2)两个地方传来的误差，所以这个地方两个都要计算。</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630154317562-311369571.png"></p>
<p>计算<img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630154712202-1906007645.png">：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630154758531-934861299.png"></p>
<p>先计算<img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630154958296-1922097086.png">：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630155015546-1106216279.png"></p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630155036406-964647962.png"></p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630155117656-1905928379.png"></p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630155158468-157032005.png"></p>
<p>同理，计算出：</p>
<p>　　　　　　　　　　<img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630155310937-2103938446.png"></p>
<p>两者相加得到总值：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630155435218-396769942.png"></p>
<p>再计算<img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630155555562-1422254830.png">：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630155628046-229505495.png"></p>
<p>再计算<img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630155731421-239852713.png">：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630155706437-964861747.png"></p>
<p>最后，三者相乘：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630155827718-189457408.png"></p>
<p> 为了简化公式，用sigma(h1)表示隐含层单元h1的误差：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630160345281-679307550.png"></p>
<p>最后，更新w1的权值：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630160523437-1906004593.png"></p>
<p>同理，额可更新w2,w3,w4的权值：</p>
<p><img src="https://images2015.cnblogs.com/blog/853467/201606/853467-20160630160603484-1471434475.png"></p>
<p>　　这样误差反向传播法就完成了，最后我们再把更新的权值重新计算，不停地迭代，在这个例子中第一次迭代之后，总误差E(total)由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为[0.015912196,0.984065734](原输入为[0.01,0.99]),证明效果还是不错的。</p>
<p>代码(Python):</p>
<p><a href="javascript:void(0);" title="复制代码"><img src="//assets.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>1 #coding:utf-8 2 import random 3 import math 4 5 # 6 # 参数解释： 7
# "pd_" ：偏导的前缀 8 # "d_" ：导数的前缀 9 # "w_ho"
：隐含层到输出层的权重系数索引 10 # "w_ih"
：输入层到隐含层的权重系数的索引 11 12 class NeuralNetwork: 13
LEARNING_RATE = 0.5 14 15 def __init__(self, num_inputs, num_hidden,
num_outputs, hidden_layer_weights = None, hidden_layer_bias = None,
output_layer_weights = None, output_layer_bias = None): 16
self.num_inputs = num_inputs 17 18 self.hidden_layer =
NeuronLayer(num_hidden, hidden_layer_bias) 19 self.output_layer =
NeuronLayer(num_outputs, output_layer_bias) 20 21
self.init_weights_from_inputs_to_hidden_layer_neurons(hidden_layer_weights)
22
self.init_weights_from_hidden_layer_neurons_to_output_layer_neurons(output_layer_weights)
23 24 def init_weights_from_inputs_to_hidden_layer_neurons(self,
hidden_layer_weights): 25 weight_num = 0 26 for h in
range(len(self.hidden_layer.neurons)): 27 for i in
range(self.num_inputs): 28 if not hidden_layer_weights: 29
self.hidden_layer.neurons[h].weights.append(random.random()) 30 else: 31
self.hidden_layer.neurons[h].weights.append(hidden_layer_weights[weight_num])
32 weight_num += 1 33 34 def
init_weights_from_hidden_layer_neurons_to_output_layer_neurons(self,
output_layer_weights): 35 weight_num = 0 36 for o in
range(len(self.output_layer.neurons)): 37 for h in
range(len(self.hidden_layer.neurons)): 38 if not output_layer_weights:
39 self.output_layer.neurons[o].weights.append(random.random()) 40 else:
41
self.output_layer.neurons[o].weights.append(output_layer_weights[weight_num])
42 weight_num += 1 43 44 def inspect(self): 45 print('------') 46
print('* Inputs: {}'.format(self.num_inputs)) 47 print('------') 48
print('Hidden Layer') 49 self.hidden_layer.inspect() 50 print('------')
51 print('* Output Layer') 52 self.output_layer.inspect() 53
print('------') 54 55 def feed_forward(self, inputs): 56
hidden_layer_outputs = self.hidden_layer.feed_forward(inputs) 57 return
self.output_layer.feed_forward(hidden_layer_outputs) 58 59 def
train(self, training_inputs, training_outputs): 60
self.feed_forward(training_inputs) 61 62 # 1. 输出神经元的值 63
pd_errors_wrt_output_neuron_total_net_input = [0] *
len(self.output_layer.neurons) 64 for o in
range(len(self.output_layer.neurons)): 65 66 # ∂E/∂zⱼ 67
pd_errors_wrt_output_neuron_total_net_input[o] =
self.output_layer.neurons[o].calculate_pd_error_wrt_total_net_input(training_outputs[o])
68 69 # 2. 隐含层神经元的值 70
pd_errors_wrt_hidden_neuron_total_net_input = [0] *
len(self.hidden_layer.neurons) 71 for h in
range(len(self.hidden_layer.neurons)): 72 73 # dE/dyⱼ = Σ ∂E/∂zⱼ *
∂z/∂yⱼ = Σ ∂E/∂zⱼ * wᵢⱼ 74 d_error_wrt_hidden_neuron_output = 0 75 for o
in range(len(self.output_layer.neurons)): 76
d_error_wrt_hidden_neuron_output +=
pd_errors_wrt_output_neuron_total_net_input[o] *
self.output_layer.neurons[o].weights[h] 77 78 # ∂E/∂zⱼ = dE/dyⱼ * ∂zⱼ/∂
79 pd_errors_wrt_hidden_neuron_total_net_input[h] =
d_error_wrt_hidden_neuron_output *
self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_input() 80
81 # 3. 更新输出层权重系数 82 for o in
range(len(self.output_layer.neurons)): 83 for w_ho in
range(len(self.output_layer.neurons[o].weights)): 84 85 # ∂Eⱼ/∂wᵢⱼ =
∂E/∂zⱼ * ∂zⱼ/∂wᵢⱼ 86 pd_error_wrt_weight =
pd_errors_wrt_output_neuron_total_net_input[o] *
self.output_layer.neurons[o].calculate_pd_total_net_input_wrt_weight(w_ho)
87 88 # Δw = α * ∂Eⱼ/∂wᵢ 89 self.output_layer.neurons[o].weights[w_ho]
-= self.LEARNING_RATE * pd_error_wrt_weight 90 91 # 4.
更新隐含层的权重系数 92 for h in range(len(self.hidden_layer.neurons)):
93 for w_ih in range(len(self.hidden_layer.neurons[h].weights)): 94 95 #
∂Eⱼ/∂wᵢ = ∂E/∂zⱼ * ∂zⱼ/∂wᵢ 96 pd_error_wrt_weight =
pd_errors_wrt_hidden_neuron_total_net_input[h] *
self.hidden_layer.neurons[h].calculate_pd_total_net_input_wrt_weight(w_ih)
97 98 # Δw = α * ∂Eⱼ/∂wᵢ 99 self.hidden_layer.neurons[h].weights[w_ih]
-= self.LEARNING_RATE * pd_error_wrt_weight 100 101 def
calculate_total_error(self, training_sets): 102 total_error = 0 103 for
t in range(len(training_sets)): 104 training_inputs, training_outputs =
training_sets[t] 105 self.feed_forward(training_inputs) 106 for o in
range(len(training_outputs)): 107 total_error +=
self.output_layer.neurons[o].calculate_error(training_outputs[o]) 108
return total_error 109 110 class NeuronLayer: 111 def __init__(self,
num_neurons, bias): 112 113 # 同一层的神经元共享一个截距项b 114
self.bias = bias if bias else random.random() 115 116 self.neurons = []
117 for i in range(num_neurons): 118
self.neurons.append(Neuron(self.bias)) 119 120 def inspect(self): 121
print('Neurons:', len(self.neurons)) 122 for n in
range(len(self.neurons)): 123 print(' Neuron', n) 124 for w in
range(len(self.neurons[n].weights)): 125 print(' Weight:',
self.neurons[n].weights[w]) 126 print(' Bias:', self.bias) 127 128 def
feed_forward(self, inputs): 129 outputs = [] 130 for neuron in
self.neurons: 131 outputs.append(neuron.calculate_output(inputs)) 132
return outputs 133 134 def get_outputs(self): 135 outputs = [] 136 for
neuron in self.neurons: 137 outputs.append(neuron.output) 138 return
outputs 139 140 class Neuron: 141 def __init__(self, bias): 142
self.bias = bias 143 self.weights = [] 144 145 def
calculate_output(self, inputs): 146 self.inputs = inputs 147 self.output
= self.squash(self.calculate_total_net_input()) 148 return self.output
149 150 def calculate_total_net_input(self): 151 total = 0 152 for i in
range(len(self.inputs)): 153 total += self.inputs[i] * self.weights[i]
154 return total + self.bias 155 156 # 激活函数sigmoid 157 def
squash(self, total_net_input): 158 return 1 / (1 +
math.exp(-total_net_input)) 159 160 161 def
calculate_pd_error_wrt_total_net_input(self, target_output): 162 return
self.calculate_pd_error_wrt_output(target_output) *
self.calculate_pd_total_net_input_wrt_input(); 163 164 #
每一个神经元的误差是由平方差公式计算的 165 def calculate_error(self,
target_output): 166 return 0.5 * (target_output - self.output) ** 2 167
168 169 def calculate_pd_error_wrt_output(self, target_output): 170
return -(target_output - self.output) 171 172 173 def
calculate_pd_total_net_input_wrt_input(self): 174 return self.output *
(1 - self.output) 175 176 177 def
calculate_pd_total_net_input_wrt_weight(self, index): 178 return
self.inputs[index] 179 180 181 # 文中的例子: 182 183 nn =
NeuralNetwork(2, 2, 2, hidden_layer_weights=[0.15, 0.2, 0.25, 0.3],
hidden_layer_bias=0.35, output_layer_weights=[0.4, 0.45, 0.5, 0.55],
output_layer_bias=0.6) 184 for i in range(10000): 185 nn.train([0.05,
0.1], [0.01, 0.09]) 186 print(i, round(nn.calculate_total_error([[[0.05,
0.1], [0.01, 0.09]]]), 9)) 187 188 189
#另外一个例子，可以把上面的例子注释掉再运行一下: 190 191 # training_sets
= [ 192 # [[0, 0], [0]], 193 # [[0, 1], [1]], 194 # [[1, 0], [1]], 195 #
[[1, 1], [0]] 196 # ] 197 198 # nn =
NeuralNetwork(len(training_sets[0][0]), 5, len(training_sets[0][1])) 199
# for i in range(10000): 200 # training_inputs, training_outputs =
random.choice(training_sets) 201 # nn.train(training_inputs,
training_outputs) 202 # print(i,
nn.calculate_total_error(training_sets))</p>
<p><a href="javascript:void(0);" title="复制代码"><img src="//assets.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>　　最后写到这里就结束了，现在还不会用latex编辑数学公式，本来都直接想写在草稿纸上然后扫描了传上来，但是觉得太影响阅读体验了。以后会用公式编辑器后再重把公式重新编辑一遍。稳重使用的是sigmoid激活函数，实际还有几种不同的激活函数可以选择，具体的可以参考文献[3]，最后推荐一个在线演示神经网络变化的网址：http://www.emergentmind.com/neural-network，可以自己填输入输出，然后观看每一次迭代权值的变化，很好玩~如果有错误的或者不懂的欢迎留言：）</p>
<p>参考文献：</p>
<p>1.Poll的笔记：<a target="_blank" rel="noopener" href="http://www.cnblogs.com/maybe2030/p/5597716.html">[Mechine Learning
&amp; Algorithm]
神经网络基础</a>（http://www.cnblogs.com/maybe2030/p/5597716.html#3457159
）</p>
<p>2.Rachel_Zhang:http://blog.csdn.net/abcjennifer/article/details/7758797</p>
<p>3.http://www.cedar.buffalo.edu/%7Esrihari/CSE574/Chap5/Chap5.3-BackProp.pdf</p>
<p>4.https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</p>
<p>------------------------------------本博客所有内容以学习、研究和分享为主，如需转载，请联系本人，标明作者和出处，并且是非商业用途，谢谢！--------------------------------</p>
<p>本文转自 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/charlotte77/p/5629865.html">https://www.cnblogs.com/charlotte77/p/5629865.html</a>，如有侵权，请联系删除。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz">ALTNT</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="http://blog.705553939.xyz/2024/12/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%B8%80%E6%96%87%E5%BC%84%E6%87%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95%E2%80%94%E2%80%94BackPropagation/">http://blog.705553939.xyz/2024/12/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%B8%80%E6%96%87%E5%BC%84%E6%87%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%B3%95%E2%80%94%E2%80%94BackPropagation/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/img/altnt.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/12/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/%E8%A7%A3%E8%80%A6%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0/" title="“解耦表征学习&quot;"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">“解耦表征学习&quot;</div></div></a></div><div class="next-post pull-right"><a href="/2024/12/02/crop_classification/Crop%20classification/12%E6%9C%882%E6%97%A5%E5%91%A8%E6%8A%A5/" title=""><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info"></div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/altnt.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ALTNT</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">114</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">29</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ALTNT"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/02/crop_classification/Crop%20classification/%E9%87%8D%E6%96%B0%E5%A4%8D%E7%8E%B0CACM%20%E8%AE%B0%E5%BD%95/" title="重新复现CACM记录">重新复现CACM记录</a><time datetime="2025-02-02T06:13:13.347Z" title="Created 2025-02-02 14:13:13">2025-02-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/21/crop_classification/Crop%20classification/1%E6%9C%8820%E6%97%A5%E5%91%A8%E6%8A%A5-2025/" title="“周报2025年1月20日&quot;">“周报2025年1月20日&quot;</a><time datetime="2025-01-21T05:36:10.775Z" title="Created 2025-01-21 13:36:10">2025-01-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/21/crop_classification/Crop%20classification/%E5%91%A8%E6%8A%A51%E6%9C%8811%E6%97%A5-2025/" title="“周报 1 月 11 日2025&quot;">“周报 1 月 11 日2025&quot;</a><time datetime="2025-01-21T04:01:55.781Z" title="Created 2025-01-21 12:01:55">2025-01-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/19/sequence-processing/2019-Bert/" title="“BERT Pre-training of Deep Bidirectional Transformers for Language Understanding&quot;">“BERT Pre-training of Deep Bidirectional Transformers for Language Understanding&quot;</a><time datetime="2025-01-19T13:31:49.199Z" title="Created 2025-01-19 21:31:49">2025-01-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/19/crop_classification/Crop%20classification/2024-PhenoCropNet/" title="Untitled">Untitled</a><time datetime="2025-01-19T09:29:22.650Z" title="Created 2025-01-19 17:29:22">2025-01-19</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By ALTNT</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>